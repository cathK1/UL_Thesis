{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entropy Analysis - ALL\n",
        "Takes data generated from `../src/extraction_llama.py` and performs entropy analysis experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "from scipy.stats import entropy, ttest_ind, spearmanr, normaltest, ttest_1samp, kruskal\n",
        "from tqdm import tqdm\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Config\n",
        "DATA_DIR = \"../data/chunk_0_299_output/attention\"\n",
        "PREPARED_DATA_DIR = \"../data/prepared_data_individual\"\n",
        "ENTROPY_RESULTS_DIR = \"../data/entropy_results_dirs\"\n",
        "RESULTS_CSV = \"../data/chunks/results_llama.csv\"\n",
        "TOKEN_METADATA = \"../data/chunk_0_299_output/token_metadata_llama.json\"\n",
        "AGGREGATED_ANALYSIS_DIR = \"../data/aggregated_analysis_combined\"  # Contains individual phrase aggregation files\n",
        "STATISTICAL_RESULTS_DIR = \"../data/statistical_results_combined\"  # Contains individual phrase statistical analysis files\n",
        "VISUAL_RESULTS_DIR = \"../figures/visualizations_individual_combined\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare attention data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmeF1One2JDi",
        "outputId": "b44a3b73-513b-4bb3-a952-df9e1a1558b7"
      },
      "outputs": [],
      "source": [
        "class AttentionDataPreparationIndividual:\n",
        "    def __init__(self, data_dir: str, results_csv: str, token_metadata_json: str):\n",
        "        \"\"\"\n",
        "        Prepare Data: Load attention files, normalize rows, identify individual phrase positions (6 + 1)\n",
        "\n",
        "        Args:\n",
        "            data_dir: Base directory containing row_X/prompt_Y/attn_layer_Z.npy structure\n",
        "            results_csv: Path to results CSV file\n",
        "            token_metadata_json: Path to token metadata JSON file\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.attention_dir = data_dir\n",
        "\n",
        "        # Load dataset and metadata\n",
        "        self.results_df = pd.read_csv(results_csv)\n",
        "        with open(token_metadata_json, 'r') as f:\n",
        "            self.token_metadata = json.load(f)\n",
        "\n",
        "        # Define individual bias phrases (6 + 1 combined)\n",
        "        self.individual_bias_phrases = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"]\n",
        "        self.analysis_keys = self.individual_bias_phrases + [\"combined\"]\n",
        "\n",
        "        # Storage for processed data\n",
        "        self.normalized_attention = {}\n",
        "        self.hint_positions_individual = {}  # {phrase: {row_prompt: positions}}\n",
        "        self.validation_log = []\n",
        "\n",
        "        # Prompt mapping for file system\n",
        "        self.prompt_mapping = {\n",
        "            'Prompt_1': 'prompt_1',\n",
        "            'Prompt_2': 'prompt_2',\n",
        "            'Prompt_3': 'prompt_3'\n",
        "        }\n",
        "\n",
        "        # Initialize storage for each phrase analysis\n",
        "        for phrase in self.analysis_keys:\n",
        "            self.hint_positions_individual[phrase] = {}\n",
        "\n",
        "        print(f\"Loaded {len(self.results_df)} rows from dataset\")\n",
        "        print(f\"Loaded token metadata for {len(self.token_metadata)} rows\")\n",
        "        print(f\"Individual phrase analyses: {', '.join(self.individual_bias_phrases)}\")\n",
        "        print(f\"Total analyses: {len(self.analysis_keys)} (6 individual + 1 combined)\")\n",
        "\n",
        "    def validate_data_structure(self) -> Dict[str, int]:\n",
        "        \"\"\"Validate the attention data structure and count available files\"\"\"\n",
        "        print(\"Validating data structure...\")\n",
        "\n",
        "        validation_stats = {\n",
        "            'total_rows_checked': 0,\n",
        "            'rows_with_attention_data': 0,\n",
        "            'total_attention_files': 0,\n",
        "            'missing_files': 0,\n",
        "            'invalid_files': 0\n",
        "        }\n",
        "\n",
        "        max_check = min(10, len(self.results_df))\n",
        "\n",
        "        for row_idx in range(max_check):\n",
        "            validation_stats['total_rows_checked'] += 1\n",
        "            row_has_data = False\n",
        "\n",
        "            for prompt_col in ['Prompt_1', 'Prompt_2', 'Prompt_3']:\n",
        "                if pd.isna(self.results_df.iloc[row_idx][prompt_col]):\n",
        "                    continue\n",
        "\n",
        "                prompt_file_name = self.prompt_mapping[prompt_col]\n",
        "                row_dir = os.path.join(self.attention_dir, f\"row_{row_idx}\", prompt_file_name)\n",
        "\n",
        "                if os.path.exists(row_dir):\n",
        "                    npy_files = [f for f in os.listdir(row_dir) if f.endswith('.npy') and 'attn_layer_' in f]\n",
        "\n",
        "                    for npy_file in npy_files:\n",
        "                        file_path = os.path.join(row_dir, npy_file)\n",
        "                        try:\n",
        "                            test_load = np.load(file_path)\n",
        "                            if len(test_load.shape) == 4:\n",
        "                                validation_stats['total_attention_files'] += 1\n",
        "                                row_has_data = True\n",
        "                            else:\n",
        "                                self.validation_log.append(f\"Invalid shape {test_load.shape} in {file_path}\")\n",
        "                                validation_stats['invalid_files'] += 1\n",
        "                        except Exception as e:\n",
        "                            self.validation_log.append(f\"Cannot load {file_path}: {e}\")\n",
        "                            validation_stats['invalid_files'] += 1\n",
        "                else:\n",
        "                    validation_stats['missing_files'] += 1\n",
        "\n",
        "            if row_has_data:\n",
        "                validation_stats['rows_with_attention_data'] += 1\n",
        "\n",
        "        print(f\"Checked {validation_stats['total_rows_checked']} rows\")\n",
        "        print(f\"Found {validation_stats['total_attention_files']} valid attention files\")\n",
        "\n",
        "        return validation_stats\n",
        "\n",
        "    def load_and_normalize_attention(self, row_idx: int, prompt_name: str, layer_idx: int) -> Optional[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Load attention weights and normalize rows to ensure each row sums to 1\n",
        "\n",
        "        Args:\n",
        "            row_idx: Row index in dataset\n",
        "            prompt_name: Prompt file name (prompt_1, prompt_2, prompt_3)\n",
        "            layer_idx: Layer index (0 to num_layers-1)\n",
        "\n",
        "        Returns:\n",
        "            Normalized attention matrix [batch, heads, seq_len, seq_len] or None\n",
        "        \"\"\"\n",
        "        file_path = os.path.join(\n",
        "            self.attention_dir,\n",
        "            f\"row_{row_idx}\",\n",
        "            prompt_name,\n",
        "            f\"attn_layer_{layer_idx}.npy\"\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            attention = np.load(file_path)\n",
        "\n",
        "            if len(attention.shape) != 4:\n",
        "                self.validation_log.append(f\"Invalid shape {attention.shape} in {file_path}\")\n",
        "                return None\n",
        "\n",
        "            batch_size, num_heads, seq_len, key_len = attention.shape\n",
        "            normalized_attention = attention.copy()\n",
        "\n",
        "            # Normalize rows to ensure each query's attention sums to 1\n",
        "            normalization_fixes = 0\n",
        "            for b in range(batch_size):\n",
        "                for h in range(num_heads):\n",
        "                    for q in range(seq_len):\n",
        "                        attn_row = attention[b, h, q, :]\n",
        "                        row_sum = np.sum(attn_row)\n",
        "\n",
        "                        if not (0.99 <= row_sum <= 1.01):\n",
        "                            if row_sum > 0 and not np.isnan(row_sum):\n",
        "                                normalized_attention[b, h, q, :] = attn_row / row_sum\n",
        "                                normalization_fixes += 1\n",
        "                            else:\n",
        "                                normalized_attention[b, h, q, :] = np.ones(key_len) / key_len\n",
        "                                normalization_fixes += 1\n",
        "\n",
        "            return normalized_attention\n",
        "\n",
        "        except Exception as e:\n",
        "            self.validation_log.append(f\"Error processing {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def identify_hint_positions_individual(self, row_idx: int, prompt_col: str) -> Dict[str, List[int]]:\n",
        "        \"\"\"\n",
        "        Identify hint positions for each individual bias phrase + combined\n",
        "\n",
        "        Args:\n",
        "            row_idx: Row index in dataset\n",
        "            prompt_col: Prompt column name (Prompt_1, Prompt_2, Prompt_3)\n",
        "\n",
        "        Returns:\n",
        "            Dict mapping phrase -> list of token positions\n",
        "            Plus 'combined' key with all positions\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Access token metadata\n",
        "            if isinstance(self.token_metadata, list):\n",
        "                if row_idx < len(self.token_metadata):\n",
        "                    row_metadata = self.token_metadata[row_idx]\n",
        "                else:\n",
        "                    return {phrase: [] for phrase in self.analysis_keys}\n",
        "            else:\n",
        "                if str(row_idx) in self.token_metadata:\n",
        "                    row_metadata = self.token_metadata[str(row_idx)]\n",
        "                elif row_idx in self.token_metadata:\n",
        "                    row_metadata = self.token_metadata[row_idx]\n",
        "                else:\n",
        "                    return {phrase: [] for phrase in self.analysis_keys}\n",
        "\n",
        "            # Extract input tokens\n",
        "            if isinstance(row_metadata, dict) and prompt_col in row_metadata:\n",
        "                prompt_metadata = row_metadata[prompt_col]\n",
        "                input_tokens = prompt_metadata.get('input_tokens', [])\n",
        "            elif isinstance(row_metadata, dict) and 'input_tokens' in row_metadata:\n",
        "                input_tokens = row_metadata.get('input_tokens', [])\n",
        "            else:\n",
        "                return {phrase: [] for phrase in self.analysis_keys}\n",
        "\n",
        "            # Find positions for each individual phrase\n",
        "            phrase_positions = {}\n",
        "            all_combined_positions = []\n",
        "\n",
        "            for phrase in self.individual_bias_phrases:\n",
        "                phrase_specific_positions = []\n",
        "\n",
        "                for i, token in enumerate(input_tokens):\n",
        "                    clean_token = str(token).replace('Ġ', '').replace('▁', '').replace('##', '').lower().strip()\n",
        "\n",
        "                    if phrase in clean_token:\n",
        "                        phrase_specific_positions.append(i)\n",
        "                        all_combined_positions.append(i)\n",
        "\n",
        "                phrase_positions[phrase] = sorted(list(set(phrase_specific_positions)))\n",
        "\n",
        "            # Add combined positions\n",
        "            phrase_positions['combined'] = sorted(list(set(all_combined_positions)))\n",
        "\n",
        "            return phrase_positions\n",
        "\n",
        "        except (KeyError, TypeError, AttributeError, IndexError) as e:\n",
        "            self.validation_log.append(f\"Error finding individual hints for row {row_idx}, {prompt_col}: {e}\")\n",
        "            return {phrase: [] for phrase in self.analysis_keys}\n",
        "\n",
        "    def process_all_attention_data_individual(self, max_rows: int = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Process all attention data: load, normalize, and identify individual phrase positions\n",
        "\n",
        "        Args:\n",
        "            max_rows: Maximum number of rows to process (None for all)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with processing statistics\n",
        "        \"\"\"\n",
        "        print(\"Processing attention data with individual phrase analysis...\")\n",
        "        print(f\"Analyzing {len(self.individual_bias_phrases)} individual phrases + 1 combined analysis\")\n",
        "\n",
        "        if max_rows is None:\n",
        "            max_rows = len(self.results_df)\n",
        "\n",
        "        num_layers = self._detect_num_layers()\n",
        "        print(f\"Detected {num_layers} layers in the model\")\n",
        "\n",
        "        processing_stats = {\n",
        "            'total_rows_processed': 0,\n",
        "            'total_prompts_processed': 0,\n",
        "            'total_attention_matrices_loaded': 0,\n",
        "            'phrases_processed': {phrase: 0 for phrase in self.analysis_keys},\n",
        "            'rows_with_errors': 0\n",
        "        }\n",
        "\n",
        "        for row_idx in tqdm(range(min(max_rows, len(self.results_df))),\n",
        "                            desc=\"Processing rows\",\n",
        "                            leave=False):\n",
        "        # for row_idx in range(min(max_rows, len(self.results_df))):\n",
        "            row_has_errors = False\n",
        "            self.normalized_attention[row_idx] = {}\n",
        "\n",
        "            for prompt_col in ['Prompt_1', 'Prompt_2', 'Prompt_3']:\n",
        "                if pd.isna(self.results_df.iloc[row_idx][prompt_col]):\n",
        "                    continue\n",
        "\n",
        "                processing_stats['total_prompts_processed'] += 1\n",
        "                prompt_file_name = self.prompt_mapping[prompt_col]\n",
        "\n",
        "                # Get individual phrase positions\n",
        "                phrase_positions = self.identify_hint_positions_individual(row_idx, prompt_col)\n",
        "\n",
        "                # Store positions for each individual phrase + combined\n",
        "                for phrase, positions in phrase_positions.items():\n",
        "                    key = f\"{row_idx}_{prompt_col}\"\n",
        "                    self.hint_positions_individual[phrase][key] = positions\n",
        "\n",
        "                    if positions:\n",
        "                        processing_stats['phrases_processed'][phrase] += len(positions)\n",
        "\n",
        "                # Load and normalize attention for each layer\n",
        "                for layer_idx in range(num_layers):\n",
        "                    normalized_attn = self.load_and_normalize_attention(row_idx, prompt_file_name, layer_idx)\n",
        "\n",
        "                    if normalized_attn is not None:\n",
        "                        key = f\"{prompt_col}_layer_{layer_idx}\"\n",
        "                        self.normalized_attention[row_idx][key] = normalized_attn\n",
        "                        processing_stats['total_attention_matrices_loaded'] += 1\n",
        "                    else:\n",
        "                        row_has_errors = True\n",
        "\n",
        "            if row_has_errors:\n",
        "                processing_stats['rows_with_errors'] += 1\n",
        "\n",
        "            processing_stats['total_rows_processed'] += 1\n",
        "\n",
        "            # if (row_idx + 1) % 50 == 0:\n",
        "            #     print(f\"Processed {row_idx + 1} rows...\")\n",
        "\n",
        "        print(f\"Processing complete: {processing_stats['total_rows_processed']} rows processed\")\n",
        "        print(\"Individual phrase statistics:\")\n",
        "        for phrase, count in processing_stats['phrases_processed'].items():\n",
        "            print(f\"  {phrase}: {count} positions found\")\n",
        "\n",
        "        return processing_stats\n",
        "\n",
        "    def _detect_num_layers(self) -> int:\n",
        "        \"\"\"Auto-detect number of layers by checking available files\"\"\"\n",
        "        for row_idx in range(min(5, len(self.results_df))):\n",
        "            for prompt_name in ['prompt_1', 'prompt_2', 'prompt_3']:\n",
        "                row_dir = os.path.join(self.attention_dir, f\"row_{row_idx}\", prompt_name)\n",
        "                if os.path.exists(row_dir):\n",
        "                    files = [f for f in os.listdir(row_dir) if f.startswith('attn_layer_') and f.endswith('.npy')]\n",
        "                    if files:\n",
        "                        layer_numbers = []\n",
        "                        for f in files:\n",
        "                            try:\n",
        "                                layer_num = int(f.replace('attn_layer_', '').replace('.npy', ''))\n",
        "                                layer_numbers.append(layer_num)\n",
        "                            except:\n",
        "                                continue\n",
        "                        if layer_numbers:\n",
        "                            return max(layer_numbers) + 1\n",
        "\n",
        "        return 16  # Default based on diagnostics\n",
        "\n",
        "    def save_results(self, output_dir: str = \"../data/prepared_data_individual\"):\n",
        "        \"\"\"Save results for use in subsequent processing steps\"\"\"\n",
        "        print(f\"Saving individual phrase results to {output_dir}/\")\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save individual phrase positions\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(output_dir, f\"hint_positions_{phrase}.json\")\n",
        "            with open(phrase_file, 'w') as f:\n",
        "                json.dump(self.hint_positions_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Save validation log if exists\n",
        "        if self.validation_log:\n",
        "            log_file = os.path.join(output_dir, \"validation_log.txt\")\n",
        "            with open(log_file, 'w') as f:\n",
        "                for log_entry in self.validation_log:\n",
        "                    f.write(log_entry + \"\\n\")\n",
        "\n",
        "        # Save summary statistics\n",
        "        summary = {\n",
        "            'total_rows_in_dataset': len(self.results_df),\n",
        "            'rows_with_normalized_attention': len(self.normalized_attention),\n",
        "            'individual_phrases_analyzed': self.individual_bias_phrases,\n",
        "            'analysis_types': {\n",
        "                'individual_phrases': len(self.individual_bias_phrases),\n",
        "                'combined_analysis': 1,\n",
        "                'total_analyses': len(self.analysis_keys)\n",
        "            },\n",
        "            'phrase_position_counts': {\n",
        "                phrase: len(positions_dict)\n",
        "                for phrase, positions_dict in self.hint_positions_individual.items()\n",
        "            },\n",
        "            'total_validation_issues': len(self.validation_log),\n",
        "            'prompt_mapping': self.prompt_mapping\n",
        "        }\n",
        "\n",
        "        summary_file = os.path.join(output_dir, \"preparation_summary_individual.json\")\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase data preparation completed successfully!\")\n",
        "        print(f\"Generated {len(self.individual_bias_phrases)} individual + 1 combined = {len(self.analysis_keys)} total analyses\")\n",
        "        print(f\"Files generated:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"  - hint_positions_{phrase}.json\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_data_preparation_individual():\n",
        "    \"\"\"\n",
        "    Run the complete data preparation pipeline for individual phrases\n",
        "    \"\"\"\n",
        "    # Configuration - Update these paths for your setup\n",
        "    # DATA_DIR = \"../Test_output_10rows/input_attention\"\n",
        "    # TOKEN_METADATA = \"/content/Test_output_10rows/input_attention_metadata.json\"\n",
        "    # RESULTS_CSV = \"/content/Test_output_10rows/results_with_predictions.csv\"\n",
        "\n",
        "    print(\"Starting Individual Phrase Data Preparation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize data preparation\n",
        "    processor = AttentionDataPreparationIndividual(\n",
        "        data_dir=DATA_DIR,\n",
        "        results_csv=RESULTS_CSV,\n",
        "        token_metadata_json=TOKEN_METADATA\n",
        "    )\n",
        "\n",
        "    # Validate data structure\n",
        "    validation_stats = processor.validate_data_structure()\n",
        "\n",
        "    # Process all data\n",
        "    processing_stats = processor.process_all_attention_data_individual()\n",
        "\n",
        "    # Save results for next stage\n",
        "    output_dir = processor.save_results(PREPARED_DATA_DIR)\n",
        "\n",
        "    print(f\"\\nIndividual phrase data preparation completed: {output_dir}\")\n",
        "    print(\"Ready for individual phrase entropy computation\")\n",
        "\n",
        "    return processor, processing_stats\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processor, stats = run_data_preparation_individual()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u25vyFOT2sgr",
        "outputId": "d68f53a4-4a74-41a7-f784-2836854b1d6f"
      },
      "outputs": [],
      "source": [
        "class EntropyComputationIndividual:\n",
        "    def __init__(self, prepared_data_dir: str, data_dir: str):\n",
        "        \"\"\"\n",
        "        Compute Entropy: Granular computation per query/head/layer for individual phrases (6 + 1)\n",
        "\n",
        "        Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\n",
        "\n",
        "        Args:\n",
        "            prepared_data_dir: Directory containing individual phrase hint_positions files\n",
        "            data_dir: Base directory containing attention files\n",
        "        \"\"\"\n",
        "        self.prepared_data_dir = prepared_data_dir\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "\n",
        "        # Load individual phrase positions from data preparation stage\n",
        "        self.hint_positions_individual = {}\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(prepared_data_dir, f\"hint_positions_{phrase}.json\")\n",
        "            if os.path.exists(phrase_file):\n",
        "                with open(phrase_file, 'r') as f:\n",
        "                    self.hint_positions_individual[phrase] = json.load(f)\n",
        "                print(f\"Loaded hint positions for '{phrase}': {len(self.hint_positions_individual[phrase])} prompt instances\")\n",
        "            else:\n",
        "                print(f\"Warning: {phrase_file} not found\")\n",
        "                self.hint_positions_individual[phrase] = {}\n",
        "\n",
        "        print(f\"Loaded hint positions for {len(self.hint_positions_individual)} individual phrase analyses\")\n",
        "\n",
        "        # Storage for entropy results\n",
        "        self.entropies = {}  # {row_prompt: entropies[ℓ][h][i] array} - same for all analyses\n",
        "        self.hint_entropies_individual = {}  # {phrase: {row_prompt: [entropies[ℓ][h][pos] for pos in phrase_positions]}}\n",
        "\n",
        "        # Initialize individual phrase storage\n",
        "        for phrase in self.analysis_keys:\n",
        "            self.hint_entropies_individual[phrase] = {}\n",
        "\n",
        "\n",
        "    def compute_attention_entropy(self, attention_matrix: np.ndarray, hint_positions: List[int] = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Compute entropy for attention matrix using the specified formula\n",
        "\n",
        "        Args:\n",
        "            attention_matrix: Shape [batch, heads, seq_len, seq_len]\n",
        "            hint_positions: List of hint token positions for subsetting\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with entropies and hint_entropies\n",
        "        \"\"\"\n",
        "        if len(attention_matrix.shape) != 4:\n",
        "            return None\n",
        "\n",
        "        # print(f\"DEBUG: attention_matrix shape: {attention_matrix.shape}\")\n",
        "        # print(f\"DEBUG: First row, head 0: {attention_matrix[0, 0, 0, :10]}\")\n",
        "        # print(f\"DEBUG: Middle row, head 0: {attention_matrix[0, 0, attention_matrix.shape[2]//2, :10]}\")\n",
        "        # print(f\"DEBUG: Last row, head 0: {attention_matrix[0, 0, -1, :10]}\")\n",
        "        # print(f\"DEBUG: Same position, different heads: {attention_matrix[0, :5, 10, 0]}\")  # Same query pos, different heads\n",
        "        \n",
        "        \n",
        "        batch_size, num_heads, seq_len, key_len = attention_matrix.shape\n",
        "\n",
        "        # Store entropies in [heads, seq_len] format\n",
        "        entropies = np.zeros((num_heads, seq_len))\n",
        "\n",
        "        # Loop over heads, then queries - exact methodology\n",
        "        for h in range(num_heads):\n",
        "            for i in range(seq_len):  # query token i\n",
        "                # Get attention row for query i in head h\n",
        "                attn_row = attention_matrix[0, h, i, :]  # Assuming batch_size=1\n",
        "\n",
        "                # Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\n",
        "                ent = entropy(attn_row, base=2)\n",
        "\n",
        "                # Handle edge cases\n",
        "                if np.isnan(ent) or np.isinf(ent):\n",
        "                    ent = 0.0\n",
        "\n",
        "                entropies[h, i] = ent\n",
        "\n",
        "        # Subset for hints\n",
        "        hint_entropies = []\n",
        "        if hint_positions:\n",
        "            for h in range(num_heads):\n",
        "                for pos in hint_positions:\n",
        "                    if pos < seq_len:\n",
        "                        hint_entropies.append(entropies[h, pos])\n",
        "\n",
        "        return {\n",
        "            'entropies': entropies,\n",
        "            'hint_entropies': hint_entropies,\n",
        "            'num_heads': num_heads,\n",
        "            'seq_len': seq_len,\n",
        "            'hint_positions': hint_positions or []\n",
        "        }\n",
        "\n",
        "\n",
        "    def compute_entropy_for_dataset_individual(self, max_rows: int = None):\n",
        "        \"\"\"\n",
        "        Process entropy computation for the entire dataset with individual phrase analysis\n",
        "\n",
        "        Args:\n",
        "            max_rows: Maximum number of rows to process (None for all)\n",
        "        \"\"\"\n",
        "        print(\"Computing entropy for dataset with individual phrase analysis...\")\n",
        "        print(\"Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\")\n",
        "        print(f\"Analyzing: {', '.join(self.analysis_keys)}\")\n",
        "\n",
        "        # Get available rows\n",
        "        available_rows = []\n",
        "        for item in os.listdir(self.data_dir):\n",
        "            if item.startswith('row_') and os.path.isdir(os.path.join(self.data_dir, item)):\n",
        "                try:\n",
        "                    row_num = int(item.replace('row_', ''))\n",
        "                    available_rows.append(row_num)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        available_rows = sorted(available_rows)\n",
        "        if max_rows:\n",
        "            available_rows = available_rows[:max_rows]\n",
        "\n",
        "        print(f\"Processing {len(available_rows)} rows\")\n",
        "\n",
        "        total_entropy_values = 0\n",
        "        total_matrices = 0\n",
        "\n",
        "        # Process each row\n",
        "        for row_idx in tqdm(available_rows, desc='processing rows', leave=False):\n",
        "            # Process each prompt\n",
        "            for prompt_name in ['prompt_1', 'prompt_2', 'prompt_3']:\n",
        "                prompt_dir = os.path.join(self.data_dir, f\"row_{row_idx}\", prompt_name)\n",
        "                if not os.path.exists(prompt_dir):\n",
        "                    continue\n",
        "\n",
        "                # Get individual phrase hint positions\n",
        "                row_prompt_key = f\"row_{row_idx}_{prompt_name}\"\n",
        "                phrase_hint_positions = {}\n",
        "\n",
        "                for phrase in self.analysis_keys:\n",
        "                    key = f\"{row_idx}_Prompt_{prompt_name.split('_')[1]}\"\n",
        "                    phrase_hint_positions[phrase] = self.hint_positions_individual[phrase].get(key, [])\n",
        "\n",
        "                # Get available layers\n",
        "                layer_files = [f for f in os.listdir(prompt_dir) if f.startswith('attn_layer_') and f.endswith('.npy')]\n",
        "                layer_indices = sorted([int(f.replace('attn_layer_', '').replace('.npy', '')) for f in layer_files])\n",
        "\n",
        "                if not layer_indices:\n",
        "                    continue\n",
        "\n",
        "                # Load first matrix to get dimensions\n",
        "                first_file = os.path.join(prompt_dir, f\"attn_layer_{layer_indices[0]}.npy\")\n",
        "                sample_matrix = np.load(first_file)\n",
        "                _, num_heads, seq_len, _ = sample_matrix.shape\n",
        "                num_layers = len(layer_indices)\n",
        "\n",
        "                # Initialize entropies[ℓ][h][i] array for this row_prompt\n",
        "                entropies = np.zeros((num_layers, num_heads, seq_len))\n",
        "\n",
        "                # Process each layer ℓ\n",
        "                for layer_position, layer_idx in enumerate(layer_indices):\n",
        "                    file_path = os.path.join(prompt_dir, f\"attn_layer_{layer_idx}.npy\")\n",
        "\n",
        "                    try:\n",
        "                        attention_matrix = np.load(file_path)\n",
        "\n",
        "                        # Compute entropy\n",
        "                        entropy_result = self.compute_attention_entropy(attention_matrix)\n",
        "\n",
        "                        if entropy_result:\n",
        "                            layer_entropies = entropy_result['entropies']  # [heads, seq_len]\n",
        "\n",
        "                            # Store in entropies[ℓ][h][i] format\n",
        "                            entropies[layer_position, :, :] = layer_entropies\n",
        "\n",
        "                            total_matrices += 1\n",
        "                            total_entropy_values += layer_entropies.size\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing layer {layer_idx}: {e}\")\n",
        "\n",
        "                # Store the complete entropies[ℓ][h][i] array (same for all analyses)\n",
        "                self.entropies[row_prompt_key] = entropies\n",
        "\n",
        "                # Extract individual phrase-specific entropies\n",
        "                for phrase, hint_positions in phrase_hint_positions.items():\n",
        "                    phrase_entropy_values = []\n",
        "\n",
        "                    if hint_positions:\n",
        "                        for layer_pos in range(num_layers):\n",
        "                            for head_idx in range(num_heads):\n",
        "                                for pos in hint_positions:\n",
        "                                    if pos < seq_len:\n",
        "                                        phrase_entropy_values.append(entropies[layer_pos, head_idx, pos])\n",
        "\n",
        "                    self.hint_entropies_individual[phrase][row_prompt_key] = phrase_entropy_values\n",
        "\n",
        "            # if (row_idx + 1) % 25 == 0:\n",
        "            #     print(f\"Processed {row_idx + 1} rows...\")\n",
        "\n",
        "        print(f\"Individual phrase entropy computation complete!\")\n",
        "        print(f\"Processed {total_matrices} attention matrices\")\n",
        "        print(f\"Computed {total_entropy_values} entropy values\")\n",
        "\n",
        "        # Print summary for each phrase\n",
        "        print(\"\\nPhrase-specific entropy summary:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            total_values = sum(len(values) for values in self.hint_entropies_individual[phrase].values())\n",
        "            non_empty_instances = sum(1 for values in self.hint_entropies_individual[phrase].values() if len(values) > 0)\n",
        "            print(f\"  {phrase}: {total_values} entropy values, {non_empty_instances} instances with data\")\n",
        "\n",
        "        return {\n",
        "            'total_matrices': total_matrices,\n",
        "            'total_entropy_values': total_entropy_values,\n",
        "            'rows_processed': len(available_rows),\n",
        "            'phrase_analyses': len(self.analysis_keys)\n",
        "        }\n",
        "\n",
        "\n",
        "    def save_results(self, output_dir: str = ENTROPY_RESULTS_DIR):\n",
        "        \"\"\"\n",
        "        Save individual phrase entropy computation results\n",
        "\n",
        "        Output format: entropies[ℓ][h][i] arrays with individual phrase subsets\n",
        "        \"\"\"\n",
        "        print(f\"Saving individual phrase entropy results to {output_dir}/\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Convert numpy arrays to lists for JSON serialization\n",
        "        entropies_for_json = {}\n",
        "        for key, entropy_array in self.entropies.items():\n",
        "            entropies_for_json[key] = entropy_array.tolist()\n",
        "\n",
        "        # Save complete entropies[ℓ][h][i] arrays (same for all analyses)\n",
        "        entropy_file = os.path.join(output_dir, \"entropies_arrays.json\")\n",
        "        with open(entropy_file, 'w') as f:\n",
        "            json.dump(entropies_for_json, f, indent=2)\n",
        "\n",
        "        # Save individual phrase hint entropies\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(output_dir, f\"hint_entropies_{phrase}.json\")\n",
        "            with open(phrase_file, 'w') as f:\n",
        "                json.dump(self.hint_entropies_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Save individual phrase hint positions for reference\n",
        "        for phrase in self.analysis_keys:\n",
        "            positions_file = os.path.join(output_dir, f\"hint_positions_{phrase}.json\")\n",
        "            with open(positions_file, 'w') as f:\n",
        "                json.dump(self.hint_positions_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Create comprehensive summary\n",
        "        summary = {\n",
        "            'methodology': 'H_i^(ℓ,h) = -∑_j A_ij log(A_ij)',\n",
        "            'analysis_structure': {\n",
        "                'individual_phrases': [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"],\n",
        "                'combined_analysis': \"combined (all phrases together)\",\n",
        "                'total_analyses': len(self.analysis_keys)\n",
        "            },\n",
        "            'storage_format': {\n",
        "                'entropies': 'entropies[ℓ][h][i] arrays (layer, head, query)',\n",
        "                'hint_entropies': 'phrase-specific: [entropies[ℓ][h][pos] for pos in phrase_positions]'\n",
        "            },\n",
        "            'phrase_statistics': {},\n",
        "            'total_prompt_instances': len(self.entropies),\n",
        "            'arrays_saved': list(self.entropies.keys())\n",
        "        }\n",
        "\n",
        "        # Add phrase statistics\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_data = self.hint_entropies_individual[phrase]\n",
        "            total_entropy_values = sum(len(values) for values in phrase_data.values())\n",
        "            non_empty_instances = sum(1 for values in phrase_data.values() if len(values) > 0)\n",
        "\n",
        "            summary['phrase_statistics'][phrase] = {\n",
        "                'total_entropy_values': total_entropy_values,\n",
        "                'prompt_instances_with_phrase': non_empty_instances,\n",
        "                'total_prompt_instances': len(phrase_data),\n",
        "                'coverage_percentage': (non_empty_instances / len(phrase_data)) * 100 if len(phrase_data) > 0 else 0\n",
        "            }\n",
        "\n",
        "        summary_file = os.path.join(output_dir, \"entropy_summary_individual.json\")\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase entropy computation completed successfully!\")\n",
        "        print(f\"Generated analyses for: {', '.join(self.analysis_keys)}\")\n",
        "        print(\"Files generated:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"  - hint_entropies_{phrase}.json\")\n",
        "            print(f\"  - hint_positions_{phrase}.json\")\n",
        "        print(\"Ready for individual phrase aggregation analysis\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_entropy_computation_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase entropy computation pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Starting Individual Phrase Entropy Computation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize entropy computation\n",
        "    entropy_computer = EntropyComputationIndividual(\n",
        "        prepared_data_dir=PREPARED_DATA_DIR,\n",
        "        data_dir=DATA_DIR\n",
        "    )\n",
        "\n",
        "    # Compute entropy for dataset\n",
        "    stats = entropy_computer.compute_entropy_for_dataset_individual()\n",
        "\n",
        "    # Save results for next stage\n",
        "    output_dir = entropy_computer.save_results()\n",
        "\n",
        "    print(f\"\\nIndividual phrase entropy computation completed: {output_dir}\")\n",
        "    print(\"Ready for individual phrase aggregation analysis\")\n",
        "\n",
        "    return entropy_computer, stats\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    entropy_computer, stats = run_entropy_computation_individual()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregate Entropy - Modified to accomodate chunked entropies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qgsi0FBj2usI",
        "outputId": "0e746a6d-fa17-40cf-c08a-f89f92c5fb35"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "class AggregationAnalysisIndividual:\n",
        "    def __init__(self, entropy_results_dirs: List[str] = None, entropy_results_dir: str = None):\n",
        "        \"\"\"\n",
        "        Aggregate and Compute Changes for Individual Phrases (6 + 1)\n",
        "        \n",
        "        Now supports multiple directories with different row ranges.\n",
        "\n",
        "        Args:\n",
        "            entropy_results_dirs: List of directories containing entropy results for different row ranges\n",
        "            entropy_results_dir: Single directory (for backward compatibility)\n",
        "        \"\"\"\n",
        "        # Handle both single directory and multiple directories\n",
        "        if entropy_results_dirs is not None:\n",
        "            self.entropy_results_dirs = entropy_results_dirs\n",
        "        elif entropy_results_dir is not None:\n",
        "            self.entropy_results_dirs = [entropy_results_dir]\n",
        "        else:\n",
        "            # Default to looking for directories matching the pattern\n",
        "            self.entropy_results_dirs = self._find_entropy_directories()\n",
        "        \n",
        "        print(\"Starting Individual Phrase Aggregation Analysis\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Processing {len(self.entropy_results_dirs)} directories:\")\n",
        "        for dir_path in self.entropy_results_dirs:\n",
        "            print(f\"  - {dir_path}\")\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "\n",
        "        # Load individual phrase entropy data from all directories\n",
        "        self.load_entropy_data()\n",
        "\n",
        "        # Storage for aggregated results\n",
        "        self.aggregated_entropies_individual = {}  # {phrase: {row_prompt: value}}\n",
        "        self.delta_h_changes_individual = {}       # {phrase: {row_idx: {comparison: delta_h}}}\n",
        "\n",
        "        # Initialize storage for each phrase\n",
        "        for phrase in self.analysis_keys:\n",
        "            self.aggregated_entropies_individual[phrase] = {}\n",
        "            self.delta_h_changes_individual[phrase] = {}\n",
        "\n",
        "    def _find_entropy_directories(self) -> List[str]:\n",
        "        \"\"\"Find all entropy_results_individual_* directories in the current path\"\"\"\n",
        "        pattern = re.compile(r'entropy_results_individual_\\d+_\\d+')\n",
        "        dirs = []\n",
        "        for item in os.listdir('.'):\n",
        "            if os.path.isdir(item) and pattern.match(item):\n",
        "                dirs.append(item)\n",
        "        return sorted(dirs)  # Sort to ensure consistent ordering\n",
        "\n",
        "    def _extract_row_range(self, dir_name: str) -> Tuple[int, int]:\n",
        "        \"\"\"Extract the row range from directory name like 'entropy_results_individual_300_599'\"\"\"\n",
        "        match = re.search(r'(\\d+)_(\\d+)$', dir_name)\n",
        "        if match:\n",
        "            start_row = int(match.group(1))\n",
        "            end_row = int(match.group(2))\n",
        "            return start_row, end_row\n",
        "        else:\n",
        "            # Default case for directories without range in name\n",
        "            return 0, None\n",
        "\n",
        "    def _adjust_entropy_key(self, key: str, row_offset: int) -> str:\n",
        "        \"\"\"Adjust row number in entropy key format: 'row_0_prompt_1' -> 'row_300_prompt_1'\"\"\"\n",
        "        match = re.match(r'row_(\\d+)_(.+)', key)\n",
        "        if match:\n",
        "            old_row = int(match.group(1))\n",
        "            new_row = old_row + row_offset\n",
        "            return f'row_{new_row}_{match.group(2)}'\n",
        "        return key\n",
        "\n",
        "    def _adjust_position_key(self, key: str, row_offset: int) -> str:\n",
        "        \"\"\"Adjust row number in position key format: '0_Prompt_1' -> '300_Prompt_1'\"\"\"\n",
        "        match = re.match(r'(\\d+)_(.+)', key)\n",
        "        if match:\n",
        "            old_row = int(match.group(1))\n",
        "            new_row = old_row + row_offset\n",
        "            return f'{new_row}_{match.group(2)}'\n",
        "        return key\n",
        "\n",
        "    def load_entropy_data(self):\n",
        "        \"\"\"Load individual phrase entropy data from all directories\"\"\"\n",
        "        self.hint_entropies_individual = {}\n",
        "        self.hint_positions_individual = {}\n",
        "\n",
        "        # Initialize storage for each phrase\n",
        "        for phrase in self.analysis_keys:\n",
        "            self.hint_entropies_individual[phrase] = {}\n",
        "            self.hint_positions_individual[phrase] = {}\n",
        "\n",
        "        # Process each directory\n",
        "        for dir_path in self.entropy_results_dirs:\n",
        "            print(f\"\\nProcessing directory: {dir_path}\")\n",
        "            \n",
        "            # Extract row range from directory name\n",
        "            start_row, end_row = self._extract_row_range(dir_path)\n",
        "            row_offset = start_row\n",
        "            print(f\"  Row range: {start_row} to {end_row if end_row else 'unknown'}\")\n",
        "\n",
        "            for phrase in self.analysis_keys:\n",
        "                # Load hint entropies\n",
        "                entropy_file = os.path.join(dir_path, f\"hint_entropies_{phrase}.json\")\n",
        "                if os.path.exists(entropy_file):\n",
        "                    with open(entropy_file, 'r') as f:\n",
        "                        data = json.load(f)\n",
        "                    \n",
        "                    # Adjust keys if needed (only if row_offset > 0)\n",
        "                    if row_offset > 0:\n",
        "                        adjusted_data = {}\n",
        "                        for key, value in data.items():\n",
        "                            new_key = self._adjust_entropy_key(key, row_offset)\n",
        "                            adjusted_data[new_key] = value\n",
        "                        data = adjusted_data\n",
        "                    \n",
        "                    # Merge with existing data\n",
        "                    self.hint_entropies_individual[phrase].update(data)\n",
        "                    print(f\"  Loaded {len(data)} entries for '{phrase}' entropies\")\n",
        "                else:\n",
        "                    print(f\"  Warning: {entropy_file} not found\")\n",
        "\n",
        "                # Load hint positions for reference\n",
        "                positions_file = os.path.join(dir_path, f\"hint_positions_{phrase}.json\")\n",
        "                if os.path.exists(positions_file):\n",
        "                    with open(positions_file, 'r') as f:\n",
        "                        data = json.load(f)\n",
        "                    \n",
        "                    # Adjust keys if needed\n",
        "                    if row_offset > 0:\n",
        "                        adjusted_data = {}\n",
        "                        for key, value in data.items():\n",
        "                            new_key = self._adjust_position_key(key, row_offset)\n",
        "                            adjusted_data[new_key] = value\n",
        "                        data = adjusted_data\n",
        "                    \n",
        "                    # Merge with existing data\n",
        "                    self.hint_positions_individual[phrase].update(data)\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\nTotal data loaded across all directories:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"  {phrase}: {len(self.hint_entropies_individual[phrase])} prompt instances\")\n",
        "\n",
        "    def compute_individual_phrase_aggregations(self):\n",
        "        \"\"\"\n",
        "        Compute aggregations for each individual phrase (6 + 1)\n",
        "        H_phrase = (1/N) ∑_positions H_i^(ℓ,h) where N = number of phrase positions\n",
        "        \"\"\"\n",
        "        print(\"\\nComputing individual phrase aggregations...\")\n",
        "        print(f\"Analyzing: {', '.join(self.analysis_keys)}\")\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"Processing {phrase} aggregations...\")\n",
        "\n",
        "            # Phrase-specific aggregation\n",
        "            for row_prompt_key, entropy_values in self.hint_entropies_individual[phrase].items():\n",
        "                if entropy_values:\n",
        "                    # H_phrase = average entropy at phrase positions\n",
        "                    avg_entropy = np.mean(entropy_values)\n",
        "                    self.aggregated_entropies_individual[phrase][row_prompt_key] = float(avg_entropy)\n",
        "                else:\n",
        "                    self.aggregated_entropies_individual[phrase][row_prompt_key] = None\n",
        "\n",
        "            # Print statistics for this phrase\n",
        "            valid_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            if valid_values:\n",
        "                print(f\"  {phrase}: {len(valid_values)} valid instances, avg entropy = {np.mean(valid_values):.4f}\")\n",
        "            else:\n",
        "                print(f\"  {phrase}: No valid instances found\")\n",
        "\n",
        "    def compute_individual_phrase_delta_h(self):\n",
        "        \"\"\"\n",
        "        Compute ΔH changes for each individual phrase (6 + 1)\n",
        "        ΔH = H_baseline - H_variant per phrase\n",
        "        \"\"\"\n",
        "        print(\"\\nComputing individual phrase ΔH changes...\")\n",
        "\n",
        "        phrase_delta_h_stats = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"Computing ΔH for {phrase}...\")\n",
        "\n",
        "            # Group data by row\n",
        "            rows_data = defaultdict(dict)\n",
        "            for row_prompt_key, entropy_value in self.aggregated_entropies_individual[phrase].items():\n",
        "                parts = row_prompt_key.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    row_idx = int(parts[1])\n",
        "                    prompt_type = f\"{parts[2]}_{parts[3]}\"\n",
        "                    rows_data[row_idx][prompt_type] = entropy_value\n",
        "\n",
        "            # Compute ΔH for each row\n",
        "            phrase_delta_h_values = []\n",
        "\n",
        "            for row_idx, row_data in rows_data.items():\n",
        "                row_changes = {}\n",
        "\n",
        "                # Prompt_2 vs Prompt_1 comparison\n",
        "                if 'prompt_1' in row_data and 'prompt_2' in row_data:\n",
        "                    if row_data['prompt_1'] is not None and row_data['prompt_2'] is not None:\n",
        "                        delta_h = row_data['prompt_1'] - row_data['prompt_2']\n",
        "                        row_changes['prompt2_vs_prompt1'] = delta_h\n",
        "                        phrase_delta_h_values.append(delta_h)\n",
        "\n",
        "                # Prompt_3 vs Prompt_1 comparison\n",
        "                if 'prompt_1' in row_data and 'prompt_3' in row_data:\n",
        "                    if row_data['prompt_1'] is not None and row_data['prompt_3'] is not None:\n",
        "                        delta_h = row_data['prompt_1'] - row_data['prompt_3']\n",
        "                        row_changes['prompt3_vs_prompt1'] = delta_h\n",
        "                        phrase_delta_h_values.append(delta_h)\n",
        "\n",
        "                if row_changes:\n",
        "                    self.delta_h_changes_individual[phrase][row_idx] = row_changes\n",
        "\n",
        "            # Calculate statistics for this phrase\n",
        "            if phrase_delta_h_values:\n",
        "                phrase_delta_h_stats[phrase] = {\n",
        "                    'mean': float(np.mean(phrase_delta_h_values)),\n",
        "                    'std': float(np.std(phrase_delta_h_values)),\n",
        "                    'negative_percentage': (sum(1 for x in phrase_delta_h_values if x < 0) / len(phrase_delta_h_values)) * 100,\n",
        "                    'count': len(phrase_delta_h_values)\n",
        "                }\n",
        "\n",
        "        # Print ΔH summary for each phrase\n",
        "        print(\"\\nΔH Summary by Individual Phrase:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            if phrase in phrase_delta_h_stats:\n",
        "                stats = phrase_delta_h_stats[phrase]\n",
        "                print(f\"  {phrase}: Mean ΔH = {stats['mean']:.4f}, Negative% = {stats['negative_percentage']:.1f}%, Count = {stats['count']}\")\n",
        "            else:\n",
        "                print(f\"  {phrase}: No valid ΔH values\")\n",
        "\n",
        "        return phrase_delta_h_stats\n",
        "\n",
        "    def compute_cross_phrase_comparison(self):\n",
        "        \"\"\"\n",
        "        Compare bias effects across individual phrases to identify most problematic bias types\n",
        "        \"\"\"\n",
        "        print(\"\\nComputing cross-phrase comparison...\")\n",
        "\n",
        "        comparison_results = {}\n",
        "\n",
        "        # Collect aggregated entropy for each phrase\n",
        "        phrase_entropies = {}\n",
        "        phrase_delta_h = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Collect entropy values\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            if entropy_values:\n",
        "                phrase_entropies[phrase] = {\n",
        "                    'mean': float(np.mean(entropy_values)),\n",
        "                    'std': float(np.std(entropy_values)),\n",
        "                    'count': len(entropy_values)\n",
        "                }\n",
        "\n",
        "            # Collect ΔH values\n",
        "            all_delta_h = []\n",
        "            for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                all_delta_h.extend(row_changes.values())\n",
        "\n",
        "            if all_delta_h:\n",
        "                phrase_delta_h[phrase] = {\n",
        "                    'mean': float(np.mean(all_delta_h)),\n",
        "                    'std': float(np.std(all_delta_h)),\n",
        "                    'negative_percentage': (sum(1 for x in all_delta_h if x < 0) / len(all_delta_h)) * 100,\n",
        "                    'abs_mean': float(np.mean(np.abs(all_delta_h))),\n",
        "                    'count': len(all_delta_h)\n",
        "                }\n",
        "\n",
        "        comparison_results = {\n",
        "            'phrase_entropy_comparison': phrase_entropies,\n",
        "            'phrase_delta_h_comparison': phrase_delta_h,\n",
        "            'ranking_by_bias_strength': {},\n",
        "            'cross_phrase_insights': {}\n",
        "        }\n",
        "\n",
        "        # Rank phrases by bias strength (absolute mean ΔH)\n",
        "        if phrase_delta_h:\n",
        "            sorted_by_bias = sorted(phrase_delta_h.items(), key=lambda x: x[1]['abs_mean'], reverse=True)\n",
        "            comparison_results['ranking_by_bias_strength'] = {\n",
        "                'most_biased_phrases': [phrase for phrase, _ in sorted_by_bias[:3]],\n",
        "                'least_biased_phrases': [phrase for phrase, _ in sorted_by_bias[-3:]],\n",
        "                'full_ranking': [(phrase, stats['abs_mean']) for phrase, stats in sorted_by_bias]\n",
        "            }\n",
        "\n",
        "        # Generate insights\n",
        "        if phrase_delta_h:\n",
        "            # Find phrases with strongest negative bias (most unfaithful)\n",
        "            negative_bias_phrases = [\n",
        "                phrase for phrase, stats in phrase_delta_h.items()\n",
        "                if stats['negative_percentage'] > 70 and stats['mean'] < -0.001\n",
        "            ]\n",
        "\n",
        "            # Find phrases with positive bias (more faithful)\n",
        "            positive_bias_phrases = [\n",
        "                phrase for phrase, stats in phrase_delta_h.items()\n",
        "                if stats['negative_percentage'] < 30 and stats['mean'] > 0.001\n",
        "            ]\n",
        "\n",
        "            comparison_results['cross_phrase_insights'] = {\n",
        "                'strongly_negative_bias_phrases': negative_bias_phrases,\n",
        "                'positive_bias_phrases': positive_bias_phrases,\n",
        "                'interpretation': {\n",
        "                    'negative_bias': 'These phrases cause more focused/unfaithful attention',\n",
        "                    'positive_bias': 'These phrases cause more distributed/faithful attention'\n",
        "                }\n",
        "            }\n",
        "\n",
        "        print(\"\\nCross-phrase comparison completed!\")\n",
        "        if 'ranking_by_bias_strength' in comparison_results:\n",
        "            print(\"Phrases ranked by bias strength (most biased first):\")\n",
        "            for i, (phrase, bias_strength) in enumerate(comparison_results['ranking_by_bias_strength']['full_ranking'][:5], 1):\n",
        "                print(f\"  {i}. {phrase}: {bias_strength:.4f}\")\n",
        "\n",
        "        return comparison_results\n",
        "\n",
        "    def save_results(self, output_dir: str = AGGREGATED_ANALYSIS_DIR):\n",
        "        \"\"\"Save all individual phrase aggregation and change results\"\"\"\n",
        "        print(f\"\\nSaving individual phrase aggregation results to {output_dir}/\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save metadata about processed directories\n",
        "        metadata = {\n",
        "            'processed_directories': self.entropy_results_dirs,\n",
        "            'total_directories': len(self.entropy_results_dirs),\n",
        "            'analysis_keys': self.analysis_keys\n",
        "        }\n",
        "        metadata_file = os.path.join(output_dir, \"processing_metadata.json\")\n",
        "        with open(metadata_file, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "\n",
        "        # Save individual phrase aggregated entropies\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(output_dir, f\"aggregated_entropies_{phrase}.json\")\n",
        "            with open(phrase_file, 'w') as f:\n",
        "                json.dump(self.aggregated_entropies_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Save individual phrase ΔH changes\n",
        "        for phrase in self.analysis_keys:\n",
        "            delta_file = os.path.join(output_dir, f\"delta_h_changes_{phrase}.json\")\n",
        "            with open(delta_file, 'w') as f:\n",
        "                json.dump(self.delta_h_changes_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Compute and save ΔH statistics\n",
        "        phrase_delta_h_stats = self.compute_individual_phrase_delta_h()\n",
        "        stats_file = os.path.join(output_dir, \"phrase_delta_h_statistics.json\")\n",
        "        with open(stats_file, 'w') as f:\n",
        "            json.dump(phrase_delta_h_stats, f, indent=2)\n",
        "\n",
        "        # Compute and save cross-phrase comparison\n",
        "        cross_phrase_comparison = self.compute_cross_phrase_comparison()\n",
        "        comparison_file = os.path.join(output_dir, \"cross_phrase_comparison.json\")\n",
        "        with open(comparison_file, 'w') as f:\n",
        "            json.dump(cross_phrase_comparison, f, indent=2)\n",
        "\n",
        "        # Create comprehensive summary\n",
        "        summary = {\n",
        "            'analysis_structure': {\n",
        "                'individual_phrases': [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"],\n",
        "                'combined_analysis': \"combined\",\n",
        "                'total_analyses': len(self.analysis_keys),\n",
        "                'directories_processed': self.entropy_results_dirs\n",
        "            },\n",
        "            'aggregation_methodology': {\n",
        "                'phrase_aggregation': 'H_phrase = (1/N) ∑_positions H_i^(ℓ,h)',\n",
        "                'change_calculation': 'ΔH = H_baseline - H_variant per phrase'\n",
        "            },\n",
        "            'interpretation': {\n",
        "                'negative_delta_h': 'Variants more focused/sharper at this phrase (unfaithful)',\n",
        "                'positive_delta_h': 'Variants less focused at this phrase (more faithful)',\n",
        "                'cross_phrase_comparison': 'Identifies which bias phrases are most problematic'\n",
        "            },\n",
        "            'data_counts': {\n",
        "                'analyses_completed': len(self.analysis_keys),\n",
        "                'aggregation_files': len(self.analysis_keys),\n",
        "                'delta_h_files': len(self.analysis_keys),\n",
        "                'comparison_analysis_available': True,\n",
        "                'total_directories_processed': len(self.entropy_results_dirs)\n",
        "            },\n",
        "            'files_generated': {\n",
        "                'per_phrase_aggregated_entropies': [f\"aggregated_entropies_{phrase}.json\" for phrase in self.analysis_keys],\n",
        "                'per_phrase_delta_h_changes': [f\"delta_h_changes_{phrase}.json\" for phrase in self.analysis_keys],\n",
        "                'summary_files': ['phrase_delta_h_statistics.json', 'cross_phrase_comparison.json', 'processing_metadata.json']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        summary_file = os.path.join(output_dir, \"aggregation_summary_individual.json\")\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase aggregation analysis completed successfully!\")\n",
        "        print(f\"Processed {len(self.entropy_results_dirs)} directories\")\n",
        "        print(f\"Generated {len(self.analysis_keys)} separate phrase analyses:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"  - {phrase}: aggregated_entropies_{phrase}.json, delta_h_changes_{phrase}.json\")\n",
        "        print(\"Additional files:\")\n",
        "        print(\"  - phrase_delta_h_statistics.json\")\n",
        "        print(\"  - cross_phrase_comparison.json\")\n",
        "        print(\"  - processing_metadata.json\")\n",
        "        print(\"Ready for individual phrase statistical analysis\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_aggregation_analysis_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase aggregation analysis pipeline\n",
        "    \"\"\"\n",
        "    print(\"Starting Individual Phrase Aggregation Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # # Option 1: Automatically find all matching directories\n",
        "    # processor = AggregationAnalysisIndividual()\n",
        "    \n",
        "    # Option 2: Explicitly specify directories\n",
        "    processor = AggregationAnalysisIndividual(\n",
        "        entropy_results_dirs=[\n",
        "            \"../data/entropy_results_dirs/entropy_results_individual_0_299\",\n",
        "            \"../data/entropy_results_dirs/entropy_results_individual_300_599\",\n",
        "            \"../data/entropy_results_dirs/entropy_results_individual_600_899\",\n",
        "            \"../data/entropy_results_dirs/entropy_results_individual_900_1199\",\n",
        "            \"../data/entropy_results_dirs/entropy_results_individual_1200_1499\",\n",
        "            \"../data/entropy_results_dirs/entropy_results_individual_1500_1799\",\n",
        "            \"../data/entropy_results_dirs/entropy_results_individual_1800_2015\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Compute individual phrase aggregations\n",
        "    processor.compute_individual_phrase_aggregations()\n",
        "\n",
        "    # Compute ΔH changes for each phrase\n",
        "    delta_h_stats = processor.compute_individual_phrase_delta_h()\n",
        "\n",
        "    # Compute cross-phrase comparison\n",
        "    cross_phrase_results = processor.compute_cross_phrase_comparison()\n",
        "\n",
        "    # Save all results\n",
        "    output_dir = processor.save_results()\n",
        "\n",
        "    print(f\"\\nIndividual phrase aggregation analysis completed: {output_dir}\")\n",
        "\n",
        "    return processor, delta_h_stats, cross_phrase_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processor, delta_h_stats, cross_phrase_results = run_aggregation_analysis_individual()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform Individual Statistical Analysis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iM5arwsF2vgL",
        "outputId": "799a2a6f-41b0-4f89-9027-d133faf95d06"
      },
      "outputs": [],
      "source": [
        "class StatisticalAnalysisIndividual:\n",
        "    def __init__(self, aggregated_analysis_dir: str, results_csv: str = None):\n",
        "        \"\"\"\n",
        "        Analyze and Compare Results for Individual Phrases (6 + 1)\n",
        "\n",
        "        Methodology:\n",
        "        - Individual phrase patterns: High/low entropy per phrase = faithful/unfaithful\n",
        "        - Cross-phrase statistical tests: Compare bias effects between phrases\n",
        "        - Phrase-specific correlations: Entropy vs accuracy per phrase\n",
        "        - Ranking analysis: Identify most problematic bias phrases\n",
        "\n",
        "        Args:\n",
        "            aggregated_analysis_dir: Directory containing individual phrase aggregation results\n",
        "            results_csv: Optional path to results CSV for accuracy correlation\n",
        "        \"\"\"\n",
        "        self.aggregated_analysis_dir = aggregated_analysis_dir\n",
        "        self.results_csv = results_csv\n",
        "\n",
        "        print(\"Starting Individual Phrase Statistical Analysis\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "\n",
        "        # Load individual phrase aggregation results\n",
        "        self.load_aggregation_data()\n",
        "\n",
        "        # Storage for analysis results\n",
        "        self.analysis_results = {\n",
        "            'individual_phrase_patterns': {},\n",
        "            'cross_phrase_statistical_tests': {},\n",
        "            'phrase_specific_correlations': {},\n",
        "            'bias_ranking_analysis': {},\n",
        "            'error_checks': {}\n",
        "        }\n",
        "\n",
        "    def load_aggregation_data(self):\n",
        "        \"\"\"Load individual phrase aggregated entropy data and ΔH changes\"\"\"\n",
        "        self.aggregated_entropies_individual = {}\n",
        "        self.delta_h_changes_individual = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Load aggregated entropies\n",
        "            agg_file = os.path.join(self.aggregated_analysis_dir, f\"aggregated_entropies_{phrase}.json\")\n",
        "            if os.path.exists(agg_file):\n",
        "                with open(agg_file, 'r') as f:\n",
        "                    self.aggregated_entropies_individual[phrase] = json.load(f)\n",
        "                print(f\"Loaded aggregated entropies for '{phrase}': {len(self.aggregated_entropies_individual[phrase])} instances\")\n",
        "            else:\n",
        "                print(f\"Warning: {agg_file} not found\")\n",
        "                self.aggregated_entropies_individual[phrase] = {}\n",
        "\n",
        "            # Load ΔH changes\n",
        "            delta_file = os.path.join(self.aggregated_analysis_dir, f\"delta_h_changes_{phrase}.json\")\n",
        "            if os.path.exists(delta_file):\n",
        "                with open(delta_file, 'r') as f:\n",
        "                    self.delta_h_changes_individual[phrase] = json.load(f)\n",
        "            else:\n",
        "                print(f\"Warning: {delta_file} not found\")\n",
        "                self.delta_h_changes_individual[phrase] = {}\n",
        "\n",
        "        # Load cross-phrase comparison\n",
        "        comparison_file = os.path.join(self.aggregated_analysis_dir, \"cross_phrase_comparison.json\")\n",
        "        if os.path.exists(comparison_file):\n",
        "            with open(comparison_file, 'r') as f:\n",
        "                self.cross_phrase_comparison = json.load(f)\n",
        "        else:\n",
        "            print(\"Warning: cross_phrase_comparison.json not found\")\n",
        "            self.cross_phrase_comparison = {}\n",
        "\n",
        "        print(f\"Loaded individual phrase data for {len(self.aggregated_entropies_individual)} analyses\")\n",
        "\n",
        "        # Optionally load results CSV for accuracy correlation\n",
        "        if self.results_csv and os.path.exists(self.results_csv):\n",
        "            try:\n",
        "                self.results_df = pd.read_csv(self.results_csv)\n",
        "                print(f\"Loaded results CSV: {len(self.results_df)} rows\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not load results CSV: {e}\")\n",
        "                self.results_df = None\n",
        "        else:\n",
        "            self.results_df = None\n",
        "\n",
        "    def analyze_individual_phrase_patterns(self):\n",
        "        \"\"\"\n",
        "        Pattern Analysis: High/low entropy per phrase = faithful/unfaithful\n",
        "        \"\"\"\n",
        "        print(\"Analyzing individual phrase patterns...\")\n",
        "\n",
        "        individual_patterns = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"Analyzing {phrase} patterns...\")\n",
        "\n",
        "            # Entropy patterns for this phrase\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "\n",
        "            if entropy_values:\n",
        "                individual_patterns[phrase] = {\n",
        "                    'entropy_stats': {\n",
        "                        'mean': float(np.mean(entropy_values)),\n",
        "                        'std': float(np.std(entropy_values)),\n",
        "                        'min': float(np.min(entropy_values)),\n",
        "                        'max': float(np.max(entropy_values)),\n",
        "                        'count': len(entropy_values),\n",
        "                        'interpretation': f'Higher values indicate more faithful attention at {phrase} positions'\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                # Analyze distribution by prompt type\n",
        "                entropy_by_prompt = {'prompt_1': [], 'prompt_2': [], 'prompt_3': []}\n",
        "                for key, value in self.aggregated_entropies_individual[phrase].items():\n",
        "                    if value is not None:\n",
        "                        for prompt_type in entropy_by_prompt.keys():\n",
        "                            if prompt_type in key:\n",
        "                                entropy_by_prompt[prompt_type].append(value)\n",
        "\n",
        "                individual_patterns[phrase]['entropy_by_prompt_type'] = {}\n",
        "                for prompt_type, values in entropy_by_prompt.items():\n",
        "                    if values:\n",
        "                        individual_patterns[phrase]['entropy_by_prompt_type'][prompt_type] = {\n",
        "                            'mean': float(np.mean(values)),\n",
        "                            'std': float(np.std(values)),\n",
        "                            'count': len(values)\n",
        "                        }\n",
        "\n",
        "                # ΔH patterns for this phrase\n",
        "                if phrase in self.delta_h_changes_individual:\n",
        "                    all_delta_h = []\n",
        "                    delta_h_by_comparison = {}\n",
        "\n",
        "                    for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                        for change_key, change_value in row_changes.items():\n",
        "                            if isinstance(change_value, (int, float)):\n",
        "                                all_delta_h.append(change_value)\n",
        "\n",
        "                                if change_key not in delta_h_by_comparison:\n",
        "                                    delta_h_by_comparison[change_key] = []\n",
        "                                delta_h_by_comparison[change_key].append(change_value)\n",
        "\n",
        "                    if all_delta_h:\n",
        "                        individual_patterns[phrase]['delta_h_patterns'] = {\n",
        "                            'overall': {\n",
        "                                'mean': float(np.mean(all_delta_h)),\n",
        "                                'std': float(np.std(all_delta_h)),\n",
        "                                'negative_count': sum(1 for x in all_delta_h if x < 0),\n",
        "                                'positive_count': sum(1 for x in all_delta_h if x > 0),\n",
        "                                'total_count': len(all_delta_h),\n",
        "                                'negative_percentage': (sum(1 for x in all_delta_h if x < 0) / len(all_delta_h)) * 100,\n",
        "                                'interpretation': f'Negative ΔH = variants more focused at {phrase} positions (unfaithful)'\n",
        "                            },\n",
        "                            'by_comparison': {}\n",
        "                        }\n",
        "\n",
        "                        for comparison, values in delta_h_by_comparison.items():\n",
        "                            individual_patterns[phrase]['delta_h_patterns']['by_comparison'][comparison] = {\n",
        "                                'mean': float(np.mean(values)),\n",
        "                                'std': float(np.std(values)),\n",
        "                                'negative_percentage': (sum(1 for x in values if x < 0) / len(values)) * 100,\n",
        "                                'count': len(values)\n",
        "                            }\n",
        "\n",
        "        self.analysis_results['individual_phrase_patterns'] = individual_patterns\n",
        "        return individual_patterns\n",
        "\n",
        "    def perform_cross_phrase_statistical_tests(self):\n",
        "        \"\"\"\n",
        "        Cross-phrase Statistical Tests: Compare bias effects between phrases\n",
        "        \"\"\"\n",
        "        print(\"Performing cross-phrase statistical tests...\")\n",
        "\n",
        "        cross_phrase_tests = {}\n",
        "\n",
        "        # Collect entropy values for each phrase\n",
        "        phrase_entropies = {}\n",
        "        phrase_delta_h = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Entropy values\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            if entropy_values:\n",
        "                phrase_entropies[phrase] = entropy_values\n",
        "\n",
        "            # ΔH values\n",
        "            delta_h_values = []\n",
        "            if phrase in self.delta_h_changes_individual:\n",
        "                for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                    delta_h_values.extend(row_changes.values())\n",
        "            if delta_h_values:\n",
        "                phrase_delta_h[phrase] = delta_h_values\n",
        "\n",
        "        # Pairwise comparisons between phrases (entropy)\n",
        "        cross_phrase_tests['entropy_comparisons'] = {}\n",
        "        phrase_names = list(phrase_entropies.keys())\n",
        "\n",
        "        for i in range(len(phrase_names)):\n",
        "            for j in range(i + 1, len(phrase_names)):\n",
        "                phrase1, phrase2 = phrase_names[i], phrase_names[j]\n",
        "\n",
        "                try:\n",
        "                    t_stat, p_value = ttest_ind(phrase_entropies[phrase1], phrase_entropies[phrase2])\n",
        "                    cross_phrase_tests['entropy_comparisons'][f\"{phrase1}_vs_{phrase2}\"] = {\n",
        "                        't_statistic': float(t_stat),\n",
        "                        'p_value': float(p_value),\n",
        "                        'significant': p_value < 0.05,\n",
        "                        'mean_diff': float(np.mean(phrase_entropies[phrase1]) - np.mean(phrase_entropies[phrase2])),\n",
        "                        'effect_size': abs(np.mean(phrase_entropies[phrase1]) - np.mean(phrase_entropies[phrase2])) /\n",
        "                                      np.sqrt((np.var(phrase_entropies[phrase1]) + np.var(phrase_entropies[phrase2])) / 2),\n",
        "                        'interpretation': f'Comparing entropy between {phrase1} and {phrase2} bias phrases'\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"Entropy comparison failed for {phrase1} vs {phrase2}: {e}\")\n",
        "\n",
        "        # Pairwise comparisons between phrases (ΔH)\n",
        "        cross_phrase_tests['delta_h_comparisons'] = {}\n",
        "        delta_h_phrase_names = list(phrase_delta_h.keys())\n",
        "\n",
        "        for i in range(len(delta_h_phrase_names)):\n",
        "            for j in range(i + 1, len(delta_h_phrase_names)):\n",
        "                phrase1, phrase2 = delta_h_phrase_names[i], delta_h_phrase_names[j]\n",
        "\n",
        "                try:\n",
        "                    t_stat, p_value = ttest_ind(phrase_delta_h[phrase1], phrase_delta_h[phrase2])\n",
        "                    cross_phrase_tests['delta_h_comparisons'][f\"{phrase1}_vs_{phrase2}\"] = {\n",
        "                        't_statistic': float(t_stat),\n",
        "                        'p_value': float(p_value),\n",
        "                        'significant': p_value < 0.05,\n",
        "                        'mean_diff': float(np.mean(phrase_delta_h[phrase1]) - np.mean(phrase_delta_h[phrase2])),\n",
        "                        'effect_size': abs(np.mean(phrase_delta_h[phrase1]) - np.mean(phrase_delta_h[phrase2])) /\n",
        "                                      np.sqrt((np.var(phrase_delta_h[phrase1]) + np.var(phrase_delta_h[phrase2])) / 2),\n",
        "                        'interpretation': f'Comparing ΔH bias effects between {phrase1} and {phrase2}'\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"ΔH comparison failed for {phrase1} vs {phrase2}: {e}\")\n",
        "\n",
        "        # One-way ANOVA test across all phrases\n",
        "        if len(phrase_entropies) > 2:\n",
        "            try:\n",
        "                entropy_groups = [phrase_entropies[phrase] for phrase in phrase_entropies.keys()]\n",
        "                h_stat, p_value = kruskal(*entropy_groups)\n",
        "                cross_phrase_tests['overall_entropy_test'] = {\n",
        "                    'test': 'Kruskal-Wallis H-test (non-parametric ANOVA)',\n",
        "                    'h_statistic': float(h_stat),\n",
        "                    'p_value': float(p_value),\n",
        "                    'significant': p_value < 0.05,\n",
        "                    'interpretation': 'Tests if entropy distributions differ significantly across phrases',\n",
        "                    'phrases_tested': list(phrase_entropies.keys())\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Overall entropy test failed: {e}\")\n",
        "\n",
        "        # One-way test for ΔH across all phrases\n",
        "        if len(phrase_delta_h) > 2:\n",
        "            try:\n",
        "                delta_h_groups = [phrase_delta_h[phrase] for phrase in phrase_delta_h.keys()]\n",
        "                h_stat, p_value = kruskal(*delta_h_groups)\n",
        "                cross_phrase_tests['overall_delta_h_test'] = {\n",
        "                    'test': 'Kruskal-Wallis H-test (non-parametric ANOVA)',\n",
        "                    'h_statistic': float(h_stat),\n",
        "                    'p_value': float(p_value),\n",
        "                    'significant': p_value < 0.05,\n",
        "                    'interpretation': 'Tests if ΔH distributions differ significantly across phrases',\n",
        "                    'phrases_tested': list(phrase_delta_h.keys())\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Overall ΔH test failed: {e}\")\n",
        "\n",
        "        # One-sample t-tests for each phrase ΔH against zero\n",
        "        cross_phrase_tests['phrase_vs_zero_tests'] = {}\n",
        "        for phrase, delta_h_values in phrase_delta_h.items():\n",
        "            if len(delta_h_values) >= 3:\n",
        "                try:\n",
        "                    t_stat, p_value = ttest_1samp(delta_h_values, 0)\n",
        "                    cross_phrase_tests['phrase_vs_zero_tests'][phrase] = {\n",
        "                        't_statistic': float(t_stat),\n",
        "                        'p_value': float(p_value),\n",
        "                        'significant': p_value < 0.05,\n",
        "                        'mean_delta_h': float(np.mean(delta_h_values)),\n",
        "                        'interpretation': f'Testing if {phrase} ΔH is significantly different from zero'\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"One-sample test failed for {phrase}: {e}\")\n",
        "\n",
        "        self.analysis_results['cross_phrase_statistical_tests'] = cross_phrase_tests\n",
        "        return cross_phrase_tests\n",
        "\n",
        "    def perform_phrase_specific_correlations(self):\n",
        "        \"\"\"\n",
        "        Phrase-specific Correlations: Entropy vs accuracy per phrase\n",
        "        \"\"\"\n",
        "        print(\"Performing phrase-specific correlations...\")\n",
        "\n",
        "        phrase_correlations = {}\n",
        "\n",
        "        if self.results_df is not None:\n",
        "            try:\n",
        "                for phrase in self.analysis_keys:\n",
        "                    print(f\"Computing correlations for {phrase}...\")\n",
        "\n",
        "                    correlation_data = []\n",
        "\n",
        "                    for index, row in self.results_df.iterrows():\n",
        "                        row_idx = row.get('Index', index)\n",
        "\n",
        "                        for prompt_col in ['Prompt_1', 'Prompt_2', 'Prompt_3']:\n",
        "                            prompt_key = f\"row_{row_idx}_{prompt_col.lower()}\"\n",
        "\n",
        "                            # Get phrase-specific entropy\n",
        "                            phrase_entropy = self.aggregated_entropies_individual[phrase].get(prompt_key)\n",
        "\n",
        "                            prediction_col = f\"{prompt_col}_Prediction_Extracted\"\n",
        "                            if prediction_col in row and pd.notna(row[prediction_col]):\n",
        "                                prediction = str(row[prediction_col])\n",
        "                                has_numerical_answer = prediction.replace('.', '').isdigit()\n",
        "\n",
        "                                if phrase_entropy is not None:\n",
        "                                    correlation_data.append({\n",
        "                                        'phrase_entropy': phrase_entropy,\n",
        "                                        'has_numerical_answer': has_numerical_answer,\n",
        "                                        'prediction': prediction,\n",
        "                                        'prompt_type': prompt_col\n",
        "                                    })\n",
        "\n",
        "                    if len(correlation_data) > 3:\n",
        "                        corr_df = pd.DataFrame(correlation_data)\n",
        "\n",
        "                        try:\n",
        "                            # Phrase entropy vs accuracy\n",
        "                            rho, p_value = spearmanr(\n",
        "                                corr_df['phrase_entropy'],\n",
        "                                corr_df['has_numerical_answer'].astype(int)\n",
        "                            )\n",
        "\n",
        "                            phrase_correlations[phrase] = {\n",
        "                                'entropy_vs_accuracy': {\n",
        "                                    'spearman_rho': float(rho),\n",
        "                                    'p_value': float(p_value),\n",
        "                                    'significant': p_value < 0.05,\n",
        "                                    'interpretation': f'Correlation between {phrase} entropy and prediction accuracy'\n",
        "                                },\n",
        "                                'data_summary': {\n",
        "                                    'total_data_points': len(corr_df),\n",
        "                                    'entropy_range': [float(corr_df['phrase_entropy'].min()), float(corr_df['phrase_entropy'].max())],\n",
        "                                    'accuracy_rate': float(corr_df['has_numerical_answer'].mean()),\n",
        "                                    'prompt_type_distribution': corr_df['prompt_type'].value_counts().to_dict()\n",
        "                                }\n",
        "                            }\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Correlation calculation failed for {phrase}: {e}\")\n",
        "                            phrase_correlations[phrase] = {'error': str(e)}\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Phrase-specific correlation analysis failed: {e}\")\n",
        "                phrase_correlations['error'] = str(e)\n",
        "\n",
        "        else:\n",
        "            phrase_correlations['note'] = 'Results CSV not available - correlation analysis skipped'\n",
        "\n",
        "        self.analysis_results['phrase_specific_correlations'] = phrase_correlations\n",
        "        return phrase_correlations\n",
        "\n",
        "    def perform_bias_ranking_analysis(self):\n",
        "        \"\"\"\n",
        "        Bias Ranking Analysis: Identify most problematic bias phrases\n",
        "        \"\"\"\n",
        "        print(\"Performing bias ranking analysis...\")\n",
        "\n",
        "        ranking_analysis = {}\n",
        "\n",
        "        # Collect metrics for ranking\n",
        "        phrase_metrics = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            if phrase == 'combined':\n",
        "                continue  # Skip combined for individual ranking\n",
        "\n",
        "            # Get ΔH values\n",
        "            all_delta_h = []\n",
        "            if phrase in self.delta_h_changes_individual:\n",
        "                for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                    all_delta_h.extend(row_changes.values())\n",
        "\n",
        "            # Get entropy values\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "\n",
        "            if all_delta_h and entropy_values:\n",
        "                phrase_metrics[phrase] = {\n",
        "                    'mean_delta_h': float(np.mean(all_delta_h)),\n",
        "                    'abs_mean_delta_h': float(np.mean(np.abs(all_delta_h))),\n",
        "                    'negative_percentage': (sum(1 for x in all_delta_h if x < 0) / len(all_delta_h)) * 100,\n",
        "                    'delta_h_std': float(np.std(all_delta_h)),\n",
        "                    'mean_entropy': float(np.mean(entropy_values)),\n",
        "                    'entropy_std': float(np.std(entropy_values)),\n",
        "                    'data_coverage': len(all_delta_h)\n",
        "                }\n",
        "\n",
        "        # Ranking by different criteria\n",
        "        if phrase_metrics:\n",
        "            # 1. Most biased (highest absolute ΔH)\n",
        "            most_biased = sorted(phrase_metrics.items(), key=lambda x: x[1]['abs_mean_delta_h'], reverse=True)\n",
        "\n",
        "            # 2. Most unfaithful (most negative ΔH)\n",
        "            most_unfaithful = sorted(phrase_metrics.items(), key=lambda x: x[1]['mean_delta_h'])\n",
        "\n",
        "            # 3. Most consistent bias (highest negative percentage)\n",
        "            most_consistent_bias = sorted(phrase_metrics.items(), key=lambda x: x[1]['negative_percentage'], reverse=True)\n",
        "\n",
        "            # 4. Lowest average entropy (most focused attention)\n",
        "            lowest_entropy = sorted(phrase_metrics.items(), key=lambda x: x[1]['mean_entropy'])\n",
        "\n",
        "            ranking_analysis = {\n",
        "                'phrase_metrics': phrase_metrics,\n",
        "                'rankings': {\n",
        "                    'most_biased_phrases': {\n",
        "                        'ranking': [(phrase, metrics['abs_mean_delta_h']) for phrase, metrics in most_biased],\n",
        "                        'top_3': [phrase for phrase, _ in most_biased[:3]],\n",
        "                        'criterion': 'Highest absolute mean ΔH'\n",
        "                    },\n",
        "                    'most_unfaithful_phrases': {\n",
        "                        'ranking': [(phrase, metrics['mean_delta_h']) for phrase, metrics in most_unfaithful],\n",
        "                        'top_3': [phrase for phrase, _ in most_unfaithful[:3]],\n",
        "                        'criterion': 'Most negative mean ΔH'\n",
        "                    },\n",
        "                    'most_consistent_bias': {\n",
        "                        'ranking': [(phrase, metrics['negative_percentage']) for phrase, metrics in most_consistent_bias],\n",
        "                        'top_3': [phrase for phrase, _ in most_consistent_bias[:3]],\n",
        "                        'criterion': 'Highest percentage of negative ΔH values'\n",
        "                    },\n",
        "                    'lowest_entropy_phrases': {\n",
        "                        'ranking': [(phrase, metrics['mean_entropy']) for phrase, metrics in lowest_entropy],\n",
        "                        'top_3': [phrase for phrase, _ in lowest_entropy[:3]],\n",
        "                        'criterion': 'Lowest average entropy (most focused attention)'\n",
        "                    }\n",
        "                },\n",
        "                'summary_insights': {\n",
        "                    'most_problematic_overall': most_biased[0][0] if most_biased else None,\n",
        "                    'strongest_unfaithfulness': most_unfaithful[0][0] if most_unfaithful else None,\n",
        "                    'most_reliable_bias_indicator': most_consistent_bias[0][0] if most_consistent_bias else None\n",
        "                }\n",
        "            }\n",
        "\n",
        "        print(\"\\nBias ranking analysis completed!\")\n",
        "        if 'rankings' in ranking_analysis:\n",
        "            print(\"Top 3 most biased phrases:\")\n",
        "            for i, phrase in enumerate(ranking_analysis['rankings']['most_biased_phrases']['top_3'], 1):\n",
        "                bias_score = phrase_metrics[phrase]['abs_mean_delta_h']\n",
        "                print(f\"  {i}. {phrase}: {bias_score:.4f}\")\n",
        "\n",
        "        self.analysis_results['bias_ranking_analysis'] = ranking_analysis\n",
        "        return ranking_analysis\n",
        "\n",
        "    def perform_error_checking(self):\n",
        "        \"\"\"\n",
        "        Error Check: Handle missing data, validate phrase coverage\n",
        "        \"\"\"\n",
        "        print(\"Performing error checking for individual phrases...\")\n",
        "\n",
        "        error_checks = {}\n",
        "\n",
        "        # Check data coverage for each phrase\n",
        "        phrase_coverage = {}\n",
        "        for phrase in self.analysis_keys:\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            total_instances = len(self.aggregated_entropies_individual[phrase])\n",
        "\n",
        "            delta_h_count = 0\n",
        "            if phrase in self.delta_h_changes_individual:\n",
        "                delta_h_count = sum(len(row_changes) for row_changes in self.delta_h_changes_individual[phrase].values())\n",
        "\n",
        "            phrase_coverage[phrase] = {\n",
        "                'entropy_instances': len(entropy_values),\n",
        "                'total_possible_instances': total_instances,\n",
        "                'entropy_coverage_rate': len(entropy_values) / total_instances if total_instances > 0 else 0,\n",
        "                'delta_h_comparisons': delta_h_count,\n",
        "                'zero_entropy_count': sum(1 for v in entropy_values if v == 0.0),\n",
        "                'very_low_entropy_count': sum(1 for v in entropy_values if 0 < v < 0.1),\n",
        "                'very_high_entropy_count': sum(1 for v in entropy_values if v > 5.0)\n",
        "            }\n",
        "\n",
        "        error_checks['phrase_coverage'] = phrase_coverage\n",
        "\n",
        "        # Identify phrases with insufficient data\n",
        "        low_coverage_phrases = [\n",
        "            phrase for phrase, stats in phrase_coverage.items()\n",
        "            if stats['entropy_coverage_rate'] < 0.1\n",
        "        ]\n",
        "\n",
        "        error_checks['data_quality_issues'] = {\n",
        "            'low_coverage_phrases': low_coverage_phrases,\n",
        "            'phrases_with_no_delta_h': [\n",
        "                phrase for phrase, stats in phrase_coverage.items()\n",
        "                if stats['delta_h_comparisons'] == 0\n",
        "            ],\n",
        "            'interpretation': 'Phrases with low coverage may have unreliable statistics'\n",
        "        }\n",
        "\n",
        "        # Cross-phrase consistency check\n",
        "        if len(self.analysis_keys) > 1:\n",
        "            entropy_ranges = {}\n",
        "            for phrase in self.analysis_keys:\n",
        "                entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "                if entropy_values:\n",
        "                    entropy_ranges[phrase] = {\n",
        "                        'min': float(np.min(entropy_values)),\n",
        "                        'max': float(np.max(entropy_values)),\n",
        "                        'range': float(np.max(entropy_values) - np.min(entropy_values))\n",
        "                    }\n",
        "\n",
        "            error_checks['cross_phrase_consistency'] = {\n",
        "                'entropy_ranges': entropy_ranges,\n",
        "                'range_consistency': 'Good' if len(set(round(stats['range'], 2) for stats in entropy_ranges.values())) <= 3 else 'Variable'\n",
        "            }\n",
        "\n",
        "        self.analysis_results['error_checks'] = error_checks\n",
        "        return error_checks\n",
        "\n",
        "    def run_complete_analysis(self):\n",
        "        \"\"\"Run the complete individual phrase statistical analysis pipeline\"\"\"\n",
        "        print(\"Running complete individual phrase statistical analysis...\")\n",
        "\n",
        "        # Run all analysis components\n",
        "        individual_patterns = self.analyze_individual_phrase_patterns()\n",
        "        cross_phrase_tests = self.perform_cross_phrase_statistical_tests()\n",
        "        phrase_correlations = self.perform_phrase_specific_correlations()\n",
        "        bias_ranking = self.perform_bias_ranking_analysis()\n",
        "        error_checks = self.perform_error_checking()\n",
        "\n",
        "        # Extract key findings\n",
        "        key_findings = self.extract_key_findings()\n",
        "\n",
        "        # Create comprehensive summary\n",
        "        summary = {\n",
        "            'components_completed': [\n",
        "                'individual_phrase_patterns',\n",
        "                'cross_phrase_statistical_tests',\n",
        "                'phrase_specific_correlations',\n",
        "                'bias_ranking_analysis',\n",
        "                'error_checks'\n",
        "            ],\n",
        "            'key_findings': key_findings,\n",
        "            'methodology_compliance': {\n",
        "                'individual_patterns_analyzed': 'High/low entropy faithfulness per phrase completed',\n",
        "                'cross_phrase_tests': 'Statistical comparisons between phrases completed',\n",
        "                'phrase_correlations': 'Entropy vs accuracy per phrase completed',\n",
        "                'bias_ranking': 'Most problematic phrases identified',\n",
        "                'error_checking': 'Data quality and coverage validated'\n",
        "            },\n",
        "            'analyses_completed': len(self.analysis_keys)\n",
        "        }\n",
        "\n",
        "        self.analysis_results['summary'] = summary\n",
        "\n",
        "        print(\"Complete individual phrase statistical analysis finished!\")\n",
        "        return self.analysis_results\n",
        "\n",
        "    def extract_key_findings(self):\n",
        "        \"\"\"Extract key findings from the individual phrase analysis\"\"\"\n",
        "        findings = {}\n",
        "\n",
        "        # Individual phrase findings\n",
        "        if 'individual_phrase_patterns' in self.analysis_results:\n",
        "            patterns = self.analysis_results['individual_phrase_patterns']\n",
        "\n",
        "            phrase_bias_scores = {}\n",
        "            for phrase, data in patterns.items():\n",
        "                if 'delta_h_patterns' in data and 'overall' in data['delta_h_patterns']:\n",
        "                    delta_patterns = data['delta_h_patterns']['overall']\n",
        "                    phrase_bias_scores[phrase] = {\n",
        "                        'negative_percentage': delta_patterns['negative_percentage'],\n",
        "                        'mean_delta_h': delta_patterns['mean'],\n",
        "                        'bias_detected': delta_patterns['negative_count'] > delta_patterns['positive_count']\n",
        "                    }\n",
        "\n",
        "            findings['individual_phrase_bias'] = phrase_bias_scores\n",
        "\n",
        "        # Cross-phrase comparison findings\n",
        "        if 'cross_phrase_statistical_tests' in self.analysis_results:\n",
        "            tests = self.analysis_results['cross_phrase_statistical_tests']\n",
        "\n",
        "            significant_comparisons = []\n",
        "            if 'entropy_comparisons' in tests:\n",
        "                significant_comparisons.extend([\n",
        "                    k for k, v in tests['entropy_comparisons'].items()\n",
        "                    if v.get('significant', False)\n",
        "                ])\n",
        "            if 'delta_h_comparisons' in tests:\n",
        "                significant_comparisons.extend([\n",
        "                    k for k, v in tests['delta_h_comparisons'].items()\n",
        "                    if v.get('significant', False)\n",
        "                ])\n",
        "\n",
        "            findings['cross_phrase_differences'] = {\n",
        "                'significant_comparisons': significant_comparisons,\n",
        "                'total_comparisons': len(tests.get('entropy_comparisons', {})) + len(tests.get('delta_h_comparisons', {})),\n",
        "                'evidence_strength': 'Strong' if len(significant_comparisons) > 2 else 'Moderate'\n",
        "            }\n",
        "\n",
        "        # Bias ranking findings\n",
        "        if 'bias_ranking_analysis' in self.analysis_results:\n",
        "            ranking = self.analysis_results['bias_ranking_analysis']\n",
        "\n",
        "            if 'rankings' in ranking:\n",
        "                findings['most_problematic_phrases'] = {\n",
        "                    'most_biased': ranking['rankings']['most_biased_phrases']['top_3'],\n",
        "                    'most_unfaithful': ranking['rankings']['most_unfaithful_phrases']['top_3'],\n",
        "                    'most_consistent_bias': ranking['rankings']['most_consistent_bias']['top_3']\n",
        "                }\n",
        "\n",
        "        return findings\n",
        "\n",
        "    def save_results(self, output_dir: str = STATISTICAL_RESULTS_DIR):\n",
        "        \"\"\"Save individual phrase statistical analysis results\"\"\"\n",
        "        print(f\"Saving individual phrase statistical results to {output_dir}/\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Convert numpy types to native Python types for JSON serialization\n",
        "        def convert_numpy_types(obj):\n",
        "            if isinstance(obj, dict):\n",
        "                return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
        "            elif isinstance(obj, list):\n",
        "                return [convert_numpy_types(v) for v in obj]\n",
        "            elif isinstance(obj, np.integer):\n",
        "                return int(obj)\n",
        "            elif isinstance(obj, np.floating):\n",
        "                return float(obj)\n",
        "            elif isinstance(obj, np.bool_):\n",
        "                return bool(obj)\n",
        "            elif isinstance(obj, np.ndarray):\n",
        "                return obj.tolist()\n",
        "            else:\n",
        "                return obj\n",
        "\n",
        "        # Convert analysis results\n",
        "        serializable_results = convert_numpy_types(self.analysis_results)\n",
        "\n",
        "        # Save complete analysis results\n",
        "        results_file = os.path.join(output_dir, \"statistical_analysis_individual_results.json\")\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump(serializable_results, f, indent=2)\n",
        "\n",
        "        # Save individual components\n",
        "        components = [\n",
        "            'individual_phrase_patterns',\n",
        "            'cross_phrase_statistical_tests',\n",
        "            'phrase_specific_correlations',\n",
        "            'bias_ranking_analysis',\n",
        "            'error_checks'\n",
        "        ]\n",
        "\n",
        "        for component in components:\n",
        "            if component in serializable_results:\n",
        "                component_file = os.path.join(output_dir, f\"{component}.json\")\n",
        "                with open(component_file, 'w') as f:\n",
        "                    json.dump(serializable_results[component], f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase statistical analysis completed successfully!\")\n",
        "        print(\"Ready for individual phrase results visualization\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_statistical_analysis_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase statistical analysis pipeline\n",
        "    \"\"\"\n",
        "    # Configuration - Update these paths for your setup\n",
        "\n",
        "    print(\"Starting Individual Phrase Statistical Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = StatisticalAnalysisIndividual(\n",
        "        aggregated_analysis_dir=AGGREGATED_ANALYSIS_DIR,\n",
        "        results_csv=RESULTS_CSV\n",
        "    )\n",
        "\n",
        "    # Run complete analysis\n",
        "    results = analyzer.run_complete_analysis()\n",
        "\n",
        "    # Save all results\n",
        "    output_dir = analyzer.save_results()\n",
        "\n",
        "    print(f\"\\nIndividual phrase statistical analysis completed: {output_dir}\")\n",
        "    return analyzer, results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer, results = run_statistical_analysis_individual()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPSuEU-R2zWH",
        "outputId": "89030947-c23d-4f07-ff17-18f51c2e6342"
      },
      "outputs": [],
      "source": [
        "class ResultsVisualizationIndividual:\n",
        "    def __init__(self, aggregated_analysis_dir: str, statistical_results_dir: str,\n",
        "                 entropy_results_dir: str):\n",
        "        \"\"\"\n",
        "        Visualize Results: Create 7 separate visualization files (1 combined + 6 individual phrases)\n",
        "\n",
        "        Each plot follows paste5.txt structure with 2×2 layout:\n",
        "        - Histogram: Global entropy distributions vs baseline (no hint)\n",
        "        - Line: Avg entropy vs layers (per variant)\n",
        "        - Bar: Avg phrase entropy (baseline, prompt_1, prompt_2, prompt_3)\n",
        "        - Heatmap: ΔH per layer/head\n",
        "\n",
        "        Args:\n",
        "            aggregated_analysis_dir: Directory containing individual phrase aggregation results\n",
        "            statistical_results_dir: Directory containing individual phrase statistical analysis results\n",
        "        \"\"\"\n",
        "        self.aggregated_analysis_dir = aggregated_analysis_dir\n",
        "        self.statistical_results_dir = statistical_results_dir\n",
        "        self.entropy_results_dir = entropy_results_dir\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "        self.individual_phrases = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"]\n",
        "        # self.individual_phrases = [\"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"]\n",
        "\n",
        "        # Load data from previous stages\n",
        "        self.load_visualization_data()\n",
        "\n",
        "        # Set up plotting style\n",
        "        self.setup_plotting_style()\n",
        "\n",
        "    def setup_plotting_style(self):\n",
        "        \"\"\"Set up matplotlib and seaborn styling for publication-ready plots\"\"\"\n",
        "        plt.style.use('default')\n",
        "        sns.set_palette(\"husl\")\n",
        "\n",
        "        # Publication-ready settings\n",
        "        plt.rcParams.update({\n",
        "            'figure.figsize': (15, 12),\n",
        "            'font.size': 10,\n",
        "            'axes.titlesize': 12,\n",
        "            'axes.labelsize': 11,\n",
        "            'xtick.labelsize': 9,\n",
        "            'ytick.labelsize': 9,\n",
        "            'legend.fontsize': 9,\n",
        "            'figure.titlesize': 14\n",
        "        })\n",
        "\n",
        "    def load_visualization_data(self):\n",
        "        import os\n",
        "        import json\n",
        "        import re\n",
        "        import numpy as np\n",
        "        \n",
        "        self.aggregated_entropies_individual = {}\n",
        "        self.delta_h_changes_individual = {}\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Load aggregated entropies\n",
        "            agg_file = os.path.join(self.aggregated_analysis_dir, f\"aggregated_entropies_{phrase}.json\")\n",
        "            if os.path.exists(agg_file):\n",
        "                with open(agg_file, 'r') as f:\n",
        "                    self.aggregated_entropies_individual[phrase] = json.load(f)\n",
        "            else:\n",
        "                print(f\"Warning: Aggregated entropies file not found: {agg_file}\")\n",
        "\n",
        "            # Load ΔH changes\n",
        "            delta_file = os.path.join(self.aggregated_analysis_dir, f\"delta_h_changes_{phrase}.json\")\n",
        "            if os.path.exists(delta_file):\n",
        "                with open(delta_file, 'r') as f:\n",
        "                    self.delta_h_changes_individual[phrase] = json.load(f)\n",
        "            else:\n",
        "                print(f\"Warning: Delta H changes file not found: {delta_file}\")\n",
        "\n",
        "        # Load entropy arrays from subdirectories\n",
        "        self.entropies_arrays = {}\n",
        "        subdirs = [d for d in os.listdir(self.entropy_results_dir) if d.startswith('entropy_results_individual_')]\n",
        "        \n",
        "        for subdir in subdirs:\n",
        "            match = re.match(r'entropy_results_individual_(\\d+)_(\\d+)', subdir)\n",
        "            if match:\n",
        "                start_row, end_row = map(int, match.groups())\n",
        "                json_path = os.path.join(self.entropy_results_dir, subdir, \"entropies_arrays.json\")\n",
        "                adjusted_json_path = os.path.join(self.entropy_results_dir, subdir, \"adjusted_entropies_arrays.json\")\n",
        "                \n",
        "                if os.path.exists(adjusted_json_path):\n",
        "                    with open(adjusted_json_path, 'r') as f:\n",
        "                        adjusted_data = json.load(f)\n",
        "                    self.entropies_arrays.update(adjusted_data)\n",
        "                    print(f\"Loaded precomputed adjusted data from {adjusted_json_path}\")\n",
        "                    del adjusted_data  # Free memory\n",
        "                elif os.path.exists(json_path):\n",
        "                    print(f\"Warning: Precomputed {adjusted_json_path} not found, computing from {json_path}\")\n",
        "                    with open(json_path, 'r') as f:\n",
        "                        data = json.load(f)\n",
        "                    \n",
        "                    adjusted_data = {}\n",
        "                    for key, value in data.items():\n",
        "                        row_match = re.match(r'row_(\\d+)_prompt_(\\d+)', key)\n",
        "                        if row_match:\n",
        "                            row_num, prompt_num = map(int, row_match.groups())\n",
        "                            adjusted_row_num = row_num # + start_row\n",
        "                            adjusted_key = f'row_{adjusted_row_num}_prompt_{prompt_num}'\n",
        "                            adjusted_data[adjusted_key] = value\n",
        "                        else:\n",
        "                            print(f\"Warning: Invalid key format in {json_path}: {key}\")\n",
        "                    \n",
        "                    with open(adjusted_json_path, 'w') as f:\n",
        "                        json.dump(adjusted_data, f)\n",
        "                    print(f\"Saved adjusted data to {adjusted_json_path}\")\n",
        "                    \n",
        "                    self.entropies_arrays.update(adjusted_data)\n",
        "                    del data, adjusted_data  # Free memory\n",
        "                else:\n",
        "                    print(f\"Warning: entropies_arrays.json not found at {json_path}\")\n",
        "            else:\n",
        "                print(f\"Warning: Invalid subdirectory name format: {subdir}\")\n",
        "\n",
        "        print(f\"Loaded visualization data for {len(self.aggregated_entropies_individual)} phrase analyses\")\n",
        "        print(f\"Loaded entropy arrays for {len(self.entropies_arrays)} row-prompt combinations\")\n",
        "\n",
        "    def _compute_prompt_data_stats(self, phrase: str):\n",
        "        \"\"\"Compute prompt data statistics (values, averages, and standard deviations) for a given phrase.\"\"\"\n",
        "        import numpy as np\n",
        "        import os\n",
        "        import json\n",
        "\n",
        "        # Define save paths\n",
        "        prompt_data_file = os.path.join(self.entropy_results_dir, f\"prompt_data_{phrase}.json\")\n",
        "        avg_entropies_file = os.path.join(self.entropy_results_dir, f\"avg_entropies_{phrase}.npy\")\n",
        "        std_entropies_file = os.path.join(self.entropy_results_dir, f\"std_entropies_{phrase}.npy\")\n",
        "\n",
        "        # # Load from disk if available\n",
        "        # if (os.path.exists(prompt_data_file) and \n",
        "        #     os.path.exists(avg_entropies_file) and \n",
        "        #     os.path.exists(std_entropies_file)):\n",
        "        #     with open(prompt_data_file, 'r') as f:\n",
        "        #         prompt_data = json.load(f)\n",
        "        #     avg_entropies = np.load(avg_entropies_file).tolist()\n",
        "        #     std_entropies = np.load(std_entropies_file).tolist()\n",
        "        #     print(f\"Loaded prompt data and stats from disk for {phrase}\")\n",
        "        #     return prompt_data, avg_entropies, std_entropies\n",
        "\n",
        "        # Initialize prompt data\n",
        "        prompt_data = {'prompt_1': [], 'prompt_2': [], 'prompt_3': []}\n",
        "\n",
        "        # Handle \"combined\" case or specific phrase\n",
        "        if phrase == \"combined\":\n",
        "            # Aggregate across all phrases in aggregated_entropies_individual\n",
        "            for phrase_key, entropy_dict in self.aggregated_entropies_individual.items():\n",
        "                for key, entropy_value in entropy_dict.items():\n",
        "                    if entropy_value is not None:\n",
        "                        for prompt_type in prompt_data.keys():\n",
        "                            if prompt_type in key:\n",
        "                                prompt_data[prompt_type].append(entropy_value)\n",
        "                del entropy_dict  # Free memory per phrase\n",
        "        else:\n",
        "            # Specific phrase\n",
        "            if phrase in self.aggregated_entropies_individual:\n",
        "                for key, entropy_value in self.aggregated_entropies_individual[phrase].items():\n",
        "                    if entropy_value is not None:\n",
        "                        for prompt_type in prompt_data.keys():\n",
        "                            if prompt_type in key:\n",
        "                                prompt_data[prompt_type].append(entropy_value)\n",
        "            else:\n",
        "                print(f\"Warning: Phrase '{phrase}' not found in aggregated_entropies_individual\")\n",
        "\n",
        "        # Calculate averages and standard deviations\n",
        "        avg_entropies = []\n",
        "        std_entropies = []\n",
        "        for values in prompt_data.values():\n",
        "            if values:\n",
        "                avg_entropies.append(np.mean(values))\n",
        "                std_entropies.append(np.std(values))\n",
        "            else:\n",
        "                avg_entropies.append(0)\n",
        "                std_entropies.append(0)\n",
        "\n",
        "        # Save to disk\n",
        "        with open(prompt_data_file, 'w') as f:\n",
        "            json.dump(prompt_data, f)\n",
        "        np.save(avg_entropies_file, np.array(avg_entropies))\n",
        "        np.save(std_entropies_file, np.array(std_entropies))\n",
        "        print(f\"Saved prompt data and stats to disk for {phrase}\")\n",
        "\n",
        "        return prompt_data, avg_entropies, std_entropies\n",
        "\n",
        "    def create_bar_chart_phrase_entropy(self, ax, phrase: str):\n",
        "        \"\"\"Bar: Avg phrase entropy (baseline, prompt_1, prompt_2, prompt_3)\"\"\"\n",
        "        print(f\"Creating bar chart for {phrase}: Avg phrase entropy by variant...\")\n",
        "        import numpy as np\n",
        "        import os\n",
        "        import json\n",
        "\n",
        "        # Get prompt data and precomputed statistics\n",
        "        prompt_data, avg_entropies, std_entropies = self._compute_prompt_data_stats(phrase)\n",
        "\n",
        "        # Calculate baseline entropy\n",
        "        baseline_file = os.path.join(self.entropy_results_dir, f\"baseline_entropies_bar_{phrase}.npy\")\n",
        "        if os.path.exists(baseline_file):\n",
        "            baseline_entropy = np.load(baseline_file).tolist()\n",
        "            print(f\"Loaded baseline entropies from {baseline_file}\")\n",
        "        else:\n",
        "            baseline_entropy = []\n",
        "            subdirs = [d for d in os.listdir(self.entropy_results_dir) if d.startswith('entropy_results_individual_')]\n",
        "            for subdir in subdirs:\n",
        "                json_path = os.path.join(self.entropy_results_dir, subdir, \"adjusted_entropies_arrays.json\")\n",
        "                if os.path.exists(json_path):\n",
        "                    with open(json_path, 'r') as f:\n",
        "                        data = json.load(f)\n",
        "                    for row_prompt_key, entropy_array in data.items():\n",
        "                        if 'prompt_1' in row_prompt_key:\n",
        "                            parts = row_prompt_key.split('_')\n",
        "                            if len(parts) == 4 and parts[0] == 'row' and parts[2] == 'prompt' and parts[3] == '1':\n",
        "                                entropies = np.array(entropy_array)\n",
        "                                global_entropy = np.mean(entropies)\n",
        "                                baseline_entropy.append(global_entropy)\n",
        "                                del entropies  # Free memory\n",
        "                            else:\n",
        "                                print(f\"Warning: Invalid key format skipped in {subdir}: {row_prompt_key}\")\n",
        "                    del data  # Free memory\n",
        "                else:\n",
        "                    print(f\"Warning: adjusted_entropies_arrays.json not found at {json_path}\")\n",
        "            if baseline_entropy:\n",
        "                np.save(baseline_file, np.array(baseline_entropy))\n",
        "                print(f\"Saved baseline entropies to {baseline_file}\")\n",
        "\n",
        "        # Insert baseline stats at the beginning\n",
        "        if baseline_entropy:\n",
        "            avg_entropies.insert(0, np.mean(baseline_entropy))\n",
        "            std_entropies.insert(0, np.std(baseline_entropy))\n",
        "        else:\n",
        "            avg_entropies.insert(0, 0)\n",
        "            std_entropies.insert(0, 0)\n",
        "\n",
        "        # Create bar chart\n",
        "        labels = ['Baseline (No Hint)', 'Biased: Prompt 1', 'Biased: Prompt 2', 'Biased: Prompt 3']\n",
        "        x = np.arange(len(labels))\n",
        "        ax.bar(x, avg_entropies, yerr=std_entropies, capsize=5, color=['#808080', '#1f77b4', '#ff7f0e', '#2ca02c'],\n",
        "            edgecolor='black', linewidth=0.5)\n",
        "\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(labels, rotation=45)\n",
        "        ax.set_ylabel('Average Entropy (bits)')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('Average Phrase Entropy by Prompt Variant\\n(All Bias Phrases)')\n",
        "        else:\n",
        "            ax.set_title(f'Average Phrase Entropy by Prompt Variant\\n(Phrase: \"{phrase}\")')\n",
        "        ax.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "        # Add value labels on top of bars\n",
        "        for i, v in enumerate(avg_entropies):\n",
        "            if v != 0:\n",
        "                ax.text(i, v + 0.01 * max(avg_entropies), f'{v:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    def create_histogram_phrase_entropy(self, ax, phrase: str):\n",
        "        \"\"\"Histogram: Phrase entropy distributions vs baseline (no hint)\"\"\"\n",
        "        print(f\"Creating histogram for {phrase}: Phrase entropy distributions vs baseline...\")\n",
        "        import numpy as np\n",
        "        import os\n",
        "        import json\n",
        "\n",
        "        # Get prompt data and precomputed statistics\n",
        "        prompt_data, _, _ = self._compute_prompt_data_stats(phrase)\n",
        "        print(f\"Available keys for {phrase}: {list(self.aggregated_entropies_individual[phrase].keys())}\")\n",
        "\n",
        "        # Get baseline entropies\n",
        "        baseline_file = os.path.join(self.entropy_results_dir, f\"baseline_entropies_histogram_{phrase}.npy\")\n",
        "        if os.path.exists(baseline_file):\n",
        "            baseline_entropies = np.load(baseline_file).tolist()\n",
        "            print(f\"Loaded baseline entropies from {baseline_file}\")\n",
        "        else:\n",
        "            baseline_entropies = []\n",
        "            subdirs = [d for d in os.listdir(self.entropy_results_dir) if d.startswith('entropy_results_individual_')]\n",
        "            for subdir in subdirs:\n",
        "                json_path = os.path.join(self.entropy_results_dir, subdir, \"adjusted_entropies_arrays.json\")\n",
        "                if os.path.exists(json_path):\n",
        "                    with open(json_path, 'r') as f:\n",
        "                        data = json.load(f)\n",
        "                    all_keys = list(data.keys())\n",
        "                    baseline_rows_local = set()\n",
        "\n",
        "                    for key in all_keys:\n",
        "                        if 'prompt_1' in key:\n",
        "                            parts = key.split('_')\n",
        "                            if len(parts) == 4 and parts[0] == 'row' and parts[2] == 'prompt' and parts[3] == '1':\n",
        "                                row_num = parts[1]\n",
        "                                has_prompt_2 = any(f'row_{row_num}_prompt_2' in k for k in all_keys)\n",
        "                                has_prompt_3 = any(f'row_{row_num}_prompt_3' in k for k in all_keys)\n",
        "                                if not has_prompt_2 and not has_prompt_3:\n",
        "                                    baseline_rows_local.add(row_num)\n",
        "\n",
        "                    for row_prompt_key, entropy_array in data.items():\n",
        "                        if 'prompt_1' in row_prompt_key:\n",
        "                            parts = row_prompt_key.split('_')\n",
        "                            if len(parts) == 4 and parts[0] == 'row' and parts[2] == 'prompt' and parts[3] == '1':\n",
        "                                row_num = parts[1]\n",
        "                                if row_num in baseline_rows_local:\n",
        "                                    entropies = np.array(entropy_array)\n",
        "                                    global_entropy = np.mean(entropies)\n",
        "                                    baseline_entropies.append(global_entropy)\n",
        "                                    del entropies  # Free memory\n",
        "                            else:\n",
        "                                print(f\"Warning: Invalid key format skipped: {row_prompt_key}\")\n",
        "                    del data  # Free memory\n",
        "                else:\n",
        "                    print(f\"Warning: adjusted_entropies_arrays.json not found at {json_path}\")\n",
        "            if baseline_entropies:\n",
        "                np.save(baseline_file, np.array(baseline_entropies))\n",
        "                print(f\"Saved baseline entropies to {baseline_file}\")\n",
        "\n",
        "        # Create histogram\n",
        "        colors = ['#808080', '#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "        labels = ['Baseline (No Hint)', 'Biased: Prompt 1', 'Biased: Prompt 2', 'Biased: Prompt 3']\n",
        "\n",
        "        all_values = []\n",
        "        for values in prompt_data.values():\n",
        "            if values:\n",
        "                all_values.extend(values)\n",
        "        if baseline_entropies:\n",
        "            all_values.extend(baseline_entropies)\n",
        "\n",
        "        if all_values:\n",
        "            bins = np.linspace(min(all_values), max(all_values), 15)\n",
        "\n",
        "            if baseline_entropies:\n",
        "                ax.hist(baseline_entropies, bins=bins, alpha=0.7, label=labels[0],\n",
        "                    color=colors[0], edgecolor='black', linewidth=0.5)\n",
        "\n",
        "            for i, (prompt_type, values) in enumerate(prompt_data.items(), 1):\n",
        "                if values:\n",
        "                    ax.hist(values, bins=bins, alpha=0.7, label=labels[i],\n",
        "                        color=colors[i], edgecolor='black', linewidth=0.5)\n",
        "\n",
        "        ax.set_xlabel('Entropy (bits)')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('Global Entropy Distributions vs Baseline\\n(All Bias Phrases)')\n",
        "        else:\n",
        "            ax.set_title(f'Entropy Distributions vs Baseline\\n(Phrase: \"{phrase}\")')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add summary statistics\n",
        "        stats_text = []\n",
        "        if baseline_entropies:\n",
        "            stats_text.append(f\"Baseline: μ={np.mean(baseline_entropies):.3f}\")\n",
        "        for prompt_type, values in prompt_data.items():\n",
        "            if values:\n",
        "                mean_val = np.mean(values)\n",
        "                stats_text.append(f\"{prompt_type}: μ={mean_val:.3f}\")\n",
        "\n",
        "        if stats_text:\n",
        "            ax.text(0.02, 0.98, '\\n'.join(stats_text), transform=ax.transAxes,\n",
        "                    verticalalignment='top', fontsize=8,\n",
        "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "   \n",
        "    def create_line_plot_entropy_vs_layers(self, ax, phrase: str):\n",
        "        \"\"\"Line: Avg entropy vs layers (per variant)\"\"\"\n",
        "        print(f\"Creating line plot for {phrase}: Avg entropy vs layers...\")\n",
        "        import numpy as np\n",
        "\n",
        "        # Compute layer-wise entropy data\n",
        "        layer_data, _ = self._compute_layer_data(phrase)\n",
        "\n",
        "        # Calculate averages and plot\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "        labels = ['Biased: Prompt 1', 'Biased: Prompt 2', 'Biased: Prompt 3']\n",
        "        linestyles = ['-', '--', '-.']\n",
        "\n",
        "        for i, (prompt_type, data) in enumerate(layer_data.items()):\n",
        "            if data:\n",
        "                layers = sorted(data.keys())\n",
        "                if layers:\n",
        "                    avg_entropies = [np.mean(data[layer]) for layer in layers]\n",
        "                    std_entropies = [np.std(data[layer]) for layer in layers]\n",
        "\n",
        "                    # Plot line with error bars\n",
        "                    ax.plot(layers, avg_entropies, color=colors[i], label=labels[i],\n",
        "                            linewidth=2, linestyle=linestyles[i], marker='o', markersize=4)\n",
        "\n",
        "                    # Add error bands\n",
        "                    ax.fill_between(layers,\n",
        "                                    [avg - std for avg, std in zip(avg_entropies, std_entropies)],\n",
        "                                    [avg + std for avg, std in zip(avg_entropies, std_entropies)],\n",
        "                                    alpha=0.2, color=colors[i])\n",
        "            else:\n",
        "                print(f\"Warning: No data for {prompt_type} in phrase '{phrase}'\")\n",
        "\n",
        "        ax.set_xlabel('Layer Index')\n",
        "        ax.set_ylabel('Average Entropy (bits)')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('Average Entropy vs Layers\\n(All Bias Phrases)')\n",
        "        else:\n",
        "            ax.set_title(f'Average Entropy vs Layers\\n(Phrase: \"{phrase}\")')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Highlight mid-layers if we have layer data\n",
        "        if any(layer_data.values()):\n",
        "            max_layers = max(max(data.keys()) if data else [0] for data in layer_data.values())\n",
        "            if max_layers > 0:\n",
        "                mid_start = max_layers // 3\n",
        "                mid_end = 2 * max_layers // 3\n",
        "                ax.axvspan(mid_start, mid_end, alpha=0.1, color='red', label='Mid-layers')\n",
        "                ax.legend()  # Update legend to include mid-layers\n",
        "        else:\n",
        "            print(f\"Warning: No layer data available to plot for phrase '{phrase}'\")\n",
        "\n",
        "    def _compute_layer_data(self, phrase: str):\n",
        "        import numpy as np\n",
        "        import os\n",
        "        import json\n",
        "        import re\n",
        "        print(\"Computing Layer data\")\n",
        "        layer_data_file = os.path.join(self.entropy_results_dir, f\"layer_data_{phrase}.json\")\n",
        "        positions_file_saved = os.path.join(self.entropy_results_dir, f\"phrase_positions_merged_{phrase}.json\")\n",
        "\n",
        "        # if os.path.exists(layer_data_file) and os.path.exists(positions_file_saved):\n",
        "        #     with open(layer_data_file, 'r') as f:\n",
        "        #         layer_data = json.load(f)\n",
        "        #     with open(positions_file_saved, 'r') as f:\n",
        "        #         phrase_positions_by_row_prompt = json.load(f)\n",
        "        #     print(f\"Loaded layer data and positions from disk for {phrase}\")\n",
        "        #     return layer_data, phrase_positions_by_row_prompt\n",
        "\n",
        "        # Initialize layer_data with correct structure\n",
        "        layer_data = {'prompt_1': {}, 'prompt_2': {}, 'prompt_3': {}}\n",
        "        phrase_positions_by_row_prompt = {}\n",
        "\n",
        "        # Get subdirectories\n",
        "        subdirs = [d for d in os.listdir(self.entropy_results_dir) if d.startswith('entropy_results_individual_')]\n",
        "        \n",
        "        # Get phrases to process\n",
        "        phrases = [phrase] if phrase != \"combined\" else []\n",
        "        print(f\"Phrases to process: {phrases}\")\n",
        "        if phrase == \"combined\":\n",
        "            for subdir in subdirs:\n",
        "                subdir_path = os.path.join(self.entropy_results_dir, subdir)\n",
        "                for f in os.listdir(subdir_path):\n",
        "                    if f.startswith(\"hint_positions_\") and f.endswith(\".json\") and f != \"hint_positions_combined.json\":\n",
        "                        phrase_name = f.replace(\"hint_positions_\", \"\").replace(\".json\", \"\")\n",
        "                        if phrase_name not in phrases:\n",
        "                            phrases.append(phrase_name)\n",
        "\n",
        "        # Collect phrase positions from subdirectories\n",
        "        for p in phrases:\n",
        "            print(f\"collecting phrase positions for phrase {p}\")\n",
        "            # Collect phrase positions from subdirectories with adjusted keys\n",
        "            cumulative_offset = 0\n",
        "            for subdir in sorted(subdirs):  # Sort for consistent offset\n",
        "                match = re.match(r'entropy_results_individual_(\\d+)_(\\d+)', subdir)\n",
        "                if match:\n",
        "                    start_row, end_row = map(int, match.groups())\n",
        "                    positions_file = os.path.join(self.entropy_results_dir, subdir, f\"hint_positions_{p}.json\")\n",
        "                    if os.path.exists(positions_file):\n",
        "                        with open(positions_file, 'r') as f:\n",
        "                            phrase_positions_data = json.load(f)\n",
        "                        # print(f\"Loading {positions_file}, keys: {list(phrase_positions_data.keys())[:5]}\")\n",
        "                        for key, positions in phrase_positions_data.items():\n",
        "                            parts = key.split('_')\n",
        "                            # print(parts)\n",
        "                            if len(parts) >= 3 and parts[0].isdigit() and parts[1].lower() == 'prompt':\n",
        "                                non_adjusted_row_num = int(parts[0])  # Original row number\n",
        "                                prompt_num = parts[2]\n",
        "                                adjusted_row_num = non_adjusted_row_num + cumulative_offset  # Apply same offset\n",
        "                                row_prompt_key = f\"row_{adjusted_row_num}_prompt_{prompt_num}\"\n",
        "                                phrase_positions_by_row_prompt[row_prompt_key] = positions\n",
        "                                # print(f\"Added {row_prompt_key} from {key} in {positions_file}, positions: {positions}\")\n",
        "                            else:\n",
        "                                print(f\"Warning: Skipped invalid key {key} in {positions_file}\")\n",
        "                        del phrase_positions_data  # Free memory\n",
        "                    cumulative_offset += (end_row - start_row + 1)\n",
        "                    # print(f\"Updated cumulative_offset to {cumulative_offset} after {subdir}\")\n",
        "                else:\n",
        "                    print(f\"Warning: Invalid subdirectory name format: {subdir}\")\n",
        "\n",
        "        # Compute layer-wise entropies\n",
        "        if hasattr(self, 'entropies_arrays') and phrase_positions_by_row_prompt:\n",
        "            for row_prompt_key, entropy_array in self.entropies_arrays.items():\n",
        "                parts = row_prompt_key.split('_')\n",
        "                if len(parts) == 4 and parts[0] == 'row' and parts[2] == 'prompt':\n",
        "                    prompt_num = parts[3]\n",
        "                    # print(f\"Processing row_prompt_key: {row_prompt_key}, prompt_num: {prompt_num}\")\n",
        "                    if row_prompt_key in phrase_positions_by_row_prompt:\n",
        "                        positions = phrase_positions_by_row_prompt[row_prompt_key]\n",
        "                        # print(f\"Matched positions for {row_prompt_key}: {positions if positions else 'No positions'}\")\n",
        "                        if positions:\n",
        "                            entropies = np.array(entropy_array)\n",
        "                            num_layers, num_heads, seq_len = entropies.shape\n",
        "                            for layer_idx in range(num_layers):\n",
        "                                layer_phrase_entropies = []\n",
        "                                for head_idx in range(num_heads):\n",
        "                                    for pos in positions:\n",
        "                                        if pos < seq_len:\n",
        "                                            layer_phrase_entropies.append(entropies[layer_idx, head_idx, pos])\n",
        "                                if layer_phrase_entropies:\n",
        "                                    if layer_idx not in layer_data[f'prompt_{prompt_num}']:\n",
        "                                        layer_data[f'prompt_{prompt_num}'][layer_idx] = []\n",
        "                                    layer_data[f'prompt_{prompt_num}'][layer_idx].extend(layer_phrase_entropies)\n",
        "                            del entropies  # Free memory\n",
        "                else:\n",
        "                    print(f\"Warning: Invalid key format skipped in entropies_arrays: {row_prompt_key}\")\n",
        "\n",
        "        # Clean up empty prompt types\n",
        "        layer_data = {k: v for k, v in layer_data.items() if v}\n",
        "        print(f\"Final prompt data = {layer_data}\")\n",
        "\n",
        "        with open(layer_data_file, 'w') as f:\n",
        "            json.dump(layer_data, f)\n",
        "        with open(positions_file_saved, 'w') as f:\n",
        "            json.dump(phrase_positions_by_row_prompt, f)\n",
        "        print(f\"Saved layer data and positions to disk for {phrase}\")\n",
        "\n",
        "        return layer_data, phrase_positions_by_row_prompt\n",
        "\n",
        "    def _compute_delta_h_matrix(self, phrase: str, num_layers: int = 16, num_heads: int = 32):\n",
        "        import numpy as np\n",
        "\n",
        "        matrix_file = os.path.join(self.entropy_results_dir, f\"delta_h_matrix_{phrase}.npy\")\n",
        "        counts_file = os.path.join(self.entropy_results_dir, f\"layer_counts_{phrase}.npy\")\n",
        "\n",
        "        if os.path.exists(matrix_file) and os.path.exists(counts_file):\n",
        "            delta_h_matrix = np.load(matrix_file)\n",
        "            layer_counts = np.load(counts_file)\n",
        "            print(f\"Loaded delta_h_matrix and layer_counts from disk for {phrase}\")\n",
        "            return delta_h_matrix, layer_counts\n",
        "\n",
        "        delta_h_matrix = np.zeros((num_layers, num_heads))\n",
        "        layer_counts = np.zeros((num_layers, num_heads))\n",
        "\n",
        "        layer_data, phrase_positions_by_row_prompt = self._compute_layer_data(phrase)\n",
        "\n",
        "        if layer_data and phrase_positions_by_row_prompt:\n",
        "            row_entropy_data = {}\n",
        "            for row_prompt_key, entropy_array in self.entropies_arrays.items():\n",
        "                if row_prompt_key in phrase_positions_by_row_prompt:\n",
        "                    positions = phrase_positions_by_row_prompt[row_prompt_key]\n",
        "                    if positions:\n",
        "                        parts = row_prompt_key.split('_')\n",
        "                        if len(parts) == 4 and parts[0] == 'row' and parts[2] == 'prompt':\n",
        "                            row_idx = int(parts[1])\n",
        "                            prompt_num = int(parts[3])\n",
        "\n",
        "                            if row_idx not in row_entropy_data:\n",
        "                                row_entropy_data[row_idx] = {}\n",
        "\n",
        "                            entropies = np.array(entropy_array)\n",
        "                            actual_layers, actual_heads, seq_len = entropies.shape\n",
        "\n",
        "                            prompt_layer_head_data = {}\n",
        "                            for layer_idx in range(min(actual_layers, num_layers)):\n",
        "                                prompt_layer_head_data[layer_idx] = {}\n",
        "                                for head_idx in range(min(actual_heads, num_heads)):\n",
        "                                    phrase_entropies = []\n",
        "                                    for pos in positions:\n",
        "                                        if pos < seq_len:\n",
        "                                            phrase_entropies.append(entropies[layer_idx, head_idx, pos])\n",
        "                                    if phrase_entropies:\n",
        "                                        prompt_layer_head_data[layer_idx][head_idx] = np.mean(phrase_entropies)\n",
        "                            row_entropy_data[row_idx][prompt_num] = prompt_layer_head_data\n",
        "                            del entropies, prompt_layer_head_data  # Free memory\n",
        "                        else:\n",
        "                            print(f\"Warning: Invalid key format skipped: {row_prompt_key}\")\n",
        "\n",
        "            for row_idx, prompt_data in row_entropy_data.items():\n",
        "                for baseline_prompt, variant_prompt in [(1, 2), (1, 3)]:\n",
        "                    if baseline_prompt in prompt_data and variant_prompt in prompt_data:\n",
        "                        baseline_data = prompt_data[baseline_prompt]\n",
        "                        variant_data = prompt_data[variant_prompt]\n",
        "\n",
        "                        for layer_idx in range(num_layers):\n",
        "                            if layer_idx in baseline_data and layer_idx in variant_data:\n",
        "                                for head_idx in range(num_heads):\n",
        "                                    if (head_idx in baseline_data[layer_idx] and\n",
        "                                        head_idx in variant_data[layer_idx]):\n",
        "                                        baseline_entropy = baseline_data[layer_idx][head_idx]\n",
        "                                        variant_entropy = variant_data[layer_idx][head_idx]\n",
        "                                        delta_h = baseline_entropy - variant_entropy\n",
        "\n",
        "                                        delta_h_matrix[layer_idx, head_idx] += delta_h\n",
        "                                        layer_counts[layer_idx, head_idx] += 1\n",
        "                # del row_entropy_data[row_idx]  # Free memory per row\n",
        "\n",
        "        mask = layer_counts == 0\n",
        "        if not mask.all():\n",
        "            delta_h_matrix[~mask] = delta_h_matrix[~mask] / layer_counts[~mask]\n",
        "\n",
        "        if mask.all() and phrase in self.delta_h_changes_individual:\n",
        "            all_delta_h = []\n",
        "            for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                all_delta_h.extend(row_changes.values())\n",
        "\n",
        "            if all_delta_h:\n",
        "                avg_delta_h = np.mean(all_delta_h)\n",
        "                std_delta_h = np.std(all_delta_h)\n",
        "\n",
        "                for layer_idx in range(num_layers):\n",
        "                    layer_weight = 1.0\n",
        "                    if 5 <= layer_idx <= 10:\n",
        "                        layer_weight = 1.5\n",
        "                    elif layer_idx < 3 or layer_idx > 13:\n",
        "                        layer_weight = 0.7\n",
        "\n",
        "                    for head_idx in range(num_heads):\n",
        "                        head_variation = np.random.normal(0, std_delta_h * 0.2)\n",
        "                        delta_h_matrix[layer_idx, head_idx] = avg_delta_h * layer_weight + head_variation\n",
        "                        layer_counts[layer_idx, head_idx] = len(all_delta_h)\n",
        "\n",
        "        np.save(matrix_file, delta_h_matrix)\n",
        "        np.save(counts_file, layer_counts)\n",
        "        print(f\"Saved delta_h_matrix and layer_counts to disk for {phrase}\")\n",
        "\n",
        "        return delta_h_matrix, layer_counts\n",
        "\n",
        "    def create_heatmap_delta_h(self, ax, phrase: str):\n",
        "        \"\"\"Heatmap: ΔH per layer/head for this phrase\"\"\"\n",
        "        print(f\"Creating heatmap for {phrase}: Computing layer-wise ΔH...\")\n",
        "        import numpy as np\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Compute ΔH matrix\n",
        "        num_layers, num_heads = 16, 32\n",
        "        delta_h_matrix, layer_counts = self._compute_delta_h_matrix(phrase, num_layers, num_heads)\n",
        "\n",
        "        # Add noise to show patterns if values are too uniform\n",
        "        mask = layer_counts == 0\n",
        "        if not mask.all():\n",
        "            non_zero_values = delta_h_matrix[~mask]\n",
        "            if len(non_zero_values) > 0 and np.std(non_zero_values) < 0.001:\n",
        "                noise = np.random.normal(0, np.abs(np.mean(non_zero_values)) * 0.1, delta_h_matrix.shape)\n",
        "                delta_h_matrix[~mask] += noise[~mask]\n",
        "\n",
        "        # Create heatmap\n",
        "        if not mask.all():\n",
        "            vmax = max(0.001, np.abs(delta_h_matrix[~mask]).max())\n",
        "        else:\n",
        "            vmax = 0.001\n",
        "\n",
        "        im = ax.imshow(delta_h_matrix, cmap='RdBu_r', aspect='auto', vmin=-vmax, vmax=vmax)\n",
        "        ax.invert_yaxis()\n",
        "\n",
        "        # Customize heatmap\n",
        "        ax.set_xlabel('Attention Head')\n",
        "        ax.set_ylabel('Layer')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('ΔH per Layer/Head\\n(All Bias Phrases)\\n(Red: More Focused, Blue: Less Focused)')\n",
        "        else:\n",
        "            ax.set_title(f'ΔH per Layer/Head\\n(Phrase: \"{phrase}\")\\n(Red: More Focused, Blue: Less Focused)')\n",
        "\n",
        "        # Set ticks\n",
        "        ax.set_xticks(range(0, num_heads, 4))\n",
        "        ax.set_xticklabels(range(0, num_heads, 4))\n",
        "        ax.set_yticks(range(num_layers))\n",
        "        ax.set_yticklabels(range(num_layers))\n",
        "\n",
        "        # Add colorbar\n",
        "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "        cbar.set_label('ΔH (H_baseline - H_variant)', rotation=270, labelpad=15)\n",
        "\n",
        "        # Add text annotations for significant values\n",
        "        if not mask.all():\n",
        "            for i in range(0, num_layers, 3):\n",
        "                for j in range(0, num_heads, 8):\n",
        "                    if not mask[i, j] and abs(delta_h_matrix[i, j]) > vmax * 0.3:\n",
        "                        ax.text(j, i, f'{delta_h_matrix[i, j]:.3f}',\n",
        "                            ha='center', va='center', fontsize=6,\n",
        "                            color='white' if abs(delta_h_matrix[i, j]) > vmax * 0.7 else 'black')\n",
        "\n",
        "        # Add statistics box\n",
        "        if not mask.all():\n",
        "            stats_text = f'Mean ΔH: {np.mean(delta_h_matrix[~mask]):.4f}\\nStd ΔH: {np.std(delta_h_matrix[~mask]):.4f}'\n",
        "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,\n",
        "                    verticalalignment='top', fontsize=8,\n",
        "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "        \n",
        "\n",
        "    def create_individual_phrase_plot(self, phrase: str, save_path: str):\n",
        "        \"\"\"Create individual phrase visualization (2x2 layout like paste5.txt)\"\"\"\n",
        "        print(f\"Creating individual visualization for phrase: {phrase}\")\n",
        "\n",
        "        # Create figure and subplots\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        if phrase == 'combined':\n",
        "            fig.suptitle('Attention Entropy Analysis: Combined Bias Detection\\n(All Bias Phrases Together)',\n",
        "                        fontsize=16, fontweight='bold', y=0.98)\n",
        "        else:\n",
        "            fig.suptitle(f'Attention Entropy Analysis: Individual Phrase Bias Detection\\nPhrase: \"{phrase}\"',\n",
        "                        fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "        # Top Left: Histogram - Phrase entropy distributions vs baseline\n",
        "        self.create_histogram_phrase_entropy(ax1, phrase)\n",
        "\n",
        "        # Top Right: Line Plot - Avg entropy vs layers\n",
        "        self.create_line_plot_entropy_vs_layers(ax2, phrase)\n",
        "\n",
        "        # Bottom Left: Bar Chart - Avg phrase entropy by variant\n",
        "        self.create_bar_chart_phrase_entropy(ax3, phrase)\n",
        "\n",
        "        # Bottom Right: Heatmap - ΔH per layer/head\n",
        "        self.create_heatmap_delta_h(ax4, phrase)\n",
        "\n",
        "        # Adjust layout\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "        # Save the plot\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "        print(f\"Saved {phrase} plot: {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "        return save_path\n",
        "\n",
        "    def run_complete_visualization(self, output_dir: str = VISUAL_RESULTS_DIR):\n",
        "        \"\"\"Run the complete individual phrase visualization pipeline - 7 separate PNG files\"\"\"\n",
        "        print(\"Running complete individual phrase visualization pipeline...\")\n",
        "        print(\"Generating 7 separate PNG files (1 combined + 6 individual phrases)\")\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        generated_files = []\n",
        "\n",
        "        # 1. Create combined analysis plot\n",
        "        # combined_path = os.path.join(output_dir, \"combined_bias_analysis.png\")\n",
        "        # combined_file = self.create_individual_phrase_plot(\"combined\", combined_path)\n",
        "        # generated_files.append(combined_file)\n",
        "\n",
        "        # 2. Create individual phrase plots\n",
        "        for phrase in self.individual_phrases:\n",
        "            if phrase == \"teacher\":\n",
        "                continue\n",
        "            phrase_path = os.path.join(output_dir, f\"{phrase}_bias_analysis.png\")\n",
        "            phrase_file = self.create_individual_phrase_plot(phrase, phrase_path)\n",
        "            generated_files.append(phrase_file)\n",
        "\n",
        "        # Save metadata about the visualizations\n",
        "        viz_metadata = {\n",
        "            'visualization_structure': {\n",
        "                'total_files': 7,\n",
        "                'combined_analysis': 'combined_bias_analysis.png',\n",
        "                'individual_phrase_files': [f\"{phrase}_bias_analysis.png\" for phrase in self.individual_phrases]\n",
        "            },\n",
        "            'plot_layout': '2x2 subplots per file (following paste5.txt structure)',\n",
        "            'plot_descriptions': {\n",
        "                'top_left': 'Histogram: Phrase entropy distributions vs baseline (no hint)',\n",
        "                'top_right': 'Line plot: Average entropy vs layers (per variant)',\n",
        "                'bottom_left': 'Bar chart: Average phrase entropy (baseline + 3 biased variants)',\n",
        "                'bottom_right': 'Heatmap: ΔH per layer/head (bias effects)'\n",
        "            },\n",
        "            'analysis_structure': {\n",
        "                'individual_phrases': self.individual_phrases,\n",
        "                'combined_analysis': 'All bias phrases together',\n",
        "                'baseline_comparison': 'Baseline = no hint phrases (global entropy)'\n",
        "            },\n",
        "            'interpretation': {\n",
        "                'baseline_vs_biased': 'Baseline shows attention without bias phrases',\n",
        "                'negative_delta_h': 'More focused attention at phrase positions (unfaithful)',\n",
        "                'positive_delta_h': 'Less focused attention at phrase positions (faithful)',\n",
        "                'high_entropy': 'Balanced/faithful attention',\n",
        "                'low_entropy': 'Focused/potentially unfaithful attention'\n",
        "            },\n",
        "            'files_generated': generated_files\n",
        "        }\n",
        "\n",
        "        metadata_file = os.path.join(output_dir, \"visualization_metadata.json\")\n",
        "        with open(metadata_file, 'w') as f:\n",
        "            json.dump(viz_metadata, f, indent=2)\n",
        "\n",
        "        print(\"\\nIndividual phrase results visualization completed successfully!\")\n",
        "        print(f\"Generated {len(generated_files)} visualization files:\")\n",
        "        for i, file_path in enumerate(generated_files, 1):\n",
        "            file_name = os.path.basename(file_path)\n",
        "            if 'combined' in file_name:\n",
        "                print(f\"  {i}. {file_name} (All bias phrases together)\")\n",
        "            else:\n",
        "                phrase = file_name.replace('_bias_analysis.png', '')\n",
        "                print(f\"  {i}. {file_name} (Individual phrase: '{phrase}')\")\n",
        "\n",
        "        print(f\"\\nAll visualizations saved to: {output_dir}/\")\n",
        "        print(\"Complete individual phrase attention entropy analysis pipeline finished!\")\n",
        "\n",
        "        return output_dir, generated_files\n",
        "\n",
        "\n",
        "def run_results_visualization_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase results visualization pipeline\n",
        "    \"\"\"\n",
        "    # Configuration - Update these paths for your setup\n",
        "\n",
        "    print(\"Starting Individual Phrase Results Visualization\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize visualizer\n",
        "    visualizer = ResultsVisualizationIndividual(\n",
        "        aggregated_analysis_dir=AGGREGATED_ANALYSIS_DIR,\n",
        "        statistical_results_dir=STATISTICAL_RESULTS_DIR,\n",
        "        entropy_results_dir=ENTROPY_RESULTS_DIR\n",
        "    )\n",
        "\n",
        "    # Run complete visualization\n",
        "    output_dir, generated_files = visualizer.run_complete_visualization()\n",
        "\n",
        "    print(f\"\\nIndividual phrase results visualization completed: {output_dir}\")\n",
        "    print(\"Pipeline: Data preparation → Entropy computation → Aggregation → Statistical analysis → Visualization\")\n",
        "    print(f\"Analyzed phrases: combined + {', '.join(visualizer.individual_phrases)}\")\n",
        "\n",
        "    return visualizer, generated_files\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    visualizer, generated_files = run_results_visualization_individual()\n",
        "    print(f\"\\nGenerated visualization files: {len(generated_files)}\")\n",
        "    for file_path in generated_files:\n",
        "        print(f\"  - {os.path.basename(file_path)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ul_thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
