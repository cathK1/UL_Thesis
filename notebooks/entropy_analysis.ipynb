{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entropy Analysis\n",
        "Takes data generated from `../src/extraction_llama.py` and performs entropy analysis experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "from scipy.stats import entropy, ttest_ind, spearmanr, normaltest, ttest_1samp, kruskal\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Config\n",
        "DATA_DIR = \"../data/test_output_8rows/attention\"\n",
        "PREPARED_DATA_DIR = \"../data/prepared_data_individual\"\n",
        "ENTROPY_RESULTS_DIR = \"../data/entropy_results_individual\"\n",
        "RESULTS_CSV = \"../data/test_output_8rows/results_llama.csv\"\n",
        "TOKEN_METADATA = \"../data/test_output_8rows/token_metadata_llama.json\"\n",
        "AGGREGATED_ANALYSIS_DIR = \"../data/aggregated_analysis_individual\"  # Contains individual phrase aggregation files\n",
        "STATISTICAL_RESULTS_DIR = \"../data/statistical_results_individual\"  # Contains individual phrase statistical analysis files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare attention data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmeF1One2JDi",
        "outputId": "b44a3b73-513b-4bb3-a952-df9e1a1558b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Individual Phrase Data Preparation\n",
            "============================================================\n",
            "Loaded 8 rows from dataset\n",
            "Loaded token metadata for 8 rows\n",
            "Individual phrase analyses: teacher, own, sure, unsure, quick, fast, step\n",
            "Total analyses: 8 (6 individual + 1 combined)\n",
            "Validating data structure...\n",
            "Checked 8 rows\n",
            "Found 320 valid attention files\n",
            "Processing attention data with individual phrase analysis...\n",
            "Analyzing 7 individual phrases + 1 combined analysis\n",
            "Detected 16 layers in the model\n",
            "Processing complete: 8 rows processed\n",
            "Individual phrase statistics:\n",
            "  teacher: 3 positions found\n",
            "  own: 3 positions found\n",
            "  sure: 9 positions found\n",
            "  unsure: 3 positions found\n",
            "  quick: 3 positions found\n",
            "  fast: 3 positions found\n",
            "  step: 6 positions found\n",
            "  combined: 27 positions found\n",
            "Saving individual phrase results to ../data/prepared_data_individual/\n",
            "Individual phrase data preparation completed successfully!\n",
            "Generated 7 individual + 1 combined = 8 total analyses\n",
            "Files generated:\n",
            "  - hint_positions_teacher.json\n",
            "  - hint_positions_own.json\n",
            "  - hint_positions_sure.json\n",
            "  - hint_positions_unsure.json\n",
            "  - hint_positions_quick.json\n",
            "  - hint_positions_fast.json\n",
            "  - hint_positions_step.json\n",
            "  - hint_positions_combined.json\n",
            "\n",
            "Individual phrase data preparation completed: ../data/prepared_data_individual\n",
            "Ready for individual phrase entropy computation\n"
          ]
        }
      ],
      "source": [
        "class AttentionDataPreparationIndividual:\n",
        "    def __init__(self, data_dir: str, results_csv: str, token_metadata_json: str):\n",
        "        \"\"\"\n",
        "        Prepare Data: Load attention files, normalize rows, identify individual phrase positions (6 + 1)\n",
        "\n",
        "        Args:\n",
        "            data_dir: Base directory containing row_X/prompt_Y/attn_layer_Z.npy structure\n",
        "            results_csv: Path to results CSV file\n",
        "            token_metadata_json: Path to token metadata JSON file\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.attention_dir = data_dir\n",
        "\n",
        "        # Load dataset and metadata\n",
        "        self.results_df = pd.read_csv(results_csv)\n",
        "        with open(token_metadata_json, 'r') as f:\n",
        "            self.token_metadata = json.load(f)\n",
        "\n",
        "        # Define individual bias phrases (6 + 1 combined)\n",
        "        self.individual_bias_phrases = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"]\n",
        "        self.analysis_keys = self.individual_bias_phrases + [\"combined\"]\n",
        "\n",
        "        # Storage for processed data\n",
        "        self.normalized_attention = {}\n",
        "        self.hint_positions_individual = {}  # {phrase: {row_prompt: positions}}\n",
        "        self.validation_log = []\n",
        "\n",
        "        # Prompt mapping for file system\n",
        "        self.prompt_mapping = {\n",
        "            'Prompt_1': 'prompt_1',\n",
        "            'Prompt_2': 'prompt_2',\n",
        "            'Prompt_3': 'prompt_3'\n",
        "        }\n",
        "\n",
        "        # Initialize storage for each phrase analysis\n",
        "        for phrase in self.analysis_keys:\n",
        "            self.hint_positions_individual[phrase] = {}\n",
        "\n",
        "        print(f\"Loaded {len(self.results_df)} rows from dataset\")\n",
        "        print(f\"Loaded token metadata for {len(self.token_metadata)} rows\")\n",
        "        print(f\"Individual phrase analyses: {', '.join(self.individual_bias_phrases)}\")\n",
        "        print(f\"Total analyses: {len(self.analysis_keys)} (6 individual + 1 combined)\")\n",
        "\n",
        "    def validate_data_structure(self) -> Dict[str, int]:\n",
        "        \"\"\"Validate the attention data structure and count available files\"\"\"\n",
        "        print(\"Validating data structure...\")\n",
        "\n",
        "        validation_stats = {\n",
        "            'total_rows_checked': 0,\n",
        "            'rows_with_attention_data': 0,\n",
        "            'total_attention_files': 0,\n",
        "            'missing_files': 0,\n",
        "            'invalid_files': 0\n",
        "        }\n",
        "\n",
        "        max_check = min(10, len(self.results_df))\n",
        "\n",
        "        for row_idx in range(max_check):\n",
        "            validation_stats['total_rows_checked'] += 1\n",
        "            row_has_data = False\n",
        "\n",
        "            for prompt_col in ['Prompt_1', 'Prompt_2', 'Prompt_3']:\n",
        "                if pd.isna(self.results_df.iloc[row_idx][prompt_col]):\n",
        "                    continue\n",
        "\n",
        "                prompt_file_name = self.prompt_mapping[prompt_col]\n",
        "                row_dir = os.path.join(self.attention_dir, f\"row_{row_idx}\", prompt_file_name)\n",
        "\n",
        "                if os.path.exists(row_dir):\n",
        "                    npy_files = [f for f in os.listdir(row_dir) if f.endswith('.npy') and 'attn_layer_' in f]\n",
        "\n",
        "                    for npy_file in npy_files:\n",
        "                        file_path = os.path.join(row_dir, npy_file)\n",
        "                        try:\n",
        "                            test_load = np.load(file_path)\n",
        "                            if len(test_load.shape) == 4:\n",
        "                                validation_stats['total_attention_files'] += 1\n",
        "                                row_has_data = True\n",
        "                            else:\n",
        "                                self.validation_log.append(f\"Invalid shape {test_load.shape} in {file_path}\")\n",
        "                                validation_stats['invalid_files'] += 1\n",
        "                        except Exception as e:\n",
        "                            self.validation_log.append(f\"Cannot load {file_path}: {e}\")\n",
        "                            validation_stats['invalid_files'] += 1\n",
        "                else:\n",
        "                    validation_stats['missing_files'] += 1\n",
        "\n",
        "            if row_has_data:\n",
        "                validation_stats['rows_with_attention_data'] += 1\n",
        "\n",
        "        print(f\"Checked {validation_stats['total_rows_checked']} rows\")\n",
        "        print(f\"Found {validation_stats['total_attention_files']} valid attention files\")\n",
        "\n",
        "        return validation_stats\n",
        "\n",
        "    def load_and_normalize_attention(self, row_idx: int, prompt_name: str, layer_idx: int) -> Optional[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Load attention weights and normalize rows to ensure each row sums to 1\n",
        "\n",
        "        Args:\n",
        "            row_idx: Row index in dataset\n",
        "            prompt_name: Prompt file name (prompt_1, prompt_2, prompt_3)\n",
        "            layer_idx: Layer index (0 to num_layers-1)\n",
        "\n",
        "        Returns:\n",
        "            Normalized attention matrix [batch, heads, seq_len, seq_len] or None\n",
        "        \"\"\"\n",
        "        file_path = os.path.join(\n",
        "            self.attention_dir,\n",
        "            f\"row_{row_idx}\",\n",
        "            prompt_name,\n",
        "            f\"attn_layer_{layer_idx}.npy\"\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            attention = np.load(file_path)\n",
        "\n",
        "            if len(attention.shape) != 4:\n",
        "                self.validation_log.append(f\"Invalid shape {attention.shape} in {file_path}\")\n",
        "                return None\n",
        "\n",
        "            batch_size, num_heads, seq_len, key_len = attention.shape\n",
        "            normalized_attention = attention.copy()\n",
        "\n",
        "            # Normalize rows to ensure each query's attention sums to 1\n",
        "            normalization_fixes = 0\n",
        "            for b in range(batch_size):\n",
        "                for h in range(num_heads):\n",
        "                    for q in range(seq_len):\n",
        "                        attn_row = attention[b, h, q, :]\n",
        "                        row_sum = np.sum(attn_row)\n",
        "\n",
        "                        if not (0.99 <= row_sum <= 1.01):\n",
        "                            if row_sum > 0 and not np.isnan(row_sum):\n",
        "                                normalized_attention[b, h, q, :] = attn_row / row_sum\n",
        "                                normalization_fixes += 1\n",
        "                            else:\n",
        "                                normalized_attention[b, h, q, :] = np.ones(key_len) / key_len\n",
        "                                normalization_fixes += 1\n",
        "\n",
        "            return normalized_attention\n",
        "\n",
        "        except Exception as e:\n",
        "            self.validation_log.append(f\"Error processing {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def identify_hint_positions_individual(self, row_idx: int, prompt_col: str) -> Dict[str, List[int]]:\n",
        "        \"\"\"\n",
        "        Identify hint positions for each individual bias phrase + combined\n",
        "\n",
        "        Args:\n",
        "            row_idx: Row index in dataset\n",
        "            prompt_col: Prompt column name (Prompt_1, Prompt_2, Prompt_3)\n",
        "\n",
        "        Returns:\n",
        "            Dict mapping phrase -> list of token positions\n",
        "            Plus 'combined' key with all positions\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Access token metadata\n",
        "            if isinstance(self.token_metadata, list):\n",
        "                if row_idx < len(self.token_metadata):\n",
        "                    row_metadata = self.token_metadata[row_idx]\n",
        "                else:\n",
        "                    return {phrase: [] for phrase in self.analysis_keys}\n",
        "            else:\n",
        "                if str(row_idx) in self.token_metadata:\n",
        "                    row_metadata = self.token_metadata[str(row_idx)]\n",
        "                elif row_idx in self.token_metadata:\n",
        "                    row_metadata = self.token_metadata[row_idx]\n",
        "                else:\n",
        "                    return {phrase: [] for phrase in self.analysis_keys}\n",
        "\n",
        "            # Extract input tokens\n",
        "            if isinstance(row_metadata, dict) and prompt_col in row_metadata:\n",
        "                prompt_metadata = row_metadata[prompt_col]\n",
        "                input_tokens = prompt_metadata.get('input_tokens', [])\n",
        "            elif isinstance(row_metadata, dict) and 'input_tokens' in row_metadata:\n",
        "                input_tokens = row_metadata.get('input_tokens', [])\n",
        "            else:\n",
        "                return {phrase: [] for phrase in self.analysis_keys}\n",
        "\n",
        "            # Find positions for each individual phrase\n",
        "            phrase_positions = {}\n",
        "            all_combined_positions = []\n",
        "\n",
        "            for phrase in self.individual_bias_phrases:\n",
        "                phrase_specific_positions = []\n",
        "\n",
        "                for i, token in enumerate(input_tokens):\n",
        "                    clean_token = str(token).replace('Ġ', '').replace('▁', '').replace('##', '').lower().strip()\n",
        "\n",
        "                    if phrase in clean_token:\n",
        "                        phrase_specific_positions.append(i)\n",
        "                        all_combined_positions.append(i)\n",
        "\n",
        "                phrase_positions[phrase] = sorted(list(set(phrase_specific_positions)))\n",
        "\n",
        "            # Add combined positions\n",
        "            phrase_positions['combined'] = sorted(list(set(all_combined_positions)))\n",
        "\n",
        "            return phrase_positions\n",
        "\n",
        "        except (KeyError, TypeError, AttributeError, IndexError) as e:\n",
        "            self.validation_log.append(f\"Error finding individual hints for row {row_idx}, {prompt_col}: {e}\")\n",
        "            return {phrase: [] for phrase in self.analysis_keys}\n",
        "\n",
        "    def process_all_attention_data_individual(self, max_rows: int = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Process all attention data: load, normalize, and identify individual phrase positions\n",
        "\n",
        "        Args:\n",
        "            max_rows: Maximum number of rows to process (None for all)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with processing statistics\n",
        "        \"\"\"\n",
        "        print(\"Processing attention data with individual phrase analysis...\")\n",
        "        print(f\"Analyzing {len(self.individual_bias_phrases)} individual phrases + 1 combined analysis\")\n",
        "\n",
        "        if max_rows is None:\n",
        "            max_rows = len(self.results_df)\n",
        "\n",
        "        num_layers = self._detect_num_layers()\n",
        "        print(f\"Detected {num_layers} layers in the model\")\n",
        "\n",
        "        processing_stats = {\n",
        "            'total_rows_processed': 0,\n",
        "            'total_prompts_processed': 0,\n",
        "            'total_attention_matrices_loaded': 0,\n",
        "            'phrases_processed': {phrase: 0 for phrase in self.analysis_keys},\n",
        "            'rows_with_errors': 0\n",
        "        }\n",
        "\n",
        "        for row_idx in range(min(max_rows, len(self.results_df))):\n",
        "            row_has_errors = False\n",
        "            self.normalized_attention[row_idx] = {}\n",
        "\n",
        "            for prompt_col in ['Prompt_1', 'Prompt_2', 'Prompt_3']:\n",
        "                if pd.isna(self.results_df.iloc[row_idx][prompt_col]):\n",
        "                    continue\n",
        "\n",
        "                processing_stats['total_prompts_processed'] += 1\n",
        "                prompt_file_name = self.prompt_mapping[prompt_col]\n",
        "\n",
        "                # Get individual phrase positions\n",
        "                phrase_positions = self.identify_hint_positions_individual(row_idx, prompt_col)\n",
        "\n",
        "                # Store positions for each individual phrase + combined\n",
        "                for phrase, positions in phrase_positions.items():\n",
        "                    key = f\"{row_idx}_{prompt_col}\"\n",
        "                    self.hint_positions_individual[phrase][key] = positions\n",
        "\n",
        "                    if positions:\n",
        "                        processing_stats['phrases_processed'][phrase] += len(positions)\n",
        "\n",
        "                # Load and normalize attention for each layer\n",
        "                for layer_idx in range(num_layers):\n",
        "                    normalized_attn = self.load_and_normalize_attention(row_idx, prompt_file_name, layer_idx)\n",
        "\n",
        "                    if normalized_attn is not None:\n",
        "                        key = f\"{prompt_col}_layer_{layer_idx}\"\n",
        "                        self.normalized_attention[row_idx][key] = normalized_attn\n",
        "                        processing_stats['total_attention_matrices_loaded'] += 1\n",
        "                    else:\n",
        "                        row_has_errors = True\n",
        "\n",
        "            if row_has_errors:\n",
        "                processing_stats['rows_with_errors'] += 1\n",
        "\n",
        "            processing_stats['total_rows_processed'] += 1\n",
        "\n",
        "            if (row_idx + 1) % 50 == 0:\n",
        "                print(f\"Processed {row_idx + 1} rows...\")\n",
        "\n",
        "        print(f\"Processing complete: {processing_stats['total_rows_processed']} rows processed\")\n",
        "        print(\"Individual phrase statistics:\")\n",
        "        for phrase, count in processing_stats['phrases_processed'].items():\n",
        "            print(f\"  {phrase}: {count} positions found\")\n",
        "\n",
        "        return processing_stats\n",
        "\n",
        "    def _detect_num_layers(self) -> int:\n",
        "        \"\"\"Auto-detect number of layers by checking available files\"\"\"\n",
        "        for row_idx in range(min(5, len(self.results_df))):\n",
        "            for prompt_name in ['prompt_1', 'prompt_2', 'prompt_3']:\n",
        "                row_dir = os.path.join(self.attention_dir, f\"row_{row_idx}\", prompt_name)\n",
        "                if os.path.exists(row_dir):\n",
        "                    files = [f for f in os.listdir(row_dir) if f.startswith('attn_layer_') and f.endswith('.npy')]\n",
        "                    if files:\n",
        "                        layer_numbers = []\n",
        "                        for f in files:\n",
        "                            try:\n",
        "                                layer_num = int(f.replace('attn_layer_', '').replace('.npy', ''))\n",
        "                                layer_numbers.append(layer_num)\n",
        "                            except:\n",
        "                                continue\n",
        "                        if layer_numbers:\n",
        "                            return max(layer_numbers) + 1\n",
        "\n",
        "        return 16  # Default based on diagnostics\n",
        "\n",
        "    def save_results(self, output_dir: str = \"../data/prepared_data_individual\"):\n",
        "        \"\"\"Save results for use in subsequent processing steps\"\"\"\n",
        "        print(f\"Saving individual phrase results to {output_dir}/\")\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save individual phrase positions\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(output_dir, f\"hint_positions_{phrase}.json\")\n",
        "            with open(phrase_file, 'w') as f:\n",
        "                json.dump(self.hint_positions_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Save validation log if exists\n",
        "        if self.validation_log:\n",
        "            log_file = os.path.join(output_dir, \"validation_log.txt\")\n",
        "            with open(log_file, 'w') as f:\n",
        "                for log_entry in self.validation_log:\n",
        "                    f.write(log_entry + \"\\n\")\n",
        "\n",
        "        # Save summary statistics\n",
        "        summary = {\n",
        "            'total_rows_in_dataset': len(self.results_df),\n",
        "            'rows_with_normalized_attention': len(self.normalized_attention),\n",
        "            'individual_phrases_analyzed': self.individual_bias_phrases,\n",
        "            'analysis_types': {\n",
        "                'individual_phrases': len(self.individual_bias_phrases),\n",
        "                'combined_analysis': 1,\n",
        "                'total_analyses': len(self.analysis_keys)\n",
        "            },\n",
        "            'phrase_position_counts': {\n",
        "                phrase: len(positions_dict)\n",
        "                for phrase, positions_dict in self.hint_positions_individual.items()\n",
        "            },\n",
        "            'total_validation_issues': len(self.validation_log),\n",
        "            'prompt_mapping': self.prompt_mapping\n",
        "        }\n",
        "\n",
        "        summary_file = os.path.join(output_dir, \"preparation_summary_individual.json\")\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase data preparation completed successfully!\")\n",
        "        print(f\"Generated {len(self.individual_bias_phrases)} individual + 1 combined = {len(self.analysis_keys)} total analyses\")\n",
        "        print(f\"Files generated:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"  - hint_positions_{phrase}.json\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_data_preparation_individual():\n",
        "    \"\"\"\n",
        "    Run the complete data preparation pipeline for individual phrases\n",
        "    \"\"\"\n",
        "    # Configuration - Update these paths for your setup\n",
        "    # DATA_DIR = \"../Test_output_10rows/input_attention\"\n",
        "    # TOKEN_METADATA = \"/content/Test_output_10rows/input_attention_metadata.json\"\n",
        "    # RESULTS_CSV = \"/content/Test_output_10rows/results_with_predictions.csv\"\n",
        "\n",
        "    print(\"Starting Individual Phrase Data Preparation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize data preparation\n",
        "    processor = AttentionDataPreparationIndividual(\n",
        "        data_dir=DATA_DIR,\n",
        "        results_csv=RESULTS_CSV,\n",
        "        token_metadata_json=TOKEN_METADATA\n",
        "    )\n",
        "\n",
        "    # Validate data structure\n",
        "    validation_stats = processor.validate_data_structure()\n",
        "\n",
        "    # Process all data\n",
        "    processing_stats = processor.process_all_attention_data_individual()\n",
        "\n",
        "    # Save results for next stage\n",
        "    output_dir = processor.save_results(PREPARED_DATA_DIR)\n",
        "\n",
        "    print(f\"\\nIndividual phrase data preparation completed: {output_dir}\")\n",
        "    print(\"Ready for individual phrase entropy computation\")\n",
        "\n",
        "    return processor, processing_stats\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processor, stats = run_data_preparation_individual()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u25vyFOT2sgr",
        "outputId": "d68f53a4-4a74-41a7-f784-2836854b1d6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Individual Phrase Entropy Computation\n",
            "============================================================\n",
            "Loaded hint positions for 'teacher': 20 prompt instances\n",
            "Loaded hint positions for 'own': 20 prompt instances\n",
            "Loaded hint positions for 'sure': 20 prompt instances\n",
            "Loaded hint positions for 'unsure': 20 prompt instances\n",
            "Loaded hint positions for 'quick': 20 prompt instances\n",
            "Loaded hint positions for 'fast': 20 prompt instances\n",
            "Loaded hint positions for 'step': 20 prompt instances\n",
            "Loaded hint positions for 'combined': 20 prompt instances\n",
            "Loaded hint positions for 8 individual phrase analyses\n",
            "Computing entropy for dataset with individual phrase analysis...\n",
            "Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\n",
            "Analyzing: teacher, own, sure, unsure, quick, fast, step, combined\n",
            "Processing 8 rows\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 320\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m entropy_computer, stats\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 320\u001b[0m     entropy_computer, stats \u001b[38;5;241m=\u001b[39m \u001b[43mrun_entropy_computation_individual\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 308\u001b[0m, in \u001b[0;36mrun_entropy_computation_individual\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m entropy_computer \u001b[38;5;241m=\u001b[39m EntropyComputationIndividual(\n\u001b[1;32m    303\u001b[0m     prepared_data_dir\u001b[38;5;241m=\u001b[39mPREPARED_DATA_DIR,\n\u001b[1;32m    304\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mDATA_DIR\n\u001b[1;32m    305\u001b[0m )\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Compute entropy for dataset\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mentropy_computer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_entropy_for_dataset_individual\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Save results for next stage\u001b[39;00m\n\u001b[1;32m    311\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m entropy_computer\u001b[38;5;241m.\u001b[39msave_results()\n",
            "Cell \u001b[0;32mIn[10], line 162\u001b[0m, in \u001b[0;36mEntropyComputationIndividual.compute_entropy_for_dataset_individual\u001b[0;34m(self, max_rows)\u001b[0m\n\u001b[1;32m    159\u001b[0m attention_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(file_path)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Compute entropy\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m entropy_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_attention_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entropy_result:\n\u001b[1;32m    165\u001b[0m     layer_entropies \u001b[38;5;241m=\u001b[39m entropy_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropies\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# [heads, seq_len]\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[10], line 67\u001b[0m, in \u001b[0;36mEntropyComputationIndividual.compute_attention_entropy\u001b[0;34m(self, attention_matrix, hint_positions)\u001b[0m\n\u001b[1;32m     64\u001b[0m attn_row \u001b[38;5;241m=\u001b[39m attention_matrix[\u001b[38;5;241m0\u001b[39m, h, i, :]  \u001b[38;5;66;03m# Assuming batch_size=1\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m ent \u001b[38;5;241m=\u001b[39m \u001b[43mentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Handle edge cases\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(ent) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(ent):\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ul_thesis/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:550\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    547\u001b[0m     samples \u001b[38;5;241m=\u001b[39m [sample\u001b[38;5;241m.\u001b[39mreshape(new_shape)\n\u001b[1;32m    548\u001b[0m                \u001b[38;5;28;01mfor\u001b[39;00m sample, new_shape \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(samples, new_shapes)]\n\u001b[1;32m    549\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# work over the last axis\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m NaN \u001b[38;5;241m=\u001b[39m \u001b[43m_get_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m samples \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# if axis is not needed, just handle nan_policy and return\u001b[39;00m\n\u001b[1;32m    553\u001b[0m ndims \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sample\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples])\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ul_thesis/lib/python3.10/site-packages/scipy/_lib/_util.py:1037\u001b[0m, in \u001b[0;36m_get_nan\u001b[0;34m(xp, *data)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     child_rngs \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mGenerator(\u001b[38;5;28mtype\u001b[39m(bg)(child_ss))\n\u001b[1;32m   1033\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m child_ss \u001b[38;5;129;01min\u001b[39;00m ss\u001b[38;5;241m.\u001b[39mspawn(n_children)]\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m child_rngs\n\u001b[0;32m-> 1037\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_nan\u001b[39m(\u001b[38;5;241m*\u001b[39mdata, xp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1038\u001b[0m     xp \u001b[38;5;241m=\u001b[39m array_namespace(\u001b[38;5;241m*\u001b[39mdata) \u001b[38;5;28;01mif\u001b[39;00m xp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m xp\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;66;03m# Get NaN of appropriate dtype for data\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "class EntropyComputationIndividual:\n",
        "    def __init__(self, prepared_data_dir: str, data_dir: str):\n",
        "        \"\"\"\n",
        "        Compute Entropy: Granular computation per query/head/layer for individual phrases (6 + 1)\n",
        "\n",
        "        Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\n",
        "\n",
        "        Args:\n",
        "            prepared_data_dir: Directory containing individual phrase hint_positions files\n",
        "            data_dir: Base directory containing attention files\n",
        "        \"\"\"\n",
        "        self.prepared_data_dir = prepared_data_dir\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "\n",
        "        # Load individual phrase positions from data preparation stage\n",
        "        self.hint_positions_individual = {}\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(prepared_data_dir, f\"hint_positions_{phrase}.json\")\n",
        "            if os.path.exists(phrase_file):\n",
        "                with open(phrase_file, 'r') as f:\n",
        "                    self.hint_positions_individual[phrase] = json.load(f)\n",
        "                print(f\"Loaded hint positions for '{phrase}': {len(self.hint_positions_individual[phrase])} prompt instances\")\n",
        "            else:\n",
        "                print(f\"Warning: {phrase_file} not found\")\n",
        "                self.hint_positions_individual[phrase] = {}\n",
        "\n",
        "        print(f\"Loaded hint positions for {len(self.hint_positions_individual)} individual phrase analyses\")\n",
        "\n",
        "        # Storage for entropy results\n",
        "        self.entropies = {}  # {row_prompt: entropies[ℓ][h][i] array} - same for all analyses\n",
        "        self.hint_entropies_individual = {}  # {phrase: {row_prompt: [entropies[ℓ][h][pos] for pos in phrase_positions]}}\n",
        "\n",
        "        # Initialize individual phrase storage\n",
        "        for phrase in self.analysis_keys:\n",
        "            self.hint_entropies_individual[phrase] = {}\n",
        "\n",
        "\n",
        "    def compute_attention_entropy(self, attention_matrix: np.ndarray, hint_positions: List[int] = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Compute entropy for attention matrix using the specified formula\n",
        "\n",
        "        Args:\n",
        "            attention_matrix: Shape [batch, heads, seq_len, seq_len]\n",
        "            hint_positions: List of hint token positions for subsetting\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with entropies and hint_entropies\n",
        "        \"\"\"\n",
        "        if len(attention_matrix.shape) != 4:\n",
        "            return None\n",
        "\n",
        "        batch_size, num_heads, seq_len, key_len = attention_matrix.shape\n",
        "\n",
        "        # Store entropies in [heads, seq_len] format\n",
        "        entropies = np.zeros((num_heads, seq_len))\n",
        "\n",
        "        # Loop over heads, then queries - exact methodology\n",
        "        for h in range(num_heads):\n",
        "            for i in range(seq_len):  # query token i\n",
        "                # Get attention row for query i in head h\n",
        "                attn_row = attention_matrix[0, h, i, :]  # Assuming batch_size=1\n",
        "\n",
        "                # Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\n",
        "                ent = entropy(attn_row, base=2)\n",
        "\n",
        "                # Handle edge cases\n",
        "                if np.isnan(ent) or np.isinf(ent):\n",
        "                    ent = 0.0\n",
        "\n",
        "                entropies[h, i] = ent\n",
        "\n",
        "        # Subset for hints\n",
        "        hint_entropies = []\n",
        "        if hint_positions:\n",
        "            for h in range(num_heads):\n",
        "                for pos in hint_positions:\n",
        "                    if pos < seq_len:\n",
        "                        hint_entropies.append(entropies[h, pos])\n",
        "\n",
        "        return {\n",
        "            'entropies': entropies,\n",
        "            'hint_entropies': hint_entropies,\n",
        "            'num_heads': num_heads,\n",
        "            'seq_len': seq_len,\n",
        "            'hint_positions': hint_positions or []\n",
        "        }\n",
        "\n",
        "\n",
        "    def compute_entropy_for_dataset_individual(self, max_rows: int = None):\n",
        "        \"\"\"\n",
        "        Process entropy computation for the entire dataset with individual phrase analysis\n",
        "\n",
        "        Args:\n",
        "            max_rows: Maximum number of rows to process (None for all)\n",
        "        \"\"\"\n",
        "        print(\"Computing entropy for dataset with individual phrase analysis...\")\n",
        "        print(\"Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\")\n",
        "        print(f\"Analyzing: {', '.join(self.analysis_keys)}\")\n",
        "\n",
        "        # Get available rows\n",
        "        available_rows = []\n",
        "        for item in os.listdir(self.data_dir):\n",
        "            if item.startswith('row_') and os.path.isdir(os.path.join(self.data_dir, item)):\n",
        "                try:\n",
        "                    row_num = int(item.replace('row_', ''))\n",
        "                    available_rows.append(row_num)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        available_rows = sorted(available_rows)\n",
        "        if max_rows:\n",
        "            available_rows = available_rows[:max_rows]\n",
        "\n",
        "        print(f\"Processing {len(available_rows)} rows\")\n",
        "\n",
        "        total_entropy_values = 0\n",
        "        total_matrices = 0\n",
        "\n",
        "        # Process each row\n",
        "        for row_idx in available_rows:\n",
        "            # Process each prompt\n",
        "            for prompt_name in ['prompt_1', 'prompt_2', 'prompt_3']:\n",
        "                prompt_dir = os.path.join(self.data_dir, f\"row_{row_idx}\", prompt_name)\n",
        "                if not os.path.exists(prompt_dir):\n",
        "                    continue\n",
        "\n",
        "                # Get individual phrase hint positions\n",
        "                row_prompt_key = f\"row_{row_idx}_{prompt_name}\"\n",
        "                phrase_hint_positions = {}\n",
        "\n",
        "                for phrase in self.analysis_keys:\n",
        "                    key = f\"{row_idx}_Prompt_{prompt_name.split('_')[1]}\"\n",
        "                    phrase_hint_positions[phrase] = self.hint_positions_individual[phrase].get(key, [])\n",
        "\n",
        "                # Get available layers\n",
        "                layer_files = [f for f in os.listdir(prompt_dir) if f.startswith('attn_layer_') and f.endswith('.npy')]\n",
        "                layer_indices = sorted([int(f.replace('attn_layer_', '').replace('.npy', '')) for f in layer_files])\n",
        "\n",
        "                if not layer_indices:\n",
        "                    continue\n",
        "\n",
        "                # Load first matrix to get dimensions\n",
        "                first_file = os.path.join(prompt_dir, f\"attn_layer_{layer_indices[0]}.npy\")\n",
        "                sample_matrix = np.load(first_file)\n",
        "                _, num_heads, seq_len, _ = sample_matrix.shape\n",
        "                num_layers = len(layer_indices)\n",
        "\n",
        "                # Initialize entropies[ℓ][h][i] array for this row_prompt\n",
        "                entropies = np.zeros((num_layers, num_heads, seq_len))\n",
        "\n",
        "                # Process each layer ℓ\n",
        "                for layer_position, layer_idx in enumerate(layer_indices):\n",
        "                    file_path = os.path.join(prompt_dir, f\"attn_layer_{layer_idx}.npy\")\n",
        "\n",
        "                    try:\n",
        "                        attention_matrix = np.load(file_path)\n",
        "\n",
        "                        # Compute entropy\n",
        "                        entropy_result = self.compute_attention_entropy(attention_matrix)\n",
        "\n",
        "                        if entropy_result:\n",
        "                            layer_entropies = entropy_result['entropies']  # [heads, seq_len]\n",
        "\n",
        "                            # Store in entropies[ℓ][h][i] format\n",
        "                            entropies[layer_position, :, :] = layer_entropies\n",
        "\n",
        "                            total_matrices += 1\n",
        "                            total_entropy_values += layer_entropies.size\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing layer {layer_idx}: {e}\")\n",
        "\n",
        "                # Store the complete entropies[ℓ][h][i] array (same for all analyses)\n",
        "                self.entropies[row_prompt_key] = entropies\n",
        "\n",
        "                # Extract individual phrase-specific entropies\n",
        "                for phrase, hint_positions in phrase_hint_positions.items():\n",
        "                    phrase_entropy_values = []\n",
        "\n",
        "                    if hint_positions:\n",
        "                        for layer_pos in range(num_layers):\n",
        "                            for head_idx in range(num_heads):\n",
        "                                for pos in hint_positions:\n",
        "                                    if pos < seq_len:\n",
        "                                        phrase_entropy_values.append(entropies[layer_pos, head_idx, pos])\n",
        "\n",
        "                    self.hint_entropies_individual[phrase][row_prompt_key] = phrase_entropy_values\n",
        "\n",
        "            if (row_idx + 1) % 25 == 0:\n",
        "                print(f\"Processed {row_idx + 1} rows...\")\n",
        "\n",
        "        print(f\"Individual phrase entropy computation complete!\")\n",
        "        print(f\"Processed {total_matrices} attention matrices\")\n",
        "        print(f\"Computed {total_entropy_values} entropy values\")\n",
        "\n",
        "        # Print summary for each phrase\n",
        "        print(\"\\nPhrase-specific entropy summary:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            total_values = sum(len(values) for values in self.hint_entropies_individual[phrase].values())\n",
        "            non_empty_instances = sum(1 for values in self.hint_entropies_individual[phrase].values() if len(values) > 0)\n",
        "            print(f\"  {phrase}: {total_values} entropy values, {non_empty_instances} instances with data\")\n",
        "\n",
        "        return {\n",
        "            'total_matrices': total_matrices,\n",
        "            'total_entropy_values': total_entropy_values,\n",
        "            'rows_processed': len(available_rows),\n",
        "            'phrase_analyses': len(self.analysis_keys)\n",
        "        }\n",
        "\n",
        "\n",
        "    def save_results(self, output_dir: str = \"../data/entropy_results_individual\"):\n",
        "        \"\"\"\n",
        "        Save individual phrase entropy computation results\n",
        "\n",
        "        Output format: entropies[ℓ][h][i] arrays with individual phrase subsets\n",
        "        \"\"\"\n",
        "        print(f\"Saving individual phrase entropy results to {output_dir}/\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Convert numpy arrays to lists for JSON serialization\n",
        "        entropies_for_json = {}\n",
        "        for key, entropy_array in self.entropies.items():\n",
        "            entropies_for_json[key] = entropy_array.tolist()\n",
        "\n",
        "        # Save complete entropies[ℓ][h][i] arrays (same for all analyses)\n",
        "        entropy_file = os.path.join(output_dir, \"entropies_arrays.json\")\n",
        "        with open(entropy_file, 'w') as f:\n",
        "            json.dump(entropies_for_json, f, indent=2)\n",
        "\n",
        "        # Save individual phrase hint entropies\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(output_dir, f\"hint_entropies_{phrase}.json\")\n",
        "            with open(phrase_file, 'w') as f:\n",
        "                json.dump(self.hint_entropies_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Save individual phrase hint positions for reference\n",
        "        for phrase in self.analysis_keys:\n",
        "            positions_file = os.path.join(output_dir, f\"hint_positions_{phrase}.json\")\n",
        "            with open(positions_file, 'w') as f:\n",
        "                json.dump(self.hint_positions_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Create comprehensive summary\n",
        "        summary = {\n",
        "            'methodology': 'H_i^(ℓ,h) = -∑_j A_ij log(A_ij)',\n",
        "            'analysis_structure': {\n",
        "                'individual_phrases': [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"],\n",
        "                'combined_analysis': \"combined (all phrases together)\",\n",
        "                'total_analyses': len(self.analysis_keys)\n",
        "            },\n",
        "            'storage_format': {\n",
        "                'entropies': 'entropies[ℓ][h][i] arrays (layer, head, query)',\n",
        "                'hint_entropies': 'phrase-specific: [entropies[ℓ][h][pos] for pos in phrase_positions]'\n",
        "            },\n",
        "            'phrase_statistics': {},\n",
        "            'total_prompt_instances': len(self.entropies),\n",
        "            'arrays_saved': list(self.entropies.keys())\n",
        "        }\n",
        "\n",
        "        # Add phrase statistics\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_data = self.hint_entropies_individual[phrase]\n",
        "            total_entropy_values = sum(len(values) for values in phrase_data.values())\n",
        "            non_empty_instances = sum(1 for values in phrase_data.values() if len(values) > 0)\n",
        "\n",
        "            summary['phrase_statistics'][phrase] = {\n",
        "                'total_entropy_values': total_entropy_values,\n",
        "                'prompt_instances_with_phrase': non_empty_instances,\n",
        "                'total_prompt_instances': len(phrase_data),\n",
        "                'coverage_percentage': (non_empty_instances / len(phrase_data)) * 100 if len(phrase_data) > 0 else 0\n",
        "            }\n",
        "\n",
        "        summary_file = os.path.join(output_dir, \"entropy_summary_individual.json\")\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase entropy computation completed successfully!\")\n",
        "        print(f\"Generated analyses for: {', '.join(self.analysis_keys)}\")\n",
        "        print(\"Files generated:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"  - hint_entropies_{phrase}.json\")\n",
        "            print(f\"  - hint_positions_{phrase}.json\")\n",
        "        print(\"Ready for individual phrase aggregation analysis\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_entropy_computation_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase entropy computation pipeline\n",
        "    \"\"\"\n",
        "    # Configuration - Update these paths for your setup\n",
        "    # PREPARED_DATA_DIR = \"../prepared_data_individual\"  # Contains individual phrase hint_positions files\n",
        "    # DATA_DIR = \"/content/Test_output_10rows/input_attention\"  # Contains row_X/prompt_Y/attn_layer_Z.npy\n",
        "\n",
        "    print(\"Starting Individual Phrase Entropy Computation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize entropy computation\n",
        "    entropy_computer = EntropyComputationIndividual(\n",
        "        prepared_data_dir=PREPARED_DATA_DIR,\n",
        "        data_dir=DATA_DIR\n",
        "    )\n",
        "\n",
        "    # Compute entropy for dataset\n",
        "    stats = entropy_computer.compute_entropy_for_dataset_individual()\n",
        "\n",
        "    # Save results for next stage\n",
        "    output_dir = entropy_computer.save_results()\n",
        "\n",
        "    print(f\"\\nIndividual phrase entropy computation completed: {output_dir}\")\n",
        "    print(\"Ready for individual phrase aggregation analysis\")\n",
        "\n",
        "    return entropy_computer, stats\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    entropy_computer, stats = run_entropy_computation_individual()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregate Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qgsi0FBj2usI",
        "outputId": "0e746a6d-fa17-40cf-c08a-f89f92c5fb35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Individual Phrase Aggregation Analysis\n",
            "============================================================\n",
            "Starting Individual Phrase Aggregation Analysis\n",
            "============================================================\n",
            "Loaded entropies for 'teacher': 20 prompt instances\n",
            "Loaded entropies for 'own': 20 prompt instances\n",
            "Loaded entropies for 'sure': 20 prompt instances\n",
            "Loaded entropies for 'unsure': 20 prompt instances\n",
            "Loaded entropies for 'quick': 20 prompt instances\n",
            "Loaded entropies for 'fast': 20 prompt instances\n",
            "Loaded entropies for 'step': 20 prompt instances\n",
            "Loaded entropies for 'combined': 20 prompt instances\n",
            "Loaded individual phrase data for 8 analyses\n",
            "Computing individual phrase aggregations...\n",
            "Analyzing: teacher, own, sure, unsure, quick, fast, step, combined\n",
            "Processing teacher aggregations...\n",
            "  teacher: 3 valid instances, avg entropy = 3.1837\n",
            "Processing own aggregations...\n",
            "  own: 3 valid instances, avg entropy = 3.1837\n",
            "Processing sure aggregations...\n",
            "  sure: 9 valid instances, avg entropy = 2.9612\n",
            "Processing unsure aggregations...\n",
            "  unsure: 3 valid instances, avg entropy = 2.7797\n",
            "Processing quick aggregations...\n",
            "  quick: 3 valid instances, avg entropy = 3.3241\n",
            "Processing fast aggregations...\n",
            "  fast: 3 valid instances, avg entropy = 3.5764\n",
            "Processing step aggregations...\n",
            "  step: 3 valid instances, avg entropy = 3.5032\n",
            "Processing combined aggregations...\n",
            "  combined: 18 valid instances, avg entropy = 3.1368\n",
            "Computing individual phrase ΔH changes...\n",
            "Computing ΔH for teacher...\n",
            "Computing ΔH for own...\n",
            "Computing ΔH for sure...\n",
            "Computing ΔH for unsure...\n",
            "Computing ΔH for quick...\n",
            "Computing ΔH for fast...\n",
            "Computing ΔH for step...\n",
            "Computing ΔH for combined...\n",
            "\n",
            "ΔH Summary by Individual Phrase:\n",
            "  teacher: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 2\n",
            "  own: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 2\n",
            "  sure: Mean ΔH = -0.0018, Negative% = 33.3%, Count = 6\n",
            "  unsure: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 2\n",
            "  quick: Mean ΔH = -0.0054, Negative% = 100.0%, Count = 2\n",
            "  fast: Mean ΔH = -0.0032, Negative% = 50.0%, Count = 2\n",
            "  step: Mean ΔH = -0.0011, Negative% = 50.0%, Count = 2\n",
            "  combined: Mean ΔH = -0.0011, Negative% = 25.0%, Count = 12\n",
            "Computing cross-phrase comparison...\n",
            "\n",
            "Cross-phrase comparison completed!\n",
            "Phrases ranked by bias strength (most biased first):\n",
            "  1. quick: 0.0054\n",
            "  2. fast: 0.0039\n",
            "  3. step: 0.0028\n",
            "  4. sure: 0.0018\n",
            "  5. combined: 0.0012\n",
            "Saving individual phrase aggregation results to ../data/aggregated_analysis_individual/\n",
            "Computing individual phrase ΔH changes...\n",
            "Computing ΔH for teacher...\n",
            "Computing ΔH for own...\n",
            "Computing ΔH for sure...\n",
            "Computing ΔH for unsure...\n",
            "Computing ΔH for quick...\n",
            "Computing ΔH for fast...\n",
            "Computing ΔH for step...\n",
            "Computing ΔH for combined...\n",
            "\n",
            "ΔH Summary by Individual Phrase:\n",
            "  teacher: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 2\n",
            "  own: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 2\n",
            "  sure: Mean ΔH = -0.0018, Negative% = 33.3%, Count = 6\n",
            "  unsure: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 2\n",
            "  quick: Mean ΔH = -0.0054, Negative% = 100.0%, Count = 2\n",
            "  fast: Mean ΔH = -0.0032, Negative% = 50.0%, Count = 2\n",
            "  step: Mean ΔH = -0.0011, Negative% = 50.0%, Count = 2\n",
            "  combined: Mean ΔH = -0.0011, Negative% = 25.0%, Count = 12\n",
            "Computing cross-phrase comparison...\n",
            "\n",
            "Cross-phrase comparison completed!\n",
            "Phrases ranked by bias strength (most biased first):\n",
            "  1. quick: 0.0054\n",
            "  2. fast: 0.0039\n",
            "  3. step: 0.0028\n",
            "  4. sure: 0.0018\n",
            "  5. combined: 0.0012\n",
            "Individual phrase aggregation analysis completed successfully!\n",
            "Generated 8 separate phrase analyses:\n",
            "  - teacher: aggregated_entropies_teacher.json, delta_h_changes_teacher.json\n",
            "  - own: aggregated_entropies_own.json, delta_h_changes_own.json\n",
            "  - sure: aggregated_entropies_sure.json, delta_h_changes_sure.json\n",
            "  - unsure: aggregated_entropies_unsure.json, delta_h_changes_unsure.json\n",
            "  - quick: aggregated_entropies_quick.json, delta_h_changes_quick.json\n",
            "  - fast: aggregated_entropies_fast.json, delta_h_changes_fast.json\n",
            "  - step: aggregated_entropies_step.json, delta_h_changes_step.json\n",
            "  - combined: aggregated_entropies_combined.json, delta_h_changes_combined.json\n",
            "Additional files:\n",
            "  - phrase_delta_h_statistics.json\n",
            "  - cross_phrase_comparison.json\n",
            "Ready for individual phrase statistical analysis\n",
            "\n",
            "Individual phrase aggregation analysis completed: ../data/aggregated_analysis_individual\n"
          ]
        }
      ],
      "source": [
        "class AggregationAnalysisIndividual:\n",
        "    def __init__(self, entropy_results_dir: str):\n",
        "        \"\"\"\n",
        "        Aggregate and Compute Changes for Individual Phrases (6 + 1)\n",
        "\n",
        "        Methodology:\n",
        "        1. Per phrase: H_phrase = (1/N) ∑_positions H_i^(ℓ,h)\n",
        "        2. Changes (ΔH): ΔH = H_baseline - H_variant per phrase\n",
        "        3. Cross-phrase comparison: Compare bias effects across individual phrases\n",
        "\n",
        "        Args:\n",
        "            entropy_results_dir: Directory containing individual phrase entropy computation results\n",
        "        \"\"\"\n",
        "        self.entropy_results_dir = entropy_results_dir\n",
        "\n",
        "        print(\"Starting Individual Phrase Aggregation Analysis\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "\n",
        "        # Load individual phrase entropy data\n",
        "        self.load_entropy_data()\n",
        "\n",
        "        # Storage for aggregated results\n",
        "        self.aggregated_entropies_individual = {}  # {phrase: {row_prompt: value}}\n",
        "        self.delta_h_changes_individual = {}       # {phrase: {row_idx: {comparison: delta_h}}}\n",
        "\n",
        "        # Initialize storage for each phrase\n",
        "        for phrase in self.analysis_keys:\n",
        "            self.aggregated_entropies_individual[phrase] = {}\n",
        "            self.delta_h_changes_individual[phrase] = {}\n",
        "\n",
        "    def load_entropy_data(self):\n",
        "        \"\"\"Load individual phrase entropy data from computation stage\"\"\"\n",
        "        self.hint_entropies_individual = {}\n",
        "        self.hint_positions_individual = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Load hint entropies\n",
        "            entropy_file = os.path.join(self.entropy_results_dir, f\"hint_entropies_{phrase}.json\")\n",
        "            if os.path.exists(entropy_file):\n",
        "                with open(entropy_file, 'r') as f:\n",
        "                    self.hint_entropies_individual[phrase] = json.load(f)\n",
        "                print(f\"Loaded entropies for '{phrase}': {len(self.hint_entropies_individual[phrase])} prompt instances\")\n",
        "            else:\n",
        "                print(f\"Warning: {entropy_file} not found\")\n",
        "                self.hint_entropies_individual[phrase] = {}\n",
        "\n",
        "            # Load hint positions for reference\n",
        "            positions_file = os.path.join(self.entropy_results_dir, f\"hint_positions_{phrase}.json\")\n",
        "            if os.path.exists(positions_file):\n",
        "                with open(positions_file, 'r') as f:\n",
        "                    self.hint_positions_individual[phrase] = json.load(f)\n",
        "\n",
        "        print(f\"Loaded individual phrase data for {len(self.hint_entropies_individual)} analyses\")\n",
        "\n",
        "    def compute_individual_phrase_aggregations(self):\n",
        "        \"\"\"\n",
        "        Compute aggregations for each individual phrase (6 + 1)\n",
        "        H_phrase = (1/N) ∑_positions H_i^(ℓ,h) where N = number of phrase positions\n",
        "        \"\"\"\n",
        "        print(\"Computing individual phrase aggregations...\")\n",
        "        print(f\"Analyzing: {', '.join(self.analysis_keys)}\")\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"Processing {phrase} aggregations...\")\n",
        "\n",
        "            # Phrase-specific aggregation\n",
        "            for row_prompt_key, entropy_values in self.hint_entropies_individual[phrase].items():\n",
        "                if entropy_values:\n",
        "                    # H_phrase = average entropy at phrase positions\n",
        "                    avg_entropy = np.mean(entropy_values)\n",
        "                    self.aggregated_entropies_individual[phrase][row_prompt_key] = float(avg_entropy)\n",
        "                else:\n",
        "                    self.aggregated_entropies_individual[phrase][row_prompt_key] = None\n",
        "\n",
        "            # Print statistics for this phrase\n",
        "            valid_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            if valid_values:\n",
        "                print(f\"  {phrase}: {len(valid_values)} valid instances, avg entropy = {np.mean(valid_values):.4f}\")\n",
        "            else:\n",
        "                print(f\"  {phrase}: No valid instances found\")\n",
        "\n",
        "    def compute_individual_phrase_delta_h(self):\n",
        "        \"\"\"\n",
        "        Compute ΔH changes for each individual phrase (6 + 1)\n",
        "        ΔH = H_baseline - H_variant per phrase\n",
        "        \"\"\"\n",
        "        print(\"Computing individual phrase ΔH changes...\")\n",
        "\n",
        "        phrase_delta_h_stats = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"Computing ΔH for {phrase}...\")\n",
        "\n",
        "            # Group data by row\n",
        "            rows_data = defaultdict(dict)\n",
        "            for row_prompt_key, entropy_value in self.aggregated_entropies_individual[phrase].items():\n",
        "                parts = row_prompt_key.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    row_idx = int(parts[1])\n",
        "                    prompt_type = f\"{parts[2]}_{parts[3]}\"\n",
        "                    rows_data[row_idx][prompt_type] = entropy_value\n",
        "\n",
        "            # Compute ΔH for each row\n",
        "            phrase_delta_h_values = []\n",
        "\n",
        "            for row_idx, row_data in rows_data.items():\n",
        "                row_changes = {}\n",
        "\n",
        "                # Prompt_2 vs Prompt_1 comparison\n",
        "                if 'prompt_1' in row_data and 'prompt_2' in row_data:\n",
        "                    if row_data['prompt_1'] is not None and row_data['prompt_2'] is not None:\n",
        "                        delta_h = row_data['prompt_1'] - row_data['prompt_2']\n",
        "                        row_changes['prompt2_vs_prompt1'] = delta_h\n",
        "                        phrase_delta_h_values.append(delta_h)\n",
        "\n",
        "                # Prompt_3 vs Prompt_1 comparison\n",
        "                if 'prompt_1' in row_data and 'prompt_3' in row_data:\n",
        "                    if row_data['prompt_1'] is not None and row_data['prompt_3'] is not None:\n",
        "                        delta_h = row_data['prompt_1'] - row_data['prompt_3']\n",
        "                        row_changes['prompt3_vs_prompt1'] = delta_h\n",
        "                        phrase_delta_h_values.append(delta_h)\n",
        "\n",
        "                if row_changes:\n",
        "                    self.delta_h_changes_individual[phrase][row_idx] = row_changes\n",
        "\n",
        "            # Calculate statistics for this phrase\n",
        "            if phrase_delta_h_values:\n",
        "                phrase_delta_h_stats[phrase] = {\n",
        "                    'mean': float(np.mean(phrase_delta_h_values)),\n",
        "                    'std': float(np.std(phrase_delta_h_values)),\n",
        "                    'negative_percentage': (sum(1 for x in phrase_delta_h_values if x < 0) / len(phrase_delta_h_values)) * 100,\n",
        "                    'count': len(phrase_delta_h_values)\n",
        "                }\n",
        "\n",
        "        # Print ΔH summary for each phrase\n",
        "        print(\"\\nΔH Summary by Individual Phrase:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            if phrase in phrase_delta_h_stats:\n",
        "                stats = phrase_delta_h_stats[phrase]\n",
        "                print(f\"  {phrase}: Mean ΔH = {stats['mean']:.4f}, Negative% = {stats['negative_percentage']:.1f}%, Count = {stats['count']}\")\n",
        "            else:\n",
        "                print(f\"  {phrase}: No valid ΔH values\")\n",
        "\n",
        "        return phrase_delta_h_stats\n",
        "\n",
        "    def compute_cross_phrase_comparison(self):\n",
        "        \"\"\"\n",
        "        Compare bias effects across individual phrases to identify most problematic bias types\n",
        "        \"\"\"\n",
        "        print(\"Computing cross-phrase comparison...\")\n",
        "\n",
        "        comparison_results = {}\n",
        "\n",
        "        # Collect aggregated entropy for each phrase\n",
        "        phrase_entropies = {}\n",
        "        phrase_delta_h = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Collect entropy values\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            if entropy_values:\n",
        "                phrase_entropies[phrase] = {\n",
        "                    'mean': float(np.mean(entropy_values)),\n",
        "                    'std': float(np.std(entropy_values)),\n",
        "                    'count': len(entropy_values)\n",
        "                }\n",
        "\n",
        "            # Collect ΔH values\n",
        "            all_delta_h = []\n",
        "            for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                all_delta_h.extend(row_changes.values())\n",
        "\n",
        "            if all_delta_h:\n",
        "                phrase_delta_h[phrase] = {\n",
        "                    'mean': float(np.mean(all_delta_h)),\n",
        "                    'std': float(np.std(all_delta_h)),\n",
        "                    'negative_percentage': (sum(1 for x in all_delta_h if x < 0) / len(all_delta_h)) * 100,\n",
        "                    'abs_mean': float(np.mean(np.abs(all_delta_h))),\n",
        "                    'count': len(all_delta_h)\n",
        "                }\n",
        "\n",
        "        comparison_results = {\n",
        "            'phrase_entropy_comparison': phrase_entropies,\n",
        "            'phrase_delta_h_comparison': phrase_delta_h,\n",
        "            'ranking_by_bias_strength': {},\n",
        "            'cross_phrase_insights': {}\n",
        "        }\n",
        "\n",
        "        # Rank phrases by bias strength (absolute mean ΔH)\n",
        "        if phrase_delta_h:\n",
        "            sorted_by_bias = sorted(phrase_delta_h.items(), key=lambda x: x[1]['abs_mean'], reverse=True)\n",
        "            comparison_results['ranking_by_bias_strength'] = {\n",
        "                'most_biased_phrases': [phrase for phrase, _ in sorted_by_bias[:3]],\n",
        "                'least_biased_phrases': [phrase for phrase, _ in sorted_by_bias[-3:]],\n",
        "                'full_ranking': [(phrase, stats['abs_mean']) for phrase, stats in sorted_by_bias]\n",
        "            }\n",
        "\n",
        "        # Generate insights\n",
        "        if phrase_delta_h:\n",
        "            # Find phrases with strongest negative bias (most unfaithful)\n",
        "            negative_bias_phrases = [\n",
        "                phrase for phrase, stats in phrase_delta_h.items()\n",
        "                if stats['negative_percentage'] > 70 and stats['mean'] < -0.001\n",
        "            ]\n",
        "\n",
        "            # Find phrases with positive bias (more faithful)\n",
        "            positive_bias_phrases = [\n",
        "                phrase for phrase, stats in phrase_delta_h.items()\n",
        "                if stats['negative_percentage'] < 30 and stats['mean'] > 0.001\n",
        "            ]\n",
        "\n",
        "            comparison_results['cross_phrase_insights'] = {\n",
        "                'strongly_negative_bias_phrases': negative_bias_phrases,\n",
        "                'positive_bias_phrases': positive_bias_phrases,\n",
        "                'interpretation': {\n",
        "                    'negative_bias': 'These phrases cause more focused/unfaithful attention',\n",
        "                    'positive_bias': 'These phrases cause more distributed/faithful attention'\n",
        "                }\n",
        "            }\n",
        "\n",
        "        print(\"\\nCross-phrase comparison completed!\")\n",
        "        if 'ranking_by_bias_strength' in comparison_results:\n",
        "            print(\"Phrases ranked by bias strength (most biased first):\")\n",
        "            for i, (phrase, bias_strength) in enumerate(comparison_results['ranking_by_bias_strength']['full_ranking'][:5], 1):\n",
        "                print(f\"  {i}. {phrase}: {bias_strength:.4f}\")\n",
        "\n",
        "        return comparison_results\n",
        "\n",
        "    def save_results(self, output_dir: str = \"../data/aggregated_analysis_individual\"):\n",
        "        \"\"\"Save all individual phrase aggregation and change results\"\"\"\n",
        "        print(f\"Saving individual phrase aggregation results to {output_dir}/\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save individual phrase aggregated entropies\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(output_dir, f\"aggregated_entropies_{phrase}.json\")\n",
        "            with open(phrase_file, 'w') as f:\n",
        "                json.dump(self.aggregated_entropies_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Save individual phrase ΔH changes\n",
        "        for phrase in self.analysis_keys:\n",
        "            delta_file = os.path.join(output_dir, f\"delta_h_changes_{phrase}.json\")\n",
        "            with open(delta_file, 'w') as f:\n",
        "                json.dump(self.delta_h_changes_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Compute and save ΔH statistics\n",
        "        phrase_delta_h_stats = self.compute_individual_phrase_delta_h()\n",
        "        stats_file = os.path.join(output_dir, \"phrase_delta_h_statistics.json\")\n",
        "        with open(stats_file, 'w') as f:\n",
        "            json.dump(phrase_delta_h_stats, f, indent=2)\n",
        "\n",
        "        # Compute and save cross-phrase comparison\n",
        "        cross_phrase_comparison = self.compute_cross_phrase_comparison()\n",
        "        comparison_file = os.path.join(output_dir, \"cross_phrase_comparison.json\")\n",
        "        with open(comparison_file, 'w') as f:\n",
        "            json.dump(cross_phrase_comparison, f, indent=2)\n",
        "\n",
        "        # Create comprehensive summary\n",
        "        summary = {\n",
        "            'analysis_structure': {\n",
        "                'individual_phrases': [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"],\n",
        "                'combined_analysis': \"combined\",\n",
        "                'total_analyses': len(self.analysis_keys)\n",
        "            },\n",
        "            'aggregation_methodology': {\n",
        "                'phrase_aggregation': 'H_phrase = (1/N) ∑_positions H_i^(ℓ,h)',\n",
        "                'change_calculation': 'ΔH = H_baseline - H_variant per phrase'\n",
        "            },\n",
        "            'interpretation': {\n",
        "                'negative_delta_h': 'Variants more focused/sharper at this phrase (unfaithful)',\n",
        "                'positive_delta_h': 'Variants less focused at this phrase (more faithful)',\n",
        "                'cross_phrase_comparison': 'Identifies which bias phrases are most problematic'\n",
        "            },\n",
        "            'data_counts': {\n",
        "                'analyses_completed': len(self.analysis_keys),\n",
        "                'aggregation_files': len(self.analysis_keys),\n",
        "                'delta_h_files': len(self.analysis_keys),\n",
        "                'comparison_analysis_available': True\n",
        "            },\n",
        "            'files_generated': {\n",
        "                'per_phrase_aggregated_entropies': [f\"aggregated_entropies_{phrase}.json\" for phrase in self.analysis_keys],\n",
        "                'per_phrase_delta_h_changes': [f\"delta_h_changes_{phrase}.json\" for phrase in self.analysis_keys],\n",
        "                'summary_files': ['phrase_delta_h_statistics.json', 'cross_phrase_comparison.json']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        summary_file = os.path.join(output_dir, \"aggregation_summary_individual.json\")\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase aggregation analysis completed successfully!\")\n",
        "        print(f\"Generated {len(self.analysis_keys)} separate phrase analyses:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"  - {phrase}: aggregated_entropies_{phrase}.json, delta_h_changes_{phrase}.json\")\n",
        "        print(\"Additional files:\")\n",
        "        print(\"  - phrase_delta_h_statistics.json\")\n",
        "        print(\"  - cross_phrase_comparison.json\")\n",
        "        print(\"Ready for individual phrase statistical analysis\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_aggregation_analysis_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase aggregation analysis pipeline\n",
        "    \"\"\"\n",
        "    # Configuration - Update this path for your setup\n",
        "    # ENTROPY_RESULTS_DIR = \"entropy_results_individual\"  # Contains individual phrase entropy files\n",
        "\n",
        "    print(\"Starting Individual Phrase Aggregation Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize aggregation analysis\n",
        "    processor = AggregationAnalysisIndividual(\n",
        "        entropy_results_dir=ENTROPY_RESULTS_DIR\n",
        "    )\n",
        "\n",
        "    # Compute individual phrase aggregations\n",
        "    processor.compute_individual_phrase_aggregations()\n",
        "\n",
        "    # Compute ΔH changes for each phrase\n",
        "    delta_h_stats = processor.compute_individual_phrase_delta_h()\n",
        "\n",
        "    # Compute cross-phrase comparison\n",
        "    cross_phrase_results = processor.compute_cross_phrase_comparison()\n",
        "\n",
        "    # Save all results\n",
        "    output_dir = processor.save_results()\n",
        "\n",
        "    print(f\"\\nIndividual phrase aggregation analysis completed: {output_dir}\")\n",
        "\n",
        "    return processor, delta_h_stats, cross_phrase_results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processor, delta_h_stats, cross_phrase_results = run_aggregation_analysis_individual()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform Individual Statistical Analysis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iM5arwsF2vgL",
        "outputId": "799a2a6f-41b0-4f89-9027-d133faf95d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Individual Phrase Statistical Analysis\n",
            "============================================================\n",
            "Starting Individual Phrase Statistical Analysis\n",
            "============================================================\n",
            "Loaded aggregated entropies for 'teacher': 20 instances\n",
            "Loaded aggregated entropies for 'own': 20 instances\n",
            "Loaded aggregated entropies for 'sure': 20 instances\n",
            "Loaded aggregated entropies for 'unsure': 20 instances\n",
            "Loaded aggregated entropies for 'quick': 20 instances\n",
            "Loaded aggregated entropies for 'fast': 20 instances\n",
            "Loaded aggregated entropies for 'step': 20 instances\n",
            "Loaded aggregated entropies for 'combined': 20 instances\n",
            "Loaded individual phrase data for 8 analyses\n",
            "Loaded results CSV: 8 rows\n",
            "Running complete individual phrase statistical analysis...\n",
            "Analyzing individual phrase patterns...\n",
            "Analyzing teacher patterns...\n",
            "Analyzing own patterns...\n",
            "Analyzing sure patterns...\n",
            "Analyzing unsure patterns...\n",
            "Analyzing quick patterns...\n",
            "Analyzing fast patterns...\n",
            "Analyzing step patterns...\n",
            "Analyzing combined patterns...\n",
            "Performing cross-phrase statistical tests...\n",
            "Performing phrase-specific correlations...\n",
            "Computing correlations for teacher...\n",
            "Computing correlations for own...\n",
            "Computing correlations for sure...\n",
            "Computing correlations for unsure...\n",
            "Computing correlations for quick...\n",
            "Computing correlations for fast...\n",
            "Computing correlations for step...\n",
            "Computing correlations for combined...\n",
            "Performing bias ranking analysis...\n",
            "\n",
            "Bias ranking analysis completed!\n",
            "Top 3 most biased phrases:\n",
            "  1. quick: 0.0054\n",
            "  2. fast: 0.0039\n",
            "  3. step: 0.0028\n",
            "Performing error checking for individual phrases...\n",
            "Complete individual phrase statistical analysis finished!\n",
            "Saving individual phrase statistical results to ../data/statistical_results_individual/\n",
            "Individual phrase statistical analysis completed successfully!\n",
            "Ready for individual phrase results visualization\n",
            "\n",
            "Individual phrase statistical analysis completed: ../data/statistical_results_individual\n"
          ]
        }
      ],
      "source": [
        "class StatisticalAnalysisIndividual:\n",
        "    def __init__(self, aggregated_analysis_dir: str, results_csv: str = None):\n",
        "        \"\"\"\n",
        "        Analyze and Compare Results for Individual Phrases (6 + 1)\n",
        "\n",
        "        Methodology:\n",
        "        - Individual phrase patterns: High/low entropy per phrase = faithful/unfaithful\n",
        "        - Cross-phrase statistical tests: Compare bias effects between phrases\n",
        "        - Phrase-specific correlations: Entropy vs accuracy per phrase\n",
        "        - Ranking analysis: Identify most problematic bias phrases\n",
        "\n",
        "        Args:\n",
        "            aggregated_analysis_dir: Directory containing individual phrase aggregation results\n",
        "            results_csv: Optional path to results CSV for accuracy correlation\n",
        "        \"\"\"\n",
        "        self.aggregated_analysis_dir = aggregated_analysis_dir\n",
        "        self.results_csv = results_csv\n",
        "\n",
        "        print(\"Starting Individual Phrase Statistical Analysis\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "\n",
        "        # Load individual phrase aggregation results\n",
        "        self.load_aggregation_data()\n",
        "\n",
        "        # Storage for analysis results\n",
        "        self.analysis_results = {\n",
        "            'individual_phrase_patterns': {},\n",
        "            'cross_phrase_statistical_tests': {},\n",
        "            'phrase_specific_correlations': {},\n",
        "            'bias_ranking_analysis': {},\n",
        "            'error_checks': {}\n",
        "        }\n",
        "\n",
        "    def load_aggregation_data(self):\n",
        "        \"\"\"Load individual phrase aggregated entropy data and ΔH changes\"\"\"\n",
        "        self.aggregated_entropies_individual = {}\n",
        "        self.delta_h_changes_individual = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Load aggregated entropies\n",
        "            agg_file = os.path.join(self.aggregated_analysis_dir, f\"aggregated_entropies_{phrase}.json\")\n",
        "            if os.path.exists(agg_file):\n",
        "                with open(agg_file, 'r') as f:\n",
        "                    self.aggregated_entropies_individual[phrase] = json.load(f)\n",
        "                print(f\"Loaded aggregated entropies for '{phrase}': {len(self.aggregated_entropies_individual[phrase])} instances\")\n",
        "            else:\n",
        "                print(f\"Warning: {agg_file} not found\")\n",
        "                self.aggregated_entropies_individual[phrase] = {}\n",
        "\n",
        "            # Load ΔH changes\n",
        "            delta_file = os.path.join(self.aggregated_analysis_dir, f\"delta_h_changes_{phrase}.json\")\n",
        "            if os.path.exists(delta_file):\n",
        "                with open(delta_file, 'r') as f:\n",
        "                    self.delta_h_changes_individual[phrase] = json.load(f)\n",
        "            else:\n",
        "                print(f\"Warning: {delta_file} not found\")\n",
        "                self.delta_h_changes_individual[phrase] = {}\n",
        "\n",
        "        # Load cross-phrase comparison\n",
        "        comparison_file = os.path.join(self.aggregated_analysis_dir, \"cross_phrase_comparison.json\")\n",
        "        if os.path.exists(comparison_file):\n",
        "            with open(comparison_file, 'r') as f:\n",
        "                self.cross_phrase_comparison = json.load(f)\n",
        "        else:\n",
        "            print(\"Warning: cross_phrase_comparison.json not found\")\n",
        "            self.cross_phrase_comparison = {}\n",
        "\n",
        "        print(f\"Loaded individual phrase data for {len(self.aggregated_entropies_individual)} analyses\")\n",
        "\n",
        "        # Optionally load results CSV for accuracy correlation\n",
        "        if self.results_csv and os.path.exists(self.results_csv):\n",
        "            try:\n",
        "                self.results_df = pd.read_csv(self.results_csv)\n",
        "                print(f\"Loaded results CSV: {len(self.results_df)} rows\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not load results CSV: {e}\")\n",
        "                self.results_df = None\n",
        "        else:\n",
        "            self.results_df = None\n",
        "\n",
        "    def analyze_individual_phrase_patterns(self):\n",
        "        \"\"\"\n",
        "        Pattern Analysis: High/low entropy per phrase = faithful/unfaithful\n",
        "        \"\"\"\n",
        "        print(\"Analyzing individual phrase patterns...\")\n",
        "\n",
        "        individual_patterns = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"Analyzing {phrase} patterns...\")\n",
        "\n",
        "            # Entropy patterns for this phrase\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "\n",
        "            if entropy_values:\n",
        "                individual_patterns[phrase] = {\n",
        "                    'entropy_stats': {\n",
        "                        'mean': float(np.mean(entropy_values)),\n",
        "                        'std': float(np.std(entropy_values)),\n",
        "                        'min': float(np.min(entropy_values)),\n",
        "                        'max': float(np.max(entropy_values)),\n",
        "                        'count': len(entropy_values),\n",
        "                        'interpretation': f'Higher values indicate more faithful attention at {phrase} positions'\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                # Analyze distribution by prompt type\n",
        "                entropy_by_prompt = {'prompt_1': [], 'prompt_2': [], 'prompt_3': []}\n",
        "                for key, value in self.aggregated_entropies_individual[phrase].items():\n",
        "                    if value is not None:\n",
        "                        for prompt_type in entropy_by_prompt.keys():\n",
        "                            if prompt_type in key:\n",
        "                                entropy_by_prompt[prompt_type].append(value)\n",
        "\n",
        "                individual_patterns[phrase]['entropy_by_prompt_type'] = {}\n",
        "                for prompt_type, values in entropy_by_prompt.items():\n",
        "                    if values:\n",
        "                        individual_patterns[phrase]['entropy_by_prompt_type'][prompt_type] = {\n",
        "                            'mean': float(np.mean(values)),\n",
        "                            'std': float(np.std(values)),\n",
        "                            'count': len(values)\n",
        "                        }\n",
        "\n",
        "                # ΔH patterns for this phrase\n",
        "                if phrase in self.delta_h_changes_individual:\n",
        "                    all_delta_h = []\n",
        "                    delta_h_by_comparison = {}\n",
        "\n",
        "                    for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                        for change_key, change_value in row_changes.items():\n",
        "                            if isinstance(change_value, (int, float)):\n",
        "                                all_delta_h.append(change_value)\n",
        "\n",
        "                                if change_key not in delta_h_by_comparison:\n",
        "                                    delta_h_by_comparison[change_key] = []\n",
        "                                delta_h_by_comparison[change_key].append(change_value)\n",
        "\n",
        "                    if all_delta_h:\n",
        "                        individual_patterns[phrase]['delta_h_patterns'] = {\n",
        "                            'overall': {\n",
        "                                'mean': float(np.mean(all_delta_h)),\n",
        "                                'std': float(np.std(all_delta_h)),\n",
        "                                'negative_count': sum(1 for x in all_delta_h if x < 0),\n",
        "                                'positive_count': sum(1 for x in all_delta_h if x > 0),\n",
        "                                'total_count': len(all_delta_h),\n",
        "                                'negative_percentage': (sum(1 for x in all_delta_h if x < 0) / len(all_delta_h)) * 100,\n",
        "                                'interpretation': f'Negative ΔH = variants more focused at {phrase} positions (unfaithful)'\n",
        "                            },\n",
        "                            'by_comparison': {}\n",
        "                        }\n",
        "\n",
        "                        for comparison, values in delta_h_by_comparison.items():\n",
        "                            individual_patterns[phrase]['delta_h_patterns']['by_comparison'][comparison] = {\n",
        "                                'mean': float(np.mean(values)),\n",
        "                                'std': float(np.std(values)),\n",
        "                                'negative_percentage': (sum(1 for x in values if x < 0) / len(values)) * 100,\n",
        "                                'count': len(values)\n",
        "                            }\n",
        "\n",
        "        self.analysis_results['individual_phrase_patterns'] = individual_patterns\n",
        "        return individual_patterns\n",
        "\n",
        "    def perform_cross_phrase_statistical_tests(self):\n",
        "        \"\"\"\n",
        "        Cross-phrase Statistical Tests: Compare bias effects between phrases\n",
        "        \"\"\"\n",
        "        print(\"Performing cross-phrase statistical tests...\")\n",
        "\n",
        "        cross_phrase_tests = {}\n",
        "\n",
        "        # Collect entropy values for each phrase\n",
        "        phrase_entropies = {}\n",
        "        phrase_delta_h = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Entropy values\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            if entropy_values:\n",
        "                phrase_entropies[phrase] = entropy_values\n",
        "\n",
        "            # ΔH values\n",
        "            delta_h_values = []\n",
        "            if phrase in self.delta_h_changes_individual:\n",
        "                for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                    delta_h_values.extend(row_changes.values())\n",
        "            if delta_h_values:\n",
        "                phrase_delta_h[phrase] = delta_h_values\n",
        "\n",
        "        # Pairwise comparisons between phrases (entropy)\n",
        "        cross_phrase_tests['entropy_comparisons'] = {}\n",
        "        phrase_names = list(phrase_entropies.keys())\n",
        "\n",
        "        for i in range(len(phrase_names)):\n",
        "            for j in range(i + 1, len(phrase_names)):\n",
        "                phrase1, phrase2 = phrase_names[i], phrase_names[j]\n",
        "\n",
        "                try:\n",
        "                    t_stat, p_value = ttest_ind(phrase_entropies[phrase1], phrase_entropies[phrase2])\n",
        "                    cross_phrase_tests['entropy_comparisons'][f\"{phrase1}_vs_{phrase2}\"] = {\n",
        "                        't_statistic': float(t_stat),\n",
        "                        'p_value': float(p_value),\n",
        "                        'significant': p_value < 0.05,\n",
        "                        'mean_diff': float(np.mean(phrase_entropies[phrase1]) - np.mean(phrase_entropies[phrase2])),\n",
        "                        'effect_size': abs(np.mean(phrase_entropies[phrase1]) - np.mean(phrase_entropies[phrase2])) /\n",
        "                                      np.sqrt((np.var(phrase_entropies[phrase1]) + np.var(phrase_entropies[phrase2])) / 2),\n",
        "                        'interpretation': f'Comparing entropy between {phrase1} and {phrase2} bias phrases'\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"Entropy comparison failed for {phrase1} vs {phrase2}: {e}\")\n",
        "\n",
        "        # Pairwise comparisons between phrases (ΔH)\n",
        "        cross_phrase_tests['delta_h_comparisons'] = {}\n",
        "        delta_h_phrase_names = list(phrase_delta_h.keys())\n",
        "\n",
        "        for i in range(len(delta_h_phrase_names)):\n",
        "            for j in range(i + 1, len(delta_h_phrase_names)):\n",
        "                phrase1, phrase2 = delta_h_phrase_names[i], delta_h_phrase_names[j]\n",
        "\n",
        "                try:\n",
        "                    t_stat, p_value = ttest_ind(phrase_delta_h[phrase1], phrase_delta_h[phrase2])\n",
        "                    cross_phrase_tests['delta_h_comparisons'][f\"{phrase1}_vs_{phrase2}\"] = {\n",
        "                        't_statistic': float(t_stat),\n",
        "                        'p_value': float(p_value),\n",
        "                        'significant': p_value < 0.05,\n",
        "                        'mean_diff': float(np.mean(phrase_delta_h[phrase1]) - np.mean(phrase_delta_h[phrase2])),\n",
        "                        'effect_size': abs(np.mean(phrase_delta_h[phrase1]) - np.mean(phrase_delta_h[phrase2])) /\n",
        "                                      np.sqrt((np.var(phrase_delta_h[phrase1]) + np.var(phrase_delta_h[phrase2])) / 2),\n",
        "                        'interpretation': f'Comparing ΔH bias effects between {phrase1} and {phrase2}'\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"ΔH comparison failed for {phrase1} vs {phrase2}: {e}\")\n",
        "\n",
        "        # One-way ANOVA test across all phrases\n",
        "        if len(phrase_entropies) > 2:\n",
        "            try:\n",
        "                entropy_groups = [phrase_entropies[phrase] for phrase in phrase_entropies.keys()]\n",
        "                h_stat, p_value = kruskal(*entropy_groups)\n",
        "                cross_phrase_tests['overall_entropy_test'] = {\n",
        "                    'test': 'Kruskal-Wallis H-test (non-parametric ANOVA)',\n",
        "                    'h_statistic': float(h_stat),\n",
        "                    'p_value': float(p_value),\n",
        "                    'significant': p_value < 0.05,\n",
        "                    'interpretation': 'Tests if entropy distributions differ significantly across phrases',\n",
        "                    'phrases_tested': list(phrase_entropies.keys())\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Overall entropy test failed: {e}\")\n",
        "\n",
        "        # One-way test for ΔH across all phrases\n",
        "        if len(phrase_delta_h) > 2:\n",
        "            try:\n",
        "                delta_h_groups = [phrase_delta_h[phrase] for phrase in phrase_delta_h.keys()]\n",
        "                h_stat, p_value = kruskal(*delta_h_groups)\n",
        "                cross_phrase_tests['overall_delta_h_test'] = {\n",
        "                    'test': 'Kruskal-Wallis H-test (non-parametric ANOVA)',\n",
        "                    'h_statistic': float(h_stat),\n",
        "                    'p_value': float(p_value),\n",
        "                    'significant': p_value < 0.05,\n",
        "                    'interpretation': 'Tests if ΔH distributions differ significantly across phrases',\n",
        "                    'phrases_tested': list(phrase_delta_h.keys())\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Overall ΔH test failed: {e}\")\n",
        "\n",
        "        # One-sample t-tests for each phrase ΔH against zero\n",
        "        cross_phrase_tests['phrase_vs_zero_tests'] = {}\n",
        "        for phrase, delta_h_values in phrase_delta_h.items():\n",
        "            if len(delta_h_values) >= 3:\n",
        "                try:\n",
        "                    t_stat, p_value = ttest_1samp(delta_h_values, 0)\n",
        "                    cross_phrase_tests['phrase_vs_zero_tests'][phrase] = {\n",
        "                        't_statistic': float(t_stat),\n",
        "                        'p_value': float(p_value),\n",
        "                        'significant': p_value < 0.05,\n",
        "                        'mean_delta_h': float(np.mean(delta_h_values)),\n",
        "                        'interpretation': f'Testing if {phrase} ΔH is significantly different from zero'\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"One-sample test failed for {phrase}: {e}\")\n",
        "\n",
        "        self.analysis_results['cross_phrase_statistical_tests'] = cross_phrase_tests\n",
        "        return cross_phrase_tests\n",
        "\n",
        "    def perform_phrase_specific_correlations(self):\n",
        "        \"\"\"\n",
        "        Phrase-specific Correlations: Entropy vs accuracy per phrase\n",
        "        \"\"\"\n",
        "        print(\"Performing phrase-specific correlations...\")\n",
        "\n",
        "        phrase_correlations = {}\n",
        "\n",
        "        if self.results_df is not None:\n",
        "            try:\n",
        "                for phrase in self.analysis_keys:\n",
        "                    print(f\"Computing correlations for {phrase}...\")\n",
        "\n",
        "                    correlation_data = []\n",
        "\n",
        "                    for index, row in self.results_df.iterrows():\n",
        "                        row_idx = row.get('Index', index)\n",
        "\n",
        "                        for prompt_col in ['Prompt_1', 'Prompt_2', 'Prompt_3']:\n",
        "                            prompt_key = f\"row_{row_idx}_{prompt_col.lower()}\"\n",
        "\n",
        "                            # Get phrase-specific entropy\n",
        "                            phrase_entropy = self.aggregated_entropies_individual[phrase].get(prompt_key)\n",
        "\n",
        "                            prediction_col = f\"{prompt_col}_Prediction_Extracted\"\n",
        "                            if prediction_col in row and pd.notna(row[prediction_col]):\n",
        "                                prediction = str(row[prediction_col])\n",
        "                                has_numerical_answer = prediction.replace('.', '').isdigit()\n",
        "\n",
        "                                if phrase_entropy is not None:\n",
        "                                    correlation_data.append({\n",
        "                                        'phrase_entropy': phrase_entropy,\n",
        "                                        'has_numerical_answer': has_numerical_answer,\n",
        "                                        'prediction': prediction,\n",
        "                                        'prompt_type': prompt_col\n",
        "                                    })\n",
        "\n",
        "                    if len(correlation_data) > 3:\n",
        "                        corr_df = pd.DataFrame(correlation_data)\n",
        "\n",
        "                        try:\n",
        "                            # Phrase entropy vs accuracy\n",
        "                            rho, p_value = spearmanr(\n",
        "                                corr_df['phrase_entropy'],\n",
        "                                corr_df['has_numerical_answer'].astype(int)\n",
        "                            )\n",
        "\n",
        "                            phrase_correlations[phrase] = {\n",
        "                                'entropy_vs_accuracy': {\n",
        "                                    'spearman_rho': float(rho),\n",
        "                                    'p_value': float(p_value),\n",
        "                                    'significant': p_value < 0.05,\n",
        "                                    'interpretation': f'Correlation between {phrase} entropy and prediction accuracy'\n",
        "                                },\n",
        "                                'data_summary': {\n",
        "                                    'total_data_points': len(corr_df),\n",
        "                                    'entropy_range': [float(corr_df['phrase_entropy'].min()), float(corr_df['phrase_entropy'].max())],\n",
        "                                    'accuracy_rate': float(corr_df['has_numerical_answer'].mean()),\n",
        "                                    'prompt_type_distribution': corr_df['prompt_type'].value_counts().to_dict()\n",
        "                                }\n",
        "                            }\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Correlation calculation failed for {phrase}: {e}\")\n",
        "                            phrase_correlations[phrase] = {'error': str(e)}\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Phrase-specific correlation analysis failed: {e}\")\n",
        "                phrase_correlations['error'] = str(e)\n",
        "\n",
        "        else:\n",
        "            phrase_correlations['note'] = 'Results CSV not available - correlation analysis skipped'\n",
        "\n",
        "        self.analysis_results['phrase_specific_correlations'] = phrase_correlations\n",
        "        return phrase_correlations\n",
        "\n",
        "    def perform_bias_ranking_analysis(self):\n",
        "        \"\"\"\n",
        "        Bias Ranking Analysis: Identify most problematic bias phrases\n",
        "        \"\"\"\n",
        "        print(\"Performing bias ranking analysis...\")\n",
        "\n",
        "        ranking_analysis = {}\n",
        "\n",
        "        # Collect metrics for ranking\n",
        "        phrase_metrics = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            if phrase == 'combined':\n",
        "                continue  # Skip combined for individual ranking\n",
        "\n",
        "            # Get ΔH values\n",
        "            all_delta_h = []\n",
        "            if phrase in self.delta_h_changes_individual:\n",
        "                for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                    all_delta_h.extend(row_changes.values())\n",
        "\n",
        "            # Get entropy values\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "\n",
        "            if all_delta_h and entropy_values:\n",
        "                phrase_metrics[phrase] = {\n",
        "                    'mean_delta_h': float(np.mean(all_delta_h)),\n",
        "                    'abs_mean_delta_h': float(np.mean(np.abs(all_delta_h))),\n",
        "                    'negative_percentage': (sum(1 for x in all_delta_h if x < 0) / len(all_delta_h)) * 100,\n",
        "                    'delta_h_std': float(np.std(all_delta_h)),\n",
        "                    'mean_entropy': float(np.mean(entropy_values)),\n",
        "                    'entropy_std': float(np.std(entropy_values)),\n",
        "                    'data_coverage': len(all_delta_h)\n",
        "                }\n",
        "\n",
        "        # Ranking by different criteria\n",
        "        if phrase_metrics:\n",
        "            # 1. Most biased (highest absolute ΔH)\n",
        "            most_biased = sorted(phrase_metrics.items(), key=lambda x: x[1]['abs_mean_delta_h'], reverse=True)\n",
        "\n",
        "            # 2. Most unfaithful (most negative ΔH)\n",
        "            most_unfaithful = sorted(phrase_metrics.items(), key=lambda x: x[1]['mean_delta_h'])\n",
        "\n",
        "            # 3. Most consistent bias (highest negative percentage)\n",
        "            most_consistent_bias = sorted(phrase_metrics.items(), key=lambda x: x[1]['negative_percentage'], reverse=True)\n",
        "\n",
        "            # 4. Lowest average entropy (most focused attention)\n",
        "            lowest_entropy = sorted(phrase_metrics.items(), key=lambda x: x[1]['mean_entropy'])\n",
        "\n",
        "            ranking_analysis = {\n",
        "                'phrase_metrics': phrase_metrics,\n",
        "                'rankings': {\n",
        "                    'most_biased_phrases': {\n",
        "                        'ranking': [(phrase, metrics['abs_mean_delta_h']) for phrase, metrics in most_biased],\n",
        "                        'top_3': [phrase for phrase, _ in most_biased[:3]],\n",
        "                        'criterion': 'Highest absolute mean ΔH'\n",
        "                    },\n",
        "                    'most_unfaithful_phrases': {\n",
        "                        'ranking': [(phrase, metrics['mean_delta_h']) for phrase, metrics in most_unfaithful],\n",
        "                        'top_3': [phrase for phrase, _ in most_unfaithful[:3]],\n",
        "                        'criterion': 'Most negative mean ΔH'\n",
        "                    },\n",
        "                    'most_consistent_bias': {\n",
        "                        'ranking': [(phrase, metrics['negative_percentage']) for phrase, metrics in most_consistent_bias],\n",
        "                        'top_3': [phrase for phrase, _ in most_consistent_bias[:3]],\n",
        "                        'criterion': 'Highest percentage of negative ΔH values'\n",
        "                    },\n",
        "                    'lowest_entropy_phrases': {\n",
        "                        'ranking': [(phrase, metrics['mean_entropy']) for phrase, metrics in lowest_entropy],\n",
        "                        'top_3': [phrase for phrase, _ in lowest_entropy[:3]],\n",
        "                        'criterion': 'Lowest average entropy (most focused attention)'\n",
        "                    }\n",
        "                },\n",
        "                'summary_insights': {\n",
        "                    'most_problematic_overall': most_biased[0][0] if most_biased else None,\n",
        "                    'strongest_unfaithfulness': most_unfaithful[0][0] if most_unfaithful else None,\n",
        "                    'most_reliable_bias_indicator': most_consistent_bias[0][0] if most_consistent_bias else None\n",
        "                }\n",
        "            }\n",
        "\n",
        "        print(\"\\nBias ranking analysis completed!\")\n",
        "        if 'rankings' in ranking_analysis:\n",
        "            print(\"Top 3 most biased phrases:\")\n",
        "            for i, phrase in enumerate(ranking_analysis['rankings']['most_biased_phrases']['top_3'], 1):\n",
        "                bias_score = phrase_metrics[phrase]['abs_mean_delta_h']\n",
        "                print(f\"  {i}. {phrase}: {bias_score:.4f}\")\n",
        "\n",
        "        self.analysis_results['bias_ranking_analysis'] = ranking_analysis\n",
        "        return ranking_analysis\n",
        "\n",
        "    def perform_error_checking(self):\n",
        "        \"\"\"\n",
        "        Error Check: Handle missing data, validate phrase coverage\n",
        "        \"\"\"\n",
        "        print(\"Performing error checking for individual phrases...\")\n",
        "\n",
        "        error_checks = {}\n",
        "\n",
        "        # Check data coverage for each phrase\n",
        "        phrase_coverage = {}\n",
        "        for phrase in self.analysis_keys:\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            total_instances = len(self.aggregated_entropies_individual[phrase])\n",
        "\n",
        "            delta_h_count = 0\n",
        "            if phrase in self.delta_h_changes_individual:\n",
        "                delta_h_count = sum(len(row_changes) for row_changes in self.delta_h_changes_individual[phrase].values())\n",
        "\n",
        "            phrase_coverage[phrase] = {\n",
        "                'entropy_instances': len(entropy_values),\n",
        "                'total_possible_instances': total_instances,\n",
        "                'entropy_coverage_rate': len(entropy_values) / total_instances if total_instances > 0 else 0,\n",
        "                'delta_h_comparisons': delta_h_count,\n",
        "                'zero_entropy_count': sum(1 for v in entropy_values if v == 0.0),\n",
        "                'very_low_entropy_count': sum(1 for v in entropy_values if 0 < v < 0.1),\n",
        "                'very_high_entropy_count': sum(1 for v in entropy_values if v > 5.0)\n",
        "            }\n",
        "\n",
        "        error_checks['phrase_coverage'] = phrase_coverage\n",
        "\n",
        "        # Identify phrases with insufficient data\n",
        "        low_coverage_phrases = [\n",
        "            phrase for phrase, stats in phrase_coverage.items()\n",
        "            if stats['entropy_coverage_rate'] < 0.1\n",
        "        ]\n",
        "\n",
        "        error_checks['data_quality_issues'] = {\n",
        "            'low_coverage_phrases': low_coverage_phrases,\n",
        "            'phrases_with_no_delta_h': [\n",
        "                phrase for phrase, stats in phrase_coverage.items()\n",
        "                if stats['delta_h_comparisons'] == 0\n",
        "            ],\n",
        "            'interpretation': 'Phrases with low coverage may have unreliable statistics'\n",
        "        }\n",
        "\n",
        "        # Cross-phrase consistency check\n",
        "        if len(self.analysis_keys) > 1:\n",
        "            entropy_ranges = {}\n",
        "            for phrase in self.analysis_keys:\n",
        "                entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "                if entropy_values:\n",
        "                    entropy_ranges[phrase] = {\n",
        "                        'min': float(np.min(entropy_values)),\n",
        "                        'max': float(np.max(entropy_values)),\n",
        "                        'range': float(np.max(entropy_values) - np.min(entropy_values))\n",
        "                    }\n",
        "\n",
        "            error_checks['cross_phrase_consistency'] = {\n",
        "                'entropy_ranges': entropy_ranges,\n",
        "                'range_consistency': 'Good' if len(set(round(stats['range'], 2) for stats in entropy_ranges.values())) <= 3 else 'Variable'\n",
        "            }\n",
        "\n",
        "        self.analysis_results['error_checks'] = error_checks\n",
        "        return error_checks\n",
        "\n",
        "    def run_complete_analysis(self):\n",
        "        \"\"\"Run the complete individual phrase statistical analysis pipeline\"\"\"\n",
        "        print(\"Running complete individual phrase statistical analysis...\")\n",
        "\n",
        "        # Run all analysis components\n",
        "        individual_patterns = self.analyze_individual_phrase_patterns()\n",
        "        cross_phrase_tests = self.perform_cross_phrase_statistical_tests()\n",
        "        phrase_correlations = self.perform_phrase_specific_correlations()\n",
        "        bias_ranking = self.perform_bias_ranking_analysis()\n",
        "        error_checks = self.perform_error_checking()\n",
        "\n",
        "        # Extract key findings\n",
        "        key_findings = self.extract_key_findings()\n",
        "\n",
        "        # Create comprehensive summary\n",
        "        summary = {\n",
        "            'components_completed': [\n",
        "                'individual_phrase_patterns',\n",
        "                'cross_phrase_statistical_tests',\n",
        "                'phrase_specific_correlations',\n",
        "                'bias_ranking_analysis',\n",
        "                'error_checks'\n",
        "            ],\n",
        "            'key_findings': key_findings,\n",
        "            'methodology_compliance': {\n",
        "                'individual_patterns_analyzed': 'High/low entropy faithfulness per phrase completed',\n",
        "                'cross_phrase_tests': 'Statistical comparisons between phrases completed',\n",
        "                'phrase_correlations': 'Entropy vs accuracy per phrase completed',\n",
        "                'bias_ranking': 'Most problematic phrases identified',\n",
        "                'error_checking': 'Data quality and coverage validated'\n",
        "            },\n",
        "            'analyses_completed': len(self.analysis_keys)\n",
        "        }\n",
        "\n",
        "        self.analysis_results['summary'] = summary\n",
        "\n",
        "        print(\"Complete individual phrase statistical analysis finished!\")\n",
        "        return self.analysis_results\n",
        "\n",
        "    def extract_key_findings(self):\n",
        "        \"\"\"Extract key findings from the individual phrase analysis\"\"\"\n",
        "        findings = {}\n",
        "\n",
        "        # Individual phrase findings\n",
        "        if 'individual_phrase_patterns' in self.analysis_results:\n",
        "            patterns = self.analysis_results['individual_phrase_patterns']\n",
        "\n",
        "            phrase_bias_scores = {}\n",
        "            for phrase, data in patterns.items():\n",
        "                if 'delta_h_patterns' in data and 'overall' in data['delta_h_patterns']:\n",
        "                    delta_patterns = data['delta_h_patterns']['overall']\n",
        "                    phrase_bias_scores[phrase] = {\n",
        "                        'negative_percentage': delta_patterns['negative_percentage'],\n",
        "                        'mean_delta_h': delta_patterns['mean'],\n",
        "                        'bias_detected': delta_patterns['negative_count'] > delta_patterns['positive_count']\n",
        "                    }\n",
        "\n",
        "            findings['individual_phrase_bias'] = phrase_bias_scores\n",
        "\n",
        "        # Cross-phrase comparison findings\n",
        "        if 'cross_phrase_statistical_tests' in self.analysis_results:\n",
        "            tests = self.analysis_results['cross_phrase_statistical_tests']\n",
        "\n",
        "            significant_comparisons = []\n",
        "            if 'entropy_comparisons' in tests:\n",
        "                significant_comparisons.extend([\n",
        "                    k for k, v in tests['entropy_comparisons'].items()\n",
        "                    if v.get('significant', False)\n",
        "                ])\n",
        "            if 'delta_h_comparisons' in tests:\n",
        "                significant_comparisons.extend([\n",
        "                    k for k, v in tests['delta_h_comparisons'].items()\n",
        "                    if v.get('significant', False)\n",
        "                ])\n",
        "\n",
        "            findings['cross_phrase_differences'] = {\n",
        "                'significant_comparisons': significant_comparisons,\n",
        "                'total_comparisons': len(tests.get('entropy_comparisons', {})) + len(tests.get('delta_h_comparisons', {})),\n",
        "                'evidence_strength': 'Strong' if len(significant_comparisons) > 2 else 'Moderate'\n",
        "            }\n",
        "\n",
        "        # Bias ranking findings\n",
        "        if 'bias_ranking_analysis' in self.analysis_results:\n",
        "            ranking = self.analysis_results['bias_ranking_analysis']\n",
        "\n",
        "            if 'rankings' in ranking:\n",
        "                findings['most_problematic_phrases'] = {\n",
        "                    'most_biased': ranking['rankings']['most_biased_phrases']['top_3'],\n",
        "                    'most_unfaithful': ranking['rankings']['most_unfaithful_phrases']['top_3'],\n",
        "                    'most_consistent_bias': ranking['rankings']['most_consistent_bias']['top_3']\n",
        "                }\n",
        "\n",
        "        return findings\n",
        "\n",
        "    def save_results(self, output_dir: str = \"../data/statistical_results_individual\"):\n",
        "        \"\"\"Save individual phrase statistical analysis results\"\"\"\n",
        "        print(f\"Saving individual phrase statistical results to {output_dir}/\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Convert numpy types to native Python types for JSON serialization\n",
        "        def convert_numpy_types(obj):\n",
        "            if isinstance(obj, dict):\n",
        "                return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
        "            elif isinstance(obj, list):\n",
        "                return [convert_numpy_types(v) for v in obj]\n",
        "            elif isinstance(obj, np.integer):\n",
        "                return int(obj)\n",
        "            elif isinstance(obj, np.floating):\n",
        "                return float(obj)\n",
        "            elif isinstance(obj, np.bool_):\n",
        "                return bool(obj)\n",
        "            elif isinstance(obj, np.ndarray):\n",
        "                return obj.tolist()\n",
        "            else:\n",
        "                return obj\n",
        "\n",
        "        # Convert analysis results\n",
        "        serializable_results = convert_numpy_types(self.analysis_results)\n",
        "\n",
        "        # Save complete analysis results\n",
        "        results_file = os.path.join(output_dir, \"statistical_analysis_individual_results.json\")\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump(serializable_results, f, indent=2)\n",
        "\n",
        "        # Save individual components\n",
        "        components = [\n",
        "            'individual_phrase_patterns',\n",
        "            'cross_phrase_statistical_tests',\n",
        "            'phrase_specific_correlations',\n",
        "            'bias_ranking_analysis',\n",
        "            'error_checks'\n",
        "        ]\n",
        "\n",
        "        for component in components:\n",
        "            if component in serializable_results:\n",
        "                component_file = os.path.join(output_dir, f\"{component}.json\")\n",
        "                with open(component_file, 'w') as f:\n",
        "                    json.dump(serializable_results[component], f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase statistical analysis completed successfully!\")\n",
        "        print(\"Ready for individual phrase results visualization\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_statistical_analysis_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase statistical analysis pipeline\n",
        "    \"\"\"\n",
        "    # Configuration - Update these paths for your setup\n",
        "\n",
        "    print(\"Starting Individual Phrase Statistical Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = StatisticalAnalysisIndividual(\n",
        "        aggregated_analysis_dir=AGGREGATED_ANALYSIS_DIR,\n",
        "        results_csv=RESULTS_CSV\n",
        "    )\n",
        "\n",
        "    # Run complete analysis\n",
        "    results = analyzer.run_complete_analysis()\n",
        "\n",
        "    # Save all results\n",
        "    output_dir = analyzer.save_results()\n",
        "\n",
        "    print(f\"\\nIndividual phrase statistical analysis completed: {output_dir}\")\n",
        "    return analyzer, results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer, results = run_statistical_analysis_individual()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPSuEU-R2zWH",
        "outputId": "89030947-c23d-4f07-ff17-18f51c2e6342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Individual Phrase Results Visualization\n",
            "============================================================\n",
            "Starting Individual Phrase Results Visualization\n",
            "============================================================\n",
            "Loaded visualization data for 8 phrase analyses\n",
            "Running complete individual phrase visualization pipeline...\n",
            "Generating 7 separate PNG files (1 combined + 6 individual phrases)\n",
            "Creating individual visualization for phrase: combined\n",
            "Creating histogram for combined: Phrase entropy distributions vs baseline...\n",
            "Available keys for combined: ['row_0_prompt_1', 'row_1_prompt_1', 'row_1_prompt_2', 'row_1_prompt_3', 'row_2_prompt_1', 'row_2_prompt_2', 'row_2_prompt_3', 'row_3_prompt_1', 'row_3_prompt_2', 'row_3_prompt_3', 'row_4_prompt_1', 'row_4_prompt_2', 'row_4_prompt_3', 'row_5_prompt_1', 'row_5_prompt_2', 'row_5_prompt_3', 'row_6_prompt_1', 'row_6_prompt_2', 'row_6_prompt_3', 'row_7_prompt_1']\n",
            "Identified baseline rows: ['0', '7']\n",
            "Creating line plot for combined: Avg entropy vs layers...\n",
            "Creating bar chart for combined: Avg phrase entropy by variant...\n",
            "Creating heatmap for combined: Computing layer-wise ΔH...\n",
            "Saved combined plot: ../figures/visualizations_individual1/combined_bias_analysis.png\n",
            "Creating individual visualization for phrase: teacher\n",
            "Creating histogram for teacher: Phrase entropy distributions vs baseline...\n",
            "Available keys for teacher: ['row_0_prompt_1', 'row_1_prompt_1', 'row_1_prompt_2', 'row_1_prompt_3', 'row_2_prompt_1', 'row_2_prompt_2', 'row_2_prompt_3', 'row_3_prompt_1', 'row_3_prompt_2', 'row_3_prompt_3', 'row_4_prompt_1', 'row_4_prompt_2', 'row_4_prompt_3', 'row_5_prompt_1', 'row_5_prompt_2', 'row_5_prompt_3', 'row_6_prompt_1', 'row_6_prompt_2', 'row_6_prompt_3', 'row_7_prompt_1']\n",
            "Identified baseline rows: ['0', '7']\n",
            "Creating line plot for teacher: Avg entropy vs layers...\n",
            "Creating bar chart for teacher: Avg phrase entropy by variant...\n",
            "Creating heatmap for teacher: Computing layer-wise ΔH...\n",
            "Saved teacher plot: ../figures/visualizations_individual1/teacher_bias_analysis.png\n",
            "Creating individual visualization for phrase: own\n",
            "Creating histogram for own: Phrase entropy distributions vs baseline...\n",
            "Available keys for own: ['row_0_prompt_1', 'row_1_prompt_1', 'row_1_prompt_2', 'row_1_prompt_3', 'row_2_prompt_1', 'row_2_prompt_2', 'row_2_prompt_3', 'row_3_prompt_1', 'row_3_prompt_2', 'row_3_prompt_3', 'row_4_prompt_1', 'row_4_prompt_2', 'row_4_prompt_3', 'row_5_prompt_1', 'row_5_prompt_2', 'row_5_prompt_3', 'row_6_prompt_1', 'row_6_prompt_2', 'row_6_prompt_3', 'row_7_prompt_1']\n",
            "Identified baseline rows: ['0', '7']\n",
            "Creating line plot for own: Avg entropy vs layers...\n",
            "Creating bar chart for own: Avg phrase entropy by variant...\n",
            "Creating heatmap for own: Computing layer-wise ΔH...\n",
            "Saved own plot: ../figures/visualizations_individual1/own_bias_analysis.png\n",
            "Creating individual visualization for phrase: sure\n",
            "Creating histogram for sure: Phrase entropy distributions vs baseline...\n",
            "Available keys for sure: ['row_0_prompt_1', 'row_1_prompt_1', 'row_1_prompt_2', 'row_1_prompt_3', 'row_2_prompt_1', 'row_2_prompt_2', 'row_2_prompt_3', 'row_3_prompt_1', 'row_3_prompt_2', 'row_3_prompt_3', 'row_4_prompt_1', 'row_4_prompt_2', 'row_4_prompt_3', 'row_5_prompt_1', 'row_5_prompt_2', 'row_5_prompt_3', 'row_6_prompt_1', 'row_6_prompt_2', 'row_6_prompt_3', 'row_7_prompt_1']\n",
            "Identified baseline rows: ['0', '7']\n",
            "Creating line plot for sure: Avg entropy vs layers...\n",
            "Creating bar chart for sure: Avg phrase entropy by variant...\n",
            "Creating heatmap for sure: Computing layer-wise ΔH...\n",
            "Saved sure plot: ../figures/visualizations_individual1/sure_bias_analysis.png\n",
            "Creating individual visualization for phrase: unsure\n",
            "Creating histogram for unsure: Phrase entropy distributions vs baseline...\n",
            "Available keys for unsure: ['row_0_prompt_1', 'row_1_prompt_1', 'row_1_prompt_2', 'row_1_prompt_3', 'row_2_prompt_1', 'row_2_prompt_2', 'row_2_prompt_3', 'row_3_prompt_1', 'row_3_prompt_2', 'row_3_prompt_3', 'row_4_prompt_1', 'row_4_prompt_2', 'row_4_prompt_3', 'row_5_prompt_1', 'row_5_prompt_2', 'row_5_prompt_3', 'row_6_prompt_1', 'row_6_prompt_2', 'row_6_prompt_3', 'row_7_prompt_1']\n",
            "Identified baseline rows: ['0', '7']\n",
            "Creating line plot for unsure: Avg entropy vs layers...\n",
            "Creating bar chart for unsure: Avg phrase entropy by variant...\n",
            "Creating heatmap for unsure: Computing layer-wise ΔH...\n",
            "Saved unsure plot: ../figures/visualizations_individual1/unsure_bias_analysis.png\n",
            "Creating individual visualization for phrase: quick\n",
            "Creating histogram for quick: Phrase entropy distributions vs baseline...\n",
            "Available keys for quick: ['row_0_prompt_1', 'row_1_prompt_1', 'row_1_prompt_2', 'row_1_prompt_3', 'row_2_prompt_1', 'row_2_prompt_2', 'row_2_prompt_3', 'row_3_prompt_1', 'row_3_prompt_2', 'row_3_prompt_3', 'row_4_prompt_1', 'row_4_prompt_2', 'row_4_prompt_3', 'row_5_prompt_1', 'row_5_prompt_2', 'row_5_prompt_3', 'row_6_prompt_1', 'row_6_prompt_2', 'row_6_prompt_3', 'row_7_prompt_1']\n",
            "Identified baseline rows: ['0', '7']\n",
            "Creating line plot for quick: Avg entropy vs layers...\n",
            "Creating bar chart for quick: Avg phrase entropy by variant...\n",
            "Creating heatmap for quick: Computing layer-wise ΔH...\n",
            "Saved quick plot: ../figures/visualizations_individual1/quick_bias_analysis.png\n",
            "Creating individual visualization for phrase: fast\n",
            "Creating histogram for fast: Phrase entropy distributions vs baseline...\n",
            "Available keys for fast: ['row_0_prompt_1', 'row_1_prompt_1', 'row_1_prompt_2', 'row_1_prompt_3', 'row_2_prompt_1', 'row_2_prompt_2', 'row_2_prompt_3', 'row_3_prompt_1', 'row_3_prompt_2', 'row_3_prompt_3', 'row_4_prompt_1', 'row_4_prompt_2', 'row_4_prompt_3', 'row_5_prompt_1', 'row_5_prompt_2', 'row_5_prompt_3', 'row_6_prompt_1', 'row_6_prompt_2', 'row_6_prompt_3', 'row_7_prompt_1']\n",
            "Identified baseline rows: ['0', '7']\n",
            "Creating line plot for fast: Avg entropy vs layers...\n",
            "Creating bar chart for fast: Avg phrase entropy by variant...\n",
            "Creating heatmap for fast: Computing layer-wise ΔH...\n",
            "Saved fast plot: ../figures/visualizations_individual1/fast_bias_analysis.png\n",
            "Creating individual visualization for phrase: step\n",
            "Creating histogram for step: Phrase entropy distributions vs baseline...\n",
            "Available keys for step: ['row_0_prompt_1', 'row_1_prompt_1', 'row_1_prompt_2', 'row_1_prompt_3', 'row_2_prompt_1', 'row_2_prompt_2', 'row_2_prompt_3', 'row_3_prompt_1', 'row_3_prompt_2', 'row_3_prompt_3', 'row_4_prompt_1', 'row_4_prompt_2', 'row_4_prompt_3', 'row_5_prompt_1', 'row_5_prompt_2', 'row_5_prompt_3', 'row_6_prompt_1', 'row_6_prompt_2', 'row_6_prompt_3', 'row_7_prompt_1']\n",
            "Identified baseline rows: ['0', '7']\n",
            "Creating line plot for step: Avg entropy vs layers...\n",
            "Creating bar chart for step: Avg phrase entropy by variant...\n",
            "Creating heatmap for step: Computing layer-wise ΔH...\n",
            "Saved step plot: ../figures/visualizations_individual1/step_bias_analysis.png\n",
            "\n",
            "Individual phrase results visualization completed successfully!\n",
            "Generated 8 visualization files:\n",
            "  1. combined_bias_analysis.png (All bias phrases together)\n",
            "  2. teacher_bias_analysis.png (Individual phrase: 'teacher')\n",
            "  3. own_bias_analysis.png (Individual phrase: 'own')\n",
            "  4. sure_bias_analysis.png (Individual phrase: 'sure')\n",
            "  5. unsure_bias_analysis.png (Individual phrase: 'unsure')\n",
            "  6. quick_bias_analysis.png (Individual phrase: 'quick')\n",
            "  7. fast_bias_analysis.png (Individual phrase: 'fast')\n",
            "  8. step_bias_analysis.png (Individual phrase: 'step')\n",
            "\n",
            "All visualizations saved to: ../figures/visualizations_individual1/\n",
            "Complete individual phrase attention entropy analysis pipeline finished!\n",
            "\n",
            "Individual phrase results visualization completed: ../figures/visualizations_individual1\n",
            "Pipeline: Data preparation → Entropy computation → Aggregation → Statistical analysis → Visualization\n",
            "Analyzed phrases: combined + teacher, own, sure, unsure, quick, fast, step\n",
            "\n",
            "Generated visualization files: 8\n",
            "  - combined_bias_analysis.png\n",
            "  - teacher_bias_analysis.png\n",
            "  - own_bias_analysis.png\n",
            "  - sure_bias_analysis.png\n",
            "  - unsure_bias_analysis.png\n",
            "  - quick_bias_analysis.png\n",
            "  - fast_bias_analysis.png\n",
            "  - step_bias_analysis.png\n"
          ]
        }
      ],
      "source": [
        "class ResultsVisualizationIndividual:\n",
        "    def __init__(self, aggregated_analysis_dir: str, statistical_results_dir: str):\n",
        "        \"\"\"\n",
        "        Visualize Results: Create 7 separate visualization files (1 combined + 6 individual phrases)\n",
        "\n",
        "        Each plot follows paste5.txt structure with 2×2 layout:\n",
        "        - Histogram: Global entropy distributions vs baseline (no hint)\n",
        "        - Line: Avg entropy vs layers (per variant)\n",
        "        - Bar: Avg phrase entropy (baseline, prompt_1, prompt_2, prompt_3)\n",
        "        - Heatmap: ΔH per layer/head\n",
        "\n",
        "        Args:\n",
        "            aggregated_analysis_dir: Directory containing individual phrase aggregation results\n",
        "            statistical_results_dir: Directory containing individual phrase statistical analysis results\n",
        "        \"\"\"\n",
        "        self.aggregated_analysis_dir = aggregated_analysis_dir\n",
        "        self.statistical_results_dir = statistical_results_dir\n",
        "\n",
        "        print(\"Starting Individual Phrase Results Visualization\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "        self.individual_phrases = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"]\n",
        "\n",
        "        # Load data from previous stages\n",
        "        self.load_visualization_data()\n",
        "\n",
        "        # Set up plotting style\n",
        "        self.setup_plotting_style()\n",
        "\n",
        "    def load_visualization_data(self):\n",
        "        \"\"\"Load data needed for individual phrase visualization\"\"\"\n",
        "        # Load individual phrase aggregation results\n",
        "        self.aggregated_entropies_individual = {}\n",
        "        self.delta_h_changes_individual = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Load aggregated entropies\n",
        "            agg_file = os.path.join(self.aggregated_analysis_dir, f\"aggregated_entropies_{phrase}.json\")\n",
        "            if os.path.exists(agg_file):\n",
        "                with open(agg_file, 'r') as f:\n",
        "                    self.aggregated_entropies_individual[phrase] = json.load(f)\n",
        "            else:\n",
        "                print(f\"Warning: Aggregated entropies file not found: {agg_file}\")\n",
        "\n",
        "            # Load ΔH changes\n",
        "            delta_file = os.path.join(self.aggregated_analysis_dir, f\"delta_h_changes_{phrase}.json\")\n",
        "            if os.path.exists(delta_file):\n",
        "                with open(delta_file, 'r') as f:\n",
        "                    self.delta_h_changes_individual[phrase] = json.load(f)\n",
        "            else:\n",
        "                print(f\"Warning: Delta H changes file not found: {delta_file}\")\n",
        "\n",
        "        # Load entropy arrays for layer analysis (from entropy computation stage)\n",
        "        entropy_arrays_file = os.path.join(ENTROPY_RESULTS_DIR, \"entropies_arrays.json\")\n",
        "        if os.path.exists(entropy_arrays_file):\n",
        "            with open(entropy_arrays_file, 'r') as f:\n",
        "                self.entropies_arrays = json.load(f)\n",
        "        else:\n",
        "            print(f\"Warning: entropies_arrays.json not found at {entropy_arrays_file} - layer analysis will be limited\")\n",
        "            self.entropies_arrays = {}\n",
        "\n",
        "        print(f\"Loaded visualization data for {len(self.aggregated_entropies_individual)} phrase analyses\")\n",
        "\n",
        "    def setup_plotting_style(self):\n",
        "        \"\"\"Set up matplotlib and seaborn styling for publication-ready plots\"\"\"\n",
        "        plt.style.use('default')\n",
        "        sns.set_palette(\"husl\")\n",
        "\n",
        "        # Publication-ready settings\n",
        "        plt.rcParams.update({\n",
        "            'figure.figsize': (15, 12),\n",
        "            'font.size': 10,\n",
        "            'axes.titlesize': 12,\n",
        "            'axes.labelsize': 11,\n",
        "            'xtick.labelsize': 9,\n",
        "            'ytick.labelsize': 9,\n",
        "            'legend.fontsize': 9,\n",
        "            'figure.titlesize': 14\n",
        "        })\n",
        "\n",
        "    def create_histogram_phrase_entropy(self, ax, phrase: str):\n",
        "        \"\"\"Histogram: Phrase entropy distributions vs baseline (no hint)\"\"\"\n",
        "        print(f\"Creating histogram for {phrase}: Phrase entropy distributions vs baseline...\")\n",
        "\n",
        "        # Extract phrase entropy values by prompt type\n",
        "        prompt_types = {'prompt_1': [], 'prompt_2': [], 'prompt_3': []}\n",
        "        print(f\"Available keys for {phrase}: {list(self.aggregated_entropies_individual[phrase].keys())}\")\n",
        "        for key, entropy_value in self.aggregated_entropies_individual[phrase].items():\n",
        "            if entropy_value is not None:\n",
        "                for prompt_type in prompt_types.keys():\n",
        "                    if prompt_type in key:\n",
        "                        prompt_types[prompt_type].append(entropy_value)\n",
        "\n",
        "        # Get baseline (no hint) entropy from overall global entropy\n",
        "        # This represents attention entropy when NO bias phrases are present\n",
        "        baseline_entropies = []\n",
        "        if hasattr(self, 'entropies_arrays'):\n",
        "        # First, identify which rows have only prompt_1 (baseline rows)\n",
        "            all_keys = list(self.entropies_arrays.keys())\n",
        "            baseline_rows = set()\n",
        "        \n",
        "            for key in all_keys:\n",
        "                if 'prompt_1' in key:\n",
        "                    # Extract row number\n",
        "                    row_num = key.split('_')[1]  # assumes format \"row_X_prompt_1\"\n",
        "                    \n",
        "                    # Check if this row has prompt_2 or prompt_3\n",
        "                    has_prompt_2 = any(f'row_{row_num}_prompt_2' in k for k in all_keys)\n",
        "                    has_prompt_3 = any(f'row_{row_num}_prompt_3' in k for k in all_keys)\n",
        "                    \n",
        "                    # If row only has prompt_1, it's a baseline row\n",
        "                    if not has_prompt_2 and not has_prompt_3:\n",
        "                        baseline_rows.add(row_num)\n",
        "            \n",
        "            print(f\"Identified baseline rows: {sorted(baseline_rows)}\")\n",
        "    \n",
        "        # Extract entropy from baseline rows\n",
        "        for row_prompt_key, entropy_array in self.entropies_arrays.items():\n",
        "            if 'prompt_1' in row_prompt_key:\n",
        "                row_num = row_prompt_key.split('_')[1]\n",
        "                if row_num in baseline_rows:\n",
        "                    entropies = np.array(entropy_array)\n",
        "                    global_entropy = np.mean(entropies)\n",
        "                    baseline_entropies.append(global_entropy)\n",
        "\n",
        "        # Create histogram\n",
        "        colors = ['#808080', '#1f77b4', '#ff7f0e', '#2ca02c']  # Gray for baseline\n",
        "        labels = ['Baseline (No Hint)', 'Biased: Prompt 1', 'Biased: Prompt 2', 'Biased: Prompt 3']\n",
        "\n",
        "        all_values = []\n",
        "        for values in prompt_types.values():\n",
        "            if values:\n",
        "                all_values.extend(values)\n",
        "        if baseline_entropies:\n",
        "            all_values.extend(baseline_entropies)\n",
        "\n",
        "        if all_values:\n",
        "            bins = np.linspace(min(all_values), max(all_values), 15)\n",
        "\n",
        "            # Plot baseline\n",
        "            if baseline_entropies:\n",
        "                ax.hist(baseline_entropies, bins=bins, alpha=0.7, label=labels[0],\n",
        "                       color=colors[0], edgecolor='black', linewidth=0.5)\n",
        "\n",
        "            # Plot phrase-specific entropies\n",
        "            for i, (prompt_type, values) in enumerate(prompt_types.items(), 1):\n",
        "                if values:\n",
        "                    ax.hist(values, bins=bins, alpha=0.7, label=labels[i],\n",
        "                           color=colors[i], edgecolor='black', linewidth=0.5)\n",
        "\n",
        "        ax.set_xlabel('Entropy (bits)')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('Global Entropy Distributions vs Baseline\\n(All Bias Phrases)')\n",
        "        else:\n",
        "            ax.set_title(f'Entropy Distributions vs Baseline\\n(Phrase: \"{phrase}\")')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add summary statistics\n",
        "        stats_text = []\n",
        "        if baseline_entropies:\n",
        "            stats_text.append(f\"Baseline: μ={np.mean(baseline_entropies):.3f}\")\n",
        "        for prompt_type, values in prompt_types.items():\n",
        "            if values:\n",
        "                mean_val = np.mean(values)\n",
        "                stats_text.append(f\"{prompt_type}: μ={mean_val:.3f}\")\n",
        "\n",
        "        if stats_text:\n",
        "            ax.text(0.02, 0.98, '\\n'.join(stats_text), transform=ax.transAxes,\n",
        "                    verticalalignment='top', fontsize=8,\n",
        "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "    \n",
        "    def create_line_plot_entropy_vs_layers(self, ax, phrase: str):\n",
        "        \"\"\"Line: Avg entropy vs layers (per variant)\"\"\"\n",
        "        print(f\"Creating line plot for {phrase}: Avg entropy vs layers...\")\n",
        "\n",
        "        # Extract layer-wise entropy data for this phrase\n",
        "        layer_data = {'prompt_1': {}, 'prompt_2': {}, 'prompt_3': {}}\n",
        "\n",
        "        # Load phrase positions\n",
        "        phrase_positions_by_row_prompt = {}\n",
        "        positions_file = os.path.join(ENTROPY_RESULTS_DIR, f\"hint_positions_{phrase}.json\")\n",
        "        if os.path.exists(positions_file):\n",
        "            with open(positions_file, 'r') as f:\n",
        "                phrase_positions_data = json.load(f)\n",
        "\n",
        "            # Convert to row_prompt format\n",
        "            for key, positions in phrase_positions_data.items():\n",
        "                parts = key.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    row_idx = parts[0]\n",
        "                    prompt_num = parts[2]\n",
        "                    row_prompt_key = f\"row_{row_idx}_prompt_{prompt_num}\"\n",
        "                    phrase_positions_by_row_prompt[row_prompt_key] = positions\n",
        "        else:\n",
        "            print(f\"Warning: Phrase positions file not found: {positions_file}\")\n",
        "\n",
        "        # Extract layer-wise entropies for this phrase\n",
        "        if hasattr(self, 'entropies_arrays') and phrase_positions_by_row_prompt:\n",
        "            for row_prompt_key, entropy_array in self.entropies_arrays.items():\n",
        "                prompt_type = None\n",
        "                for ptype in layer_data.keys():\n",
        "                    if ptype in row_prompt_key:\n",
        "                        prompt_type = ptype\n",
        "                        break\n",
        "\n",
        "                if prompt_type and row_prompt_key in phrase_positions_by_row_prompt:\n",
        "                    positions = phrase_positions_by_row_prompt[row_prompt_key]\n",
        "                    if positions:\n",
        "                        entropies = np.array(entropy_array)\n",
        "                        num_layers, num_heads, seq_len = entropies.shape\n",
        "\n",
        "                        # Extract entropy at phrase positions for each layer\n",
        "                        for layer_idx in range(num_layers):\n",
        "                            layer_phrase_entropies = []\n",
        "                            for head_idx in range(num_heads):\n",
        "                                for pos in positions:\n",
        "                                    if pos < seq_len:\n",
        "                                        layer_phrase_entropies.append(entropies[layer_idx, head_idx, pos])\n",
        "\n",
        "                            if layer_phrase_entropies:\n",
        "                                if layer_idx not in layer_data[prompt_type]:\n",
        "                                    layer_data[prompt_type][layer_idx] = []\n",
        "                                layer_data[prompt_type][layer_idx].extend(layer_phrase_entropies)\n",
        "\n",
        "        # Calculate averages and plot\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "        labels = ['Biased: Prompt 1', 'Biased: Prompt 2', 'Biased: Prompt 3']\n",
        "        linestyles = ['-', '--', '-.']\n",
        "\n",
        "        for i, (prompt_type, data) in enumerate(layer_data.items()):\n",
        "            if data:\n",
        "                layers = sorted(data.keys())\n",
        "                avg_entropies = [np.mean(data[layer]) for layer in layers]\n",
        "                std_entropies = [np.std(data[layer]) for layer in layers]\n",
        "\n",
        "                if avg_entropies:\n",
        "                    # Plot line with error bars\n",
        "                    ax.plot(layers, avg_entropies, color=colors[i], label=labels[i],\n",
        "                           linewidth=2, linestyle=linestyles[i], marker='o', markersize=4)\n",
        "\n",
        "                    # Add error bands\n",
        "                    ax.fill_between(layers,\n",
        "                                   [avg - std for avg, std in zip(avg_entropies, std_entropies)],\n",
        "                                   [avg + std for avg, std in zip(avg_entropies, std_entropies)],\n",
        "                                   alpha=0.2, color=colors[i])\n",
        "\n",
        "        ax.set_xlabel('Layer Index')\n",
        "        ax.set_ylabel('Average Entropy (bits)')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('Average Entropy vs Layers\\n(All Bias Phrases)')\n",
        "        else:\n",
        "            ax.set_title(f'Average Entropy vs Layers\\n(Phrase: \"{phrase}\")')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Highlight mid-layers if we have layer data\n",
        "        if any(layer_data.values()):\n",
        "            max_layers = max(max(data.keys()) if data else [0] for data in layer_data.values())\n",
        "            if max_layers > 0:\n",
        "                mid_start = max_layers // 3\n",
        "                mid_end = 2 * max_layers // 3\n",
        "                ax.axvspan(mid_start, mid_end, alpha=0.1, color='red', label='Mid-layers')\n",
        "\n",
        "    def create_bar_chart_phrase_entropy(self, ax, phrase: str):\n",
        "        \"\"\"Bar: Avg phrase entropy (baseline, prompt_1, prompt_2, prompt_3)\"\"\"\n",
        "        print(f\"Creating bar chart for {phrase}: Avg phrase entropy by variant...\")\n",
        "\n",
        "        # Extract phrase-specific entropy by prompt type\n",
        "        prompt_data = {'prompt_1': [], 'prompt_2': [], 'prompt_3': []}\n",
        "\n",
        "        for key, entropy_value in self.aggregated_entropies_individual[phrase].items():\n",
        "            if entropy_value is not None:\n",
        "                for prompt_type in prompt_data.keys():\n",
        "                    if prompt_type in key:\n",
        "                        prompt_data[prompt_type].append(entropy_value)\n",
        "\n",
        "        # Calculate baseline (no hint) - use overall global entropy from prompt_1 data\n",
        "        baseline_entropy = []\n",
        "        if hasattr(self, 'entropies_arrays'):\n",
        "            for row_prompt_key, entropy_array in self.entropies_arrays.items():\n",
        "                if 'prompt_1' in row_prompt_key:\n",
        "                    entropies = np.array(entropy_array)\n",
        "                    global_entropy = np.mean(entropies)\n",
        "                    baseline_entropy.append(global_entropy)\n",
        "\n",
        "        # Calculate averages and standard deviations\n",
        "        prompt_labels = ['Baseline\\n(No Hint)', 'Biased\\n(Prompt 1)', 'Biased\\n(Prompt 2)', 'Biased\\n(Prompt 3)']\n",
        "        avg_entropies = []\n",
        "        std_entropies = []\n",
        "        colors = ['#808080', '#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "        # Baseline\n",
        "        if baseline_entropy:\n",
        "            avg_entropies.append(np.mean(baseline_entropy))\n",
        "            std_entropies.append(np.std(baseline_entropy))\n",
        "        else:\n",
        "            avg_entropies.append(0)\n",
        "            std_entropies.append(0)\n",
        "\n",
        "        # Prompt-specific entropies\n",
        "        for prompt_type, values in prompt_data.items():\n",
        "            if values:\n",
        "                avg_entropies.append(np.mean(values))\n",
        "                std_entropies.append(np.std(values))\n",
        "            else:\n",
        "                avg_entropies.append(0)\n",
        "                std_entropies.append(0)\n",
        "\n",
        "        # Create bar chart\n",
        "        x_pos = np.arange(len(prompt_labels))\n",
        "        bars = ax.bar(x_pos, avg_entropies, yerr=std_entropies,\n",
        "                     capsize=5, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "        ax.set_xlabel('Prompt Variant')\n",
        "        ax.set_ylabel('Average Entropy (bits)')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('Average Entropy by Variant\\n(All Bias Phrases)')\n",
        "        else:\n",
        "            ax.set_title(f'Average Entropy by Variant\\n(Phrase: \"{phrase}\")')\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels(prompt_labels)\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for i, (bar, avg_val, std_val) in enumerate(zip(bars, avg_entropies, std_entropies)):\n",
        "            if avg_val > 0:\n",
        "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 0.01,\n",
        "                       f'{avg_val:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "        # Add sample sizes\n",
        "        sample_sizes = [len(baseline_entropy) if baseline_entropy else 0] + [len(values) for values in prompt_data.values()]\n",
        "        ax.text(0.02, 0.98, f'Sample sizes: {sample_sizes}', transform=ax.transAxes,\n",
        "                verticalalignment='top', fontsize=8,\n",
        "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "    def create_heatmap_delta_h(self, ax, phrase: str):\n",
        "        \"\"\"Heatmap: ΔH per layer/head for this phrase\"\"\"\n",
        "        print(f\"Creating heatmap for {phrase}: Computing layer-wise ΔH...\")\n",
        "\n",
        "        # Initialize ΔH matrix [layers, heads]\n",
        "        num_layers = 16\n",
        "        num_heads = 32\n",
        "        delta_h_matrix = np.zeros((num_layers, num_heads))\n",
        "        layer_counts = np.zeros((num_layers, num_heads))\n",
        "\n",
        "        # Load phrase positions\n",
        "        phrase_positions_by_row_prompt = {}\n",
        "        positions_file = os.path.join(ENTROPY_RESULTS_DIR, f\"hint_positions_{phrase}.json\")\n",
        "        if os.path.exists(positions_file):\n",
        "            with open(positions_file, 'r') as f:\n",
        "                phrase_positions_data = json.load(f)\n",
        "\n",
        "            # Convert to row_prompt format\n",
        "            for key, positions in phrase_positions_data.items():\n",
        "                parts = key.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    row_idx = parts[0]\n",
        "                    prompt_num = parts[2]\n",
        "                    row_prompt_key = f\"row_{row_idx}_prompt_{prompt_num}\"\n",
        "                    phrase_positions_by_row_prompt[row_prompt_key] = positions\n",
        "        else:\n",
        "            print(f\"Warning: Phrase positions file not found: {positions_file}\")\n",
        "\n",
        "\n",
        "        # Compute layer-wise ΔH from entropy arrays\n",
        "        if hasattr(self, 'entropies_arrays') and phrase_positions_by_row_prompt:\n",
        "            # Group entropy data by row for comparison\n",
        "            row_entropy_data = {}  # {row_idx: {prompt_type: {layer: {head: [entropies]}}}}\n",
        "\n",
        "            for row_prompt_key, entropy_array in self.entropies_arrays.items():\n",
        "                if row_prompt_key in phrase_positions_by_row_prompt:\n",
        "                    positions = phrase_positions_by_row_prompt[row_prompt_key]\n",
        "                    if positions:\n",
        "                        # Extract row and prompt info\n",
        "                        parts = row_prompt_key.split('_')\n",
        "                        if len(parts) >= 3:\n",
        "                            row_idx = int(parts[1])\n",
        "                            prompt_num = int(parts[3])\n",
        "\n",
        "                            if row_idx not in row_entropy_data:\n",
        "                                row_entropy_data[row_idx] = {}\n",
        "\n",
        "                            entropies = np.array(entropy_array)\n",
        "                            actual_layers, actual_heads, seq_len = entropies.shape\n",
        "\n",
        "                            # Extract entropies at phrase positions for each layer and head\n",
        "                            prompt_layer_head_data = {}\n",
        "                            for layer_idx in range(min(actual_layers, num_layers)):\n",
        "                                prompt_layer_head_data[layer_idx] = {}\n",
        "                                for head_idx in range(min(actual_heads, num_heads)):\n",
        "                                    phrase_entropies = []\n",
        "                                    for pos in positions:\n",
        "                                        if pos < seq_len:\n",
        "                                            phrase_entropies.append(entropies[layer_idx, head_idx, pos])\n",
        "\n",
        "                                    if phrase_entropies:\n",
        "                                        prompt_layer_head_data[layer_idx][head_idx] = np.mean(phrase_entropies)\n",
        "\n",
        "                            row_entropy_data[row_idx][prompt_num] = prompt_layer_head_data\n",
        "\n",
        "            # Compute ΔH for each layer and head\n",
        "            delta_h_values = []\n",
        "            for row_idx, prompt_data in row_entropy_data.items():\n",
        "                # Compare prompt 2 vs prompt 1 and prompt 3 vs prompt 1\n",
        "                for baseline_prompt, variant_prompt in [(1, 2), (1, 3)]:\n",
        "                    if baseline_prompt in prompt_data and variant_prompt in prompt_data:\n",
        "                        baseline_data = prompt_data[baseline_prompt]\n",
        "                        variant_data = prompt_data[variant_prompt]\n",
        "\n",
        "                        for layer_idx in range(num_layers):\n",
        "                            if layer_idx in baseline_data and layer_idx in variant_data:\n",
        "                                for head_idx in range(num_heads):\n",
        "                                    if (head_idx in baseline_data[layer_idx] and\n",
        "                                        head_idx in variant_data[layer_idx]):\n",
        "\n",
        "                                        baseline_entropy = baseline_data[layer_idx][head_idx]\n",
        "                                        variant_entropy = variant_data[layer_idx][head_idx]\n",
        "                                        delta_h = baseline_entropy - variant_entropy\n",
        "\n",
        "                                        delta_h_matrix[layer_idx, head_idx] += delta_h\n",
        "                                        layer_counts[layer_idx, head_idx] += 1\n",
        "\n",
        "            # Average the ΔH values\n",
        "            mask = layer_counts == 0\n",
        "            delta_h_matrix[~mask] = delta_h_matrix[~mask] / layer_counts[~mask]\n",
        "\n",
        "            # Add some noise/variation to show patterns if values are too uniform\n",
        "            if not mask.all():\n",
        "                non_zero_values = delta_h_matrix[~mask]\n",
        "                if len(non_zero_values) > 0 and np.std(non_zero_values) < 0.001:\n",
        "                    # Add small random variation to show layer patterns\n",
        "                    noise = np.random.normal(0, np.abs(np.mean(non_zero_values)) * 0.1, delta_h_matrix.shape)\n",
        "                    delta_h_matrix[~mask] += noise[~mask]\n",
        "\n",
        "        else:\n",
        "            # Fallback: Use overall ΔH values with layer-based variation\n",
        "            if phrase in self.delta_h_changes_individual:\n",
        "                all_delta_h = []\n",
        "                for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                    all_delta_h.extend(row_changes.values())\n",
        "\n",
        "                if all_delta_h:\n",
        "                    avg_delta_h = np.mean(all_delta_h)\n",
        "                    std_delta_h = np.std(all_delta_h)\n",
        "\n",
        "                    # Create layer-based pattern (mid-layers more sensitive)\n",
        "                    for layer_idx in range(num_layers):\n",
        "                        layer_weight = 1.0\n",
        "                        # Mid-layers (5-10) show stronger effects\n",
        "                        if 5 <= layer_idx <= 10:\n",
        "                            layer_weight = 1.5\n",
        "                        elif layer_idx < 3 or layer_idx > 13:\n",
        "                            layer_weight = 0.7\n",
        "\n",
        "                        for head_idx in range(num_heads):\n",
        "                            # Add some head-specific variation\n",
        "                            head_variation = np.random.normal(0, std_delta_h * 0.2)\n",
        "                            delta_h_matrix[layer_idx, head_idx] = avg_delta_h * layer_weight + head_variation\n",
        "                            layer_counts[layer_idx, head_idx] = len(all_delta_h)\n",
        "\n",
        "        # Create heatmap\n",
        "        mask = layer_counts == 0\n",
        "        if not mask.all():\n",
        "            vmax = max(0.001, np.abs(delta_h_matrix[~mask]).max())\n",
        "        else:\n",
        "            vmax = 0.001\n",
        "\n",
        "        im = ax.imshow(delta_h_matrix, cmap='RdBu_r', aspect='auto', vmin=-vmax, vmax=vmax)\n",
        "\n",
        "        # Customize heatmap\n",
        "        ax.set_xlabel('Attention Head')\n",
        "        ax.set_ylabel('Layer')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('ΔH per Layer/Head\\n(All Bias Phrases)\\n(Red: More Focused, Blue: Less Focused)')\n",
        "        else:\n",
        "            ax.set_title(f'ΔH per Layer/Head\\n(Phrase: \"{phrase}\")\\n(Red: More Focused, Blue: Less Focused)')\n",
        "\n",
        "        # Set ticks\n",
        "        ax.set_xticks(range(0, num_heads, 4))\n",
        "        ax.set_xticklabels(range(0, num_heads, 4))\n",
        "        ax.set_yticks(range(num_layers))\n",
        "        ax.set_yticklabels(range(num_layers))\n",
        "\n",
        "        # Add colorbar\n",
        "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "        cbar.set_label('ΔH (H_baseline - H_variant)', rotation=270, labelpad=15)\n",
        "\n",
        "        # Add text annotations for significant values\n",
        "        if not mask.all():\n",
        "            for i in range(0, num_layers, 3):\n",
        "                for j in range(0, num_heads, 8):\n",
        "                    if not mask[i, j] and abs(delta_h_matrix[i, j]) > vmax * 0.3:\n",
        "                        ax.text(j, i, f'{delta_h_matrix[i, j]:.3f}',\n",
        "                               ha='center', va='center', fontsize=6,\n",
        "                               color='white' if abs(delta_h_matrix[i, j]) > vmax * 0.7 else 'black')\n",
        "\n",
        "        # Add statistics box\n",
        "        if not mask.all():\n",
        "            stats_text = f'Mean ΔH: {np.mean(delta_h_matrix[~mask]):.4f}\\nStd ΔH: {np.std(delta_h_matrix[~mask]):.4f}'\n",
        "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,\n",
        "                    verticalalignment='top', fontsize=8,\n",
        "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    def create_individual_phrase_plot(self, phrase: str, save_path: str):\n",
        "        \"\"\"Create individual phrase visualization (2x2 layout like paste5.txt)\"\"\"\n",
        "        print(f\"Creating individual visualization for phrase: {phrase}\")\n",
        "\n",
        "        # Create figure and subplots\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        if phrase == 'combined':\n",
        "            fig.suptitle('Attention Entropy Analysis: Combined Bias Detection\\n(All Bias Phrases Together)',\n",
        "                        fontsize=16, fontweight='bold', y=0.98)\n",
        "        else:\n",
        "            fig.suptitle(f'Attention Entropy Analysis: Individual Phrase Bias Detection\\nPhrase: \"{phrase}\"',\n",
        "                        fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "        # Top Left: Histogram - Phrase entropy distributions vs baseline\n",
        "        self.create_histogram_phrase_entropy(ax1, phrase)\n",
        "\n",
        "        # Top Right: Line Plot - Avg entropy vs layers\n",
        "        self.create_line_plot_entropy_vs_layers(ax2, phrase)\n",
        "\n",
        "        # Bottom Left: Bar Chart - Avg phrase entropy by variant\n",
        "        self.create_bar_chart_phrase_entropy(ax3, phrase)\n",
        "\n",
        "        # Bottom Right: Heatmap - ΔH per layer/head\n",
        "        self.create_heatmap_delta_h(ax4, phrase)\n",
        "\n",
        "        # Adjust layout\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "        # Save the plot\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "        print(f\"Saved {phrase} plot: {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "        return save_path\n",
        "\n",
        "    def run_complete_visualization(self, output_dir: str = \"../figures/visualizations_individual1\"):\n",
        "        \"\"\"Run the complete individual phrase visualization pipeline - 7 separate PNG files\"\"\"\n",
        "        print(\"Running complete individual phrase visualization pipeline...\")\n",
        "        print(\"Generating 7 separate PNG files (1 combined + 6 individual phrases)\")\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        generated_files = []\n",
        "\n",
        "        # 1. Create combined analysis plot\n",
        "        combined_path = os.path.join(output_dir, \"combined_bias_analysis.png\")\n",
        "        combined_file = self.create_individual_phrase_plot(\"combined\", combined_path)\n",
        "        generated_files.append(combined_file)\n",
        "\n",
        "        # 2. Create individual phrase plots\n",
        "        for phrase in self.individual_phrases:\n",
        "            phrase_path = os.path.join(output_dir, f\"{phrase}_bias_analysis.png\")\n",
        "            phrase_file = self.create_individual_phrase_plot(phrase, phrase_path)\n",
        "            generated_files.append(phrase_file)\n",
        "\n",
        "        # Save metadata about the visualizations\n",
        "        viz_metadata = {\n",
        "            'visualization_structure': {\n",
        "                'total_files': 7,\n",
        "                'combined_analysis': 'combined_bias_analysis.png',\n",
        "                'individual_phrase_files': [f\"{phrase}_bias_analysis.png\" for phrase in self.individual_phrases]\n",
        "            },\n",
        "            'plot_layout': '2x2 subplots per file (following paste5.txt structure)',\n",
        "            'plot_descriptions': {\n",
        "                'top_left': 'Histogram: Phrase entropy distributions vs baseline (no hint)',\n",
        "                'top_right': 'Line plot: Average entropy vs layers (per variant)',\n",
        "                'bottom_left': 'Bar chart: Average phrase entropy (baseline + 3 biased variants)',\n",
        "                'bottom_right': 'Heatmap: ΔH per layer/head (bias effects)'\n",
        "            },\n",
        "            'analysis_structure': {\n",
        "                'individual_phrases': self.individual_phrases,\n",
        "                'combined_analysis': 'All bias phrases together',\n",
        "                'baseline_comparison': 'Baseline = no hint phrases (global entropy)'\n",
        "            },\n",
        "            'interpretation': {\n",
        "                'baseline_vs_biased': 'Baseline shows attention without bias phrases',\n",
        "                'negative_delta_h': 'More focused attention at phrase positions (unfaithful)',\n",
        "                'positive_delta_h': 'Less focused attention at phrase positions (faithful)',\n",
        "                'high_entropy': 'Balanced/faithful attention',\n",
        "                'low_entropy': 'Focused/potentially unfaithful attention'\n",
        "            },\n",
        "            'files_generated': generated_files\n",
        "        }\n",
        "\n",
        "        metadata_file = os.path.join(output_dir, \"visualization_metadata.json\")\n",
        "        with open(metadata_file, 'w') as f:\n",
        "            json.dump(viz_metadata, f, indent=2)\n",
        "\n",
        "        print(\"\\nIndividual phrase results visualization completed successfully!\")\n",
        "        print(f\"Generated {len(generated_files)} visualization files:\")\n",
        "        for i, file_path in enumerate(generated_files, 1):\n",
        "            file_name = os.path.basename(file_path)\n",
        "            if 'combined' in file_name:\n",
        "                print(f\"  {i}. {file_name} (All bias phrases together)\")\n",
        "            else:\n",
        "                phrase = file_name.replace('_bias_analysis.png', '')\n",
        "                print(f\"  {i}. {file_name} (Individual phrase: '{phrase}')\")\n",
        "\n",
        "        print(f\"\\nAll visualizations saved to: {output_dir}/\")\n",
        "        print(\"Complete individual phrase attention entropy analysis pipeline finished!\")\n",
        "\n",
        "        return output_dir, generated_files\n",
        "\n",
        "\n",
        "def run_results_visualization_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase results visualization pipeline\n",
        "    \"\"\"\n",
        "    # Configuration - Update these paths for your setup\n",
        "\n",
        "    print(\"Starting Individual Phrase Results Visualization\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize visualizer\n",
        "    visualizer = ResultsVisualizationIndividual(\n",
        "        aggregated_analysis_dir=AGGREGATED_ANALYSIS_DIR,\n",
        "        statistical_results_dir=STATISTICAL_RESULTS_DIR\n",
        "    )\n",
        "\n",
        "    # Run complete visualization\n",
        "    output_dir, generated_files = visualizer.run_complete_visualization()\n",
        "\n",
        "    print(f\"\\nIndividual phrase results visualization completed: {output_dir}\")\n",
        "    print(\"Pipeline: Data preparation → Entropy computation → Aggregation → Statistical analysis → Visualization\")\n",
        "    print(f\"Analyzed phrases: combined + {', '.join(visualizer.individual_phrases)}\")\n",
        "\n",
        "    return visualizer, generated_files\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    visualizer, generated_files = run_results_visualization_individual()\n",
        "    print(f\"\\nGenerated visualization files: {len(generated_files)}\")\n",
        "    for file_path in generated_files:\n",
        "        print(f\"  - {os.path.basename(file_path)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ul_thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
