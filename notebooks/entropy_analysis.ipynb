{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import json\n",
        "import os\n",
        "from typing import Dict, List, Tuple\n",
        "import re\n",
        "import random\n",
        "import gc\n",
        "\n",
        "class LlamaActivationExtractor:\n",
        "    def __init__(self, model_name: str, device=None):\n",
        "        if device is None:\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        self.model_name = model_name\n",
        "\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if self.device == \"cuda\" else None,\n",
        "            attn_implementation=\"eager\"  # Required for attention extraction\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        if self.device == \"cuda\" and hasattr(self.model, 'to'):\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # Storage for both activations and attention weights\n",
        "        self.activations = {}\n",
        "        self.attention_weights = {}\n",
        "        self.hooks = []\n",
        "\n",
        "        self._register_hooks()\n",
        "        print(f\"Model loaded. Registered {len(self.hooks)} hooks (MLP + Attention)\")\n",
        "    def _register_hooks(self):\n",
        "\n",
        "      def get_activation(name):\n",
        "          def hook(module, input, output):\n",
        "              try:\n",
        "                  if isinstance(output, tuple):\n",
        "                      hidden_states = output[0]\n",
        "                  else:\n",
        "                      hidden_states = output\n",
        "\n",
        "                  if hasattr(hidden_states, 'shape') and len(hidden_states.shape) == 3:\n",
        "                      activation = hidden_states.detach().cpu().numpy()\n",
        "                      self.activations[name] = activation\n",
        "              except Exception as e:\n",
        "                  print(f\"Error in activation hook {name}: {e}\")\n",
        "              # Always return None - don't modify the output\n",
        "              return None\n",
        "          return hook\n",
        "\n",
        "        # Find transformer layers\n",
        "      if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):\n",
        "          layers = self.model.model.layers\n",
        "      elif hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):\n",
        "          layers = self.model.transformer.h\n",
        "      else:\n",
        "          print(\"Could not find transformer layers!\")\n",
        "          return\n",
        "\n",
        "    # Register hooks ONLY for MLP activations (safer approach)\n",
        "      for i, layer in enumerate(layers):\n",
        "          # MLP activation hook (whole layer)\n",
        "          mlp_hook = layer.register_forward_hook(get_activation(f'layer_{i}'))\n",
        "          self.hooks.append(mlp_hook)\n",
        "\n",
        "      print(f\"Registered {len(self.hooks)} activation hooks\")\n",
        "    def extract_input_only_activations(self, prompt: str):\n",
        "        \"\"\"Extract activations and attention from input prompt only (like second script)\"\"\"\n",
        "        self.activations.clear()\n",
        "        self.attention_weights.clear()\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        print(f\"        Processing input with {len(inputs['input_ids'][0])} tokens\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Forward pass with attention extraction\n",
        "            outputs = self.model(\n",
        "                **inputs,\n",
        "                output_attentions=True,  # Force attention computation\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "            print(f\"        Model forward pass completed\")\n",
        "\n",
        "            # Also capture attention from model output as backup\n",
        "            if hasattr(outputs, 'attentions') and outputs.attentions:\n",
        "                print(f\"        Found {len(outputs.attentions)} attention matrices from model output\")\n",
        "                for i, attn in enumerate(outputs.attentions):\n",
        "                    if attn is not None:\n",
        "                        attn_np = attn.detach().cpu().numpy()\n",
        "                        key = f\"attn_layer_{i}_direct\"\n",
        "                        self.attention_weights[key] = attn_np\n",
        "                        print(f\"          Captured attention layer {i}: shape {attn_np.shape}\")\n",
        "            else:\n",
        "                print(f\"        Warning: No attention weights found in model outputs\")\n",
        "\n",
        "        print(f\"        Final counts - Activations: {len(self.activations)}, Attention: {len(self.attention_weights)}\")\n",
        "\n",
        "        # Ensure we return dictionaries, not None\n",
        "        activations_result = self.activations.copy() if self.activations else {}\n",
        "        attention_result = self.attention_weights.copy() if self.attention_weights else {}\n",
        "\n",
        "        return activations_result, attention_result\n",
        "\n",
        "    def create_few_shot_prompt(self, problem: str) -> str:\n",
        "        \"\"\"Create a few-shot prompt with examples to improve model performance\"\"\"\n",
        "\n",
        "        few_shot_examples = \"\"\"Here are some examples of how to solve math word problems step by step:\n",
        "\n",
        "Example 1:\n",
        "Problem: Sarah has 24 apples. She gives 8 apples to her friend and then buys 15 more apples. How many apples does Sarah have now?\n",
        "Solution: Let me solve this step by step.\n",
        "Sarah starts with 24 apples.\n",
        "She gives away 8 apples: 24 - 8 = 16 apples remaining.\n",
        "She buys 15 more apples: 16 + 15 = 31 apples.\n",
        "The answer is 31.\n",
        "\n",
        "Example 2:\n",
        "Problem: A classroom has 6 rows of desks with 4 desks in each row. If 3 desks are broken, how many working desks are there?\n",
        "Solution: Let me solve this step by step.\n",
        "Total desks = 6 rows × 4 desks per row = 24 desks.\n",
        "Broken desks = 3 desks.\n",
        "Working desks = 24 - 3 = 21 desks.\n",
        "The answer is 21.\n",
        "\n",
        "Example 3:\n",
        "Problem: Tom bought 3 packs of stickers. Each pack contains 12 stickers. He used 8 stickers for his project. How many stickers does Tom have left?\n",
        "Solution: Let me solve this step by step.\n",
        "Total stickers = 3 packs × 12 stickers per pack = 36 stickers.\n",
        "Stickers used = 8 stickers.\n",
        "Stickers left = 36 - 8 = 28 stickers.\n",
        "The answer is 28.\n",
        "\n",
        "Now solve this problem:\n",
        "Problem: \"\"\"\n",
        "\n",
        "        return few_shot_examples + problem + \"\\nSolution: Let me solve this step by step.\\n\"\n",
        "\n",
        "    def generate_with_full_sequence_attention_capture(self, original_prompt: str, generation_prompt: str, max_new_tokens: int = 150):\n",
        "        \"\"\"Generate text with few-shot and capture full sequence attention\"\"\"\n",
        "        full_sequence_attention = {}\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Use generation prompt (with few-shot) for actual generation\n",
        "        inputs = self.tokenizer(generation_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        print(f\"        Generating with {len(inputs['input_ids'][0])} input tokens\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Generate with attention capture and improved settings\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=len(inputs['input_ids'][0]) + max_new_tokens,\n",
        "                num_return_sequences=1,\n",
        "                do_sample=False,\n",
        "                temperature=None,\n",
        "                top_p=None,\n",
        "                repetition_penalty=1.1,  # Reduce repetition\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                output_attentions=True,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True\n",
        "            )\n",
        "\n",
        "            print(f\"        Generation completed\")\n",
        "\n",
        "            # Extract full sequence attention weights\n",
        "            if hasattr(outputs, 'attentions') and outputs.attentions:\n",
        "                print(f\"        Found {len(outputs.attentions)} generation steps with attention\")\n",
        "                # Get the final attention matrices (most comprehensive)\n",
        "                if len(outputs.attentions) > 0:\n",
        "                    final_step_attentions = outputs.attentions[-1]\n",
        "                    print(f\"        Processing final step with {len(final_step_attentions)} layers\")\n",
        "                    for layer_idx, layer_attention in enumerate(final_step_attentions):\n",
        "                        if layer_attention is not None:\n",
        "                            attention_np = layer_attention.detach().cpu().numpy()\n",
        "                            key = f\"full_seq_attn_layer_{layer_idx}\"\n",
        "                            full_sequence_attention[key] = attention_np\n",
        "                            print(f\"          Captured full sequence attention layer {layer_idx}: shape {attention_np.shape}\")\n",
        "            else:\n",
        "                print(f\"        Warning: No attention weights found in generation outputs\")\n",
        "\n",
        "            generated_sequence = outputs.sequences\n",
        "\n",
        "        print(f\"        Final full sequence attention count: {len(full_sequence_attention)}\")\n",
        "\n",
        "        # Ensure we return proper objects\n",
        "        sequence_result = generated_sequence if 'generated_sequence' in locals() else None\n",
        "        attention_result = full_sequence_attention if full_sequence_attention else {}\n",
        "\n",
        "        return sequence_result, attention_result\n",
        "\n",
        "    def extract_numerical_answer(self, text: str) -> str:\n",
        "        # Look for \"the answer is\" pattern first\n",
        "        answer_pattern = r'the answer is\\s*(\\d+(?:\\.\\d+)?)'  # Allow decimal numbers\n",
        "        answer_match = re.search(answer_pattern, text.lower())\n",
        "        if answer_match:\n",
        "            return answer_match.group(1)\n",
        "\n",
        "        # Look for \"answer:\" pattern\n",
        "        answer_pattern2 = r'answer:\\s*(\\d+(?:\\.\\d+)?)'\n",
        "        answer_match2 = re.search(answer_pattern2, text.lower())\n",
        "        if answer_match2:\n",
        "            return answer_match2.group(1)\n",
        "\n",
        "        # Fallback to last number if no clear answer pattern\n",
        "        numbers = re.findall(r'\\b\\d+(?:\\.\\d+)?\\b', text)\n",
        "        if numbers:\n",
        "            return numbers[-1]\n",
        "        return \"No numerical answer found\"\n",
        "\n",
        "    def process_single_prompt(self, prompt: str, prompt_name: str, row_index: int, seed: int = 37, use_few_shot: bool = True):\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "        print(f\"    Extracting activations and attention for {prompt_name}...\")\n",
        "\n",
        "        # 1. Extract both activations and input attention from ORIGINAL prompt ONLY\n",
        "        input_activations, input_attention_weights = self.extract_input_only_activations(prompt)\n",
        "\n",
        "        # Create few-shot prompt for generation but keep original for analysis\n",
        "        if use_few_shot:\n",
        "            generation_prompt = self.create_few_shot_prompt(prompt)\n",
        "        else:\n",
        "            # Fallback to simple instruction\n",
        "            if not prompt.strip().endswith(\"the answer is\"):\n",
        "                generation_prompt = prompt.strip() + \" Please solve step by step and end with 'the answer is X'.\"\n",
        "            else:\n",
        "                generation_prompt = prompt\n",
        "\n",
        "        # 2. Generate response with few-shot prompt and capture full sequence attention\n",
        "        generated_sequence, full_sequence_attention_weights = self.generate_with_full_sequence_attention_capture(\n",
        "            original_prompt=prompt,\n",
        "            generation_prompt=generation_prompt,\n",
        "            max_new_tokens=150\n",
        "        )\n",
        "\n",
        "        # Decode the full generated text\n",
        "        full_generated_text = self.tokenizer.decode(generated_sequence[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract only the answer part (remove the few-shot examples from output)\n",
        "        if use_few_shot:\n",
        "            # Find where the target problem starts in generated text\n",
        "            problem_marker = \"Problem: \" + prompt\n",
        "            problem_start = full_generated_text.find(problem_marker)\n",
        "\n",
        "            if problem_start != -1:\n",
        "                # Extract everything after \"Step 1:\" for this specific problem\n",
        "                step_start = full_generated_text.find(\"Step 1:\", problem_start)\n",
        "                if step_start != -1:\n",
        "                    answer_text = full_generated_text[step_start:].strip()\n",
        "                else:\n",
        "                    # Fallback: everything after the problem statement\n",
        "                    answer_text = full_generated_text[problem_start + len(problem_marker):].strip()\n",
        "            else:\n",
        "                # Fallback: remove generation prompt entirely\n",
        "                answer_text = full_generated_text[len(generation_prompt):].strip()\n",
        "        else:\n",
        "            answer_text = full_generated_text[len(generation_prompt):].strip()\n",
        "\n",
        "        numerical_answer = self.extract_numerical_answer(answer_text)\n",
        "\n",
        "        # Label activations (from ORIGINAL prompt only - clean MLP analysis)\n",
        "        labeled_activations = {}\n",
        "        for layer_name, activation in input_activations.items():\n",
        "            labeled_key = f\"row_{row_index}_{prompt_name.lower()}_{layer_name}\"\n",
        "            labeled_activations[labeled_key] = activation\n",
        "\n",
        "        # Label input attention weights (like second script)\n",
        "        labeled_input_attention_weights = {}\n",
        "        for layer_name, attention in input_attention_weights.items():\n",
        "            # Clean up naming\n",
        "            clean_name = layer_name.replace(\"_direct\", \"\")\n",
        "            labeled_key = f\"row_{row_index}_{prompt_name.lower()}_{clean_name}\"\n",
        "            labeled_input_attention_weights[labeled_key] = attention\n",
        "\n",
        "        # Label full sequence attention weights\n",
        "        labeled_full_sequence_attention_weights = {}\n",
        "        for layer_name, attention in full_sequence_attention_weights.items():\n",
        "            labeled_key = f\"row_{row_index}_{prompt_name.lower()}_{layer_name}\"\n",
        "            labeled_full_sequence_attention_weights[labeled_key] = attention\n",
        "\n",
        "        # Token metadata for the original prompt (like second script)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        input_tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "        # For generated tokens, include full sequence\n",
        "        generation_inputs = self.tokenizer(generation_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        all_tokens = self.tokenizer.convert_ids_to_tokens(generated_sequence[0])\n",
        "\n",
        "        # Input attention metadata (exact like second script)\n",
        "        input_attention_metadata = {\n",
        "            'prompt_name': prompt_name,\n",
        "            'row_index': row_index,\n",
        "            'input_tokens': input_tokens,\n",
        "            'all_tokens': all_tokens,\n",
        "            'input_token_ids': inputs['input_ids'][0].cpu().numpy().tolist(),\n",
        "            'all_token_ids': generated_sequence[0].cpu().numpy().tolist(),\n",
        "            'input_length': len(input_tokens),\n",
        "            'total_length': len(all_tokens),\n",
        "            'generated_length': len(all_tokens) - len(input_tokens),\n",
        "            'full_generated_text': full_generated_text,\n",
        "            'answer_text': answer_text,\n",
        "            'prompt_text': prompt,\n",
        "            'modified_prompt': generation_prompt if not use_few_shot else None\n",
        "        }\n",
        "\n",
        "        # Full sequence attention metadata\n",
        "        full_sequence_attention_metadata = {\n",
        "            'prompt_name': prompt_name,\n",
        "            'row_index': row_index,\n",
        "            'input_tokens': input_tokens,\n",
        "            'all_tokens': all_tokens,\n",
        "            'input_token_ids': inputs['input_ids'][0].cpu().numpy().tolist(),\n",
        "            'all_token_ids': generated_sequence[0].cpu().numpy().tolist(),\n",
        "            'input_length': len(input_tokens),\n",
        "            'total_length': len(all_tokens),\n",
        "            'generated_length': len(all_tokens) - len(generation_inputs['input_ids'][0]),\n",
        "            'clean_answer_text': answer_text,\n",
        "            'numerical_answer': numerical_answer,\n",
        "            'original_prompt': prompt,\n",
        "            'generation_prompt': generation_prompt if use_few_shot else prompt,\n",
        "            'used_few_shot': use_few_shot,\n",
        "            'few_shot_examples_count': 3 if use_few_shot else 0\n",
        "        }\n",
        "\n",
        "        return answer_text, numerical_answer, labeled_activations, labeled_input_attention_weights, labeled_full_sequence_attention_weights, input_attention_metadata, full_sequence_attention_metadata\n",
        "\n",
        "    def process_dataset(self, df: pd.DataFrame, seed: int = 42, batch_size: int = 1, use_few_shot: bool = True):\n",
        "        results_list = []\n",
        "        all_activations = []\n",
        "        all_input_attention_weights = []\n",
        "        all_full_sequence_attention_weights = []\n",
        "        all_input_attention_metadata = []\n",
        "        all_full_sequence_attention_metadata = []\n",
        "\n",
        "        total_rows = len(df)\n",
        "        print(f\"Processing {total_rows} rows with {'few-shot' if use_few_shot else 'simple'} prompting\")\n",
        "\n",
        "        for batch_start in range(0, total_rows, batch_size):\n",
        "            batch_end = min(batch_start + batch_size, total_rows)\n",
        "            batch_df = df.iloc[batch_start:batch_end]\n",
        "\n",
        "            for idx, row in batch_df.iterrows():\n",
        "                print(f\"Processing row {idx + 1}/{total_rows}\")\n",
        "\n",
        "                row_result = {\n",
        "                    'Index': row.get('Index', idx),\n",
        "                    'Source': row.get('Source', ''),\n",
        "                    'Category': row.get('Category', ''),\n",
        "                    'Difficulty_Level': row.get('Difficulty_Level', ''),\n",
        "                    'Prompt_1': row.get('Prompt_1', ''),\n",
        "                    'Prompt_2': row.get('Prompt_2', ''),\n",
        "                    'Prompt_3': row.get('Prompt_3', '')\n",
        "                }\n",
        "\n",
        "                row_activations = {}\n",
        "                row_input_attention_weights = {}\n",
        "                row_full_sequence_attention_weights = {}\n",
        "                row_input_attention_metadata = {}\n",
        "                row_full_sequence_attention_metadata = {}\n",
        "\n",
        "                for prompt_col in ['Prompt_1', 'Prompt_2', 'Prompt_3']:\n",
        "                    if prompt_col in df.columns and pd.notna(row[prompt_col]) and str(row[prompt_col]).strip():\n",
        "                        prompt_text = str(row[prompt_col]).strip()\n",
        "\n",
        "                        try:\n",
        "                            full_output, numerical_answer, activations, input_attention_weights, full_sequence_attention_weights, input_attn_metadata, full_seq_attn_metadata = self.process_single_prompt(\n",
        "                                prompt_text, prompt_col, idx, seed=seed, use_few_shot=use_few_shot\n",
        "                            )\n",
        "\n",
        "                            row_result[f\"{prompt_col}_Prediction_Full\"] = full_output\n",
        "                            row_result[f\"{prompt_col}_Prediction_Extracted\"] = numerical_answer\n",
        "\n",
        "                            # Ensure we have valid dictionaries before updating\n",
        "                            if activations:\n",
        "                                row_activations.update(activations)\n",
        "                            if input_attention_weights:\n",
        "                                row_input_attention_weights.update(input_attention_weights)\n",
        "                            if full_sequence_attention_weights:\n",
        "                                row_full_sequence_attention_weights.update(full_sequence_attention_weights)\n",
        "\n",
        "                            row_input_attention_metadata[prompt_col] = input_attn_metadata\n",
        "                            row_full_sequence_attention_metadata[prompt_col] = full_seq_attn_metadata\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error processing {prompt_col}: {str(e)}\")\n",
        "                            import traceback\n",
        "                            print(f\"Full traceback: {traceback.format_exc()}\")\n",
        "                            row_result[f\"{prompt_col}_Prediction_Full\"] = \"ERROR\"\n",
        "                            row_result[f\"{prompt_col}_Prediction_Extracted\"] = \"ERROR\"\n",
        "                            row_input_attention_metadata[prompt_col] = {\"error\": str(e)}\n",
        "                            row_full_sequence_attention_metadata[prompt_col] = {\"error\": str(e)}\n",
        "\n",
        "                    else:\n",
        "                        row_result[f\"{prompt_col}_Prediction_Full\"] = \"No prompt\"\n",
        "                        row_result[f\"{prompt_col}_Prediction_Extracted\"] = \"No prompt\"\n",
        "\n",
        "                results_list.append(row_result)\n",
        "                all_activations.append(row_activations)\n",
        "                all_input_attention_weights.append(row_input_attention_weights)\n",
        "                all_full_sequence_attention_weights.append(row_full_sequence_attention_weights)\n",
        "                all_input_attention_metadata.append(row_input_attention_metadata)\n",
        "                all_full_sequence_attention_metadata.append(row_full_sequence_attention_metadata)\n",
        "\n",
        "                if self.device == \"cuda\":\n",
        "                    torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        results_df = pd.DataFrame(results_list)\n",
        "        print(f\"Processing complete. Results shape: {results_df.shape}\")\n",
        "\n",
        "        return results_df, all_activations, all_input_attention_weights, all_full_sequence_attention_weights, all_input_attention_metadata, all_full_sequence_attention_metadata\n",
        "\n",
        "    def save_results(self, results_df: pd.DataFrame, all_activations: List[Dict],\n",
        "                    all_input_attention_weights: List[Dict], all_full_sequence_attention_weights: List[Dict],\n",
        "                    all_input_attention_metadata: List[Dict], all_full_sequence_attention_metadata: List[Dict],\n",
        "                    output_dir: str = \"llama_output\"):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save main results\n",
        "        results_df.to_csv(f\"{output_dir}/results_with_predictions.csv\", index=False)\n",
        "        print(f\"Results saved to {output_dir}/results_with_predictions.csv\")\n",
        "\n",
        "        # Save MLP activations in separate folder\n",
        "        mlp_dir = f\"{output_dir}/activations\"\n",
        "        os.makedirs(mlp_dir, exist_ok=True)\n",
        "\n",
        "        activation_files_created = 0\n",
        "        for row_idx, row_activations in enumerate(all_activations):\n",
        "            if not row_activations:\n",
        "                continue\n",
        "\n",
        "            row_dir = f\"{mlp_dir}/row_{row_idx}\"\n",
        "            os.makedirs(row_dir, exist_ok=True)\n",
        "\n",
        "            prompt_activations = {}\n",
        "            for activation_key, activation_array in row_activations.items():\n",
        "                parts = activation_key.split('_')\n",
        "                if len(parts) >= 4:\n",
        "                    prompt_part = f\"{parts[2]}_{parts[3]}\"\n",
        "                    layer_part = '_'.join(parts[4:])\n",
        "\n",
        "                    if prompt_part not in prompt_activations:\n",
        "                        prompt_activations[prompt_part] = {}\n",
        "                    prompt_activations[prompt_part][layer_part] = activation_array\n",
        "\n",
        "            for prompt_name, prompt_layers in prompt_activations.items():\n",
        "                prompt_dir = f\"{row_dir}/{prompt_name}\"\n",
        "                os.makedirs(prompt_dir, exist_ok=True)\n",
        "\n",
        "                for layer_name, activation_array in prompt_layers.items():\n",
        "                    filename = f\"{layer_name}.npy\"\n",
        "                    filepath = f\"{prompt_dir}/{filename}\"\n",
        "                    np.save(filepath, activation_array)\n",
        "                    activation_files_created += 1\n",
        "\n",
        "        print(f\"Saved {activation_files_created} MLP activation files\")\n",
        "\n",
        "        # Save input attention weights (like second script)\n",
        "        input_attn_dir = f\"{output_dir}/input_attention\"\n",
        "        os.makedirs(input_attn_dir, exist_ok=True)\n",
        "\n",
        "        input_attention_files_created = 0\n",
        "        for row_idx, row_attention_weights in enumerate(all_input_attention_weights):\n",
        "            if not row_attention_weights:\n",
        "                continue\n",
        "\n",
        "            row_dir = f\"{input_attn_dir}/row_{row_idx}\"\n",
        "            os.makedirs(row_dir, exist_ok=True)\n",
        "\n",
        "            prompt_attentions = {}\n",
        "            for attention_key, attention_array in row_attention_weights.items():\n",
        "                parts = attention_key.split('_')\n",
        "                if len(parts) >= 5:  # row_X_prompt_Y_attn_layer_Z\n",
        "                    prompt_part = f\"{parts[2]}_{parts[3]}\"\n",
        "                    layer_part = '_'.join(parts[4:])\n",
        "\n",
        "                    if prompt_part not in prompt_attentions:\n",
        "                        prompt_attentions[prompt_part] = {}\n",
        "                    prompt_attentions[prompt_part][layer_part] = attention_array\n",
        "\n",
        "            for prompt_name, prompt_layers in prompt_attentions.items():\n",
        "                prompt_dir = f\"{row_dir}/{prompt_name}\"\n",
        "                os.makedirs(prompt_dir, exist_ok=True)\n",
        "\n",
        "                for layer_name, attention_array in prompt_layers.items():\n",
        "                    filename = f\"{layer_name}.npy\"\n",
        "                    filepath = f\"{prompt_dir}/{filename}\"\n",
        "                    np.save(filepath, attention_array)\n",
        "                    input_attention_files_created += 1\n",
        "\n",
        "        print(f\"Saved {input_attention_files_created} input attention weight files\")\n",
        "\n",
        "        # Save full sequence attention weights\n",
        "        full_seq_attn_dir = f\"{output_dir}/full_sequence_attention\"\n",
        "        os.makedirs(full_seq_attn_dir, exist_ok=True)\n",
        "\n",
        "        full_sequence_attention_files_created = 0\n",
        "        for row_idx, row_attention_weights in enumerate(all_full_sequence_attention_weights):\n",
        "            if not row_attention_weights:\n",
        "                continue\n",
        "\n",
        "            row_dir = f\"{full_seq_attn_dir}/row_{row_idx}\"\n",
        "            os.makedirs(row_dir, exist_ok=True)\n",
        "\n",
        "            prompt_attentions = {}\n",
        "            for attention_key, attention_array in row_attention_weights.items():\n",
        "                parts = attention_key.split('_')\n",
        "                if len(parts) >= 5:\n",
        "                    prompt_part = f\"{parts[2]}_{parts[3]}\"\n",
        "                    layer_part = '_'.join(parts[4:])\n",
        "\n",
        "                    if prompt_part not in prompt_attentions:\n",
        "                        prompt_attentions[prompt_part] = {}\n",
        "                    prompt_attentions[prompt_part][layer_part] = attention_array\n",
        "\n",
        "            for prompt_name, prompt_layers in prompt_attentions.items():\n",
        "                prompt_dir = f\"{row_dir}/{prompt_name}\"\n",
        "                os.makedirs(prompt_dir, exist_ok=True)\n",
        "\n",
        "                for layer_name, attention_array in prompt_layers.items():\n",
        "                    filename = f\"{layer_name}.npy\"\n",
        "                    filepath = f\"{prompt_dir}/{filename}\"\n",
        "                    np.save(filepath, attention_array)\n",
        "                    full_sequence_attention_files_created += 1\n",
        "\n",
        "        print(f\"Saved {full_sequence_attention_files_created} full sequence attention weight files\")\n",
        "\n",
        "        # Save input attention metadata (like second script)\n",
        "        with open(f\"{output_dir}/input_attention_metadata.json\", 'w') as f:\n",
        "            json.dump(all_input_attention_metadata, f, indent=2)\n",
        "\n",
        "        # Save full sequence attention metadata\n",
        "        with open(f\"{output_dir}/full_sequence_attention_metadata.json\", 'w') as f:\n",
        "            json.dump(all_full_sequence_attention_metadata, f, indent=2)\n",
        "\n",
        "        # Save separate summaries - MLP Activations Summary\n",
        "        activation_summary = {}\n",
        "        total_activations = 0\n",
        "\n",
        "        for row_idx, row_activations in enumerate(all_activations):\n",
        "            if not row_activations:\n",
        "                continue\n",
        "\n",
        "            row_summary = {}\n",
        "            for key, activation in row_activations.items():\n",
        "                parts = key.split('_')\n",
        "                if len(parts) >= 4:\n",
        "                    prompt_part = f\"{parts[2]}_{parts[3]}\"\n",
        "                    layer_part = '_'.join(parts[4:])\n",
        "\n",
        "                    if prompt_part not in row_summary:\n",
        "                        row_summary[prompt_part] = {}\n",
        "\n",
        "                    row_summary[prompt_part][layer_part] = {\n",
        "                        'shape': list(activation.shape),\n",
        "                        'dtype': str(activation.dtype),\n",
        "                        'sequence_length': activation.shape[1] if len(activation.shape) > 1 else 1,\n",
        "                        'hidden_dim': activation.shape[2] if len(activation.shape) > 2 else activation.shape[-1],\n",
        "                        'file_path': f\"activations/row_{row_idx}/{prompt_part}/{layer_part}.npy\"\n",
        "                    }\n",
        "                    total_activations += 1\n",
        "\n",
        "            if row_summary:\n",
        "                activation_summary[f\"row_{row_idx}\"] = row_summary\n",
        "\n",
        "        with open(f\"{output_dir}/activation_summary.json\", 'w') as f:\n",
        "            json.dump(activation_summary, f, indent=2)\n",
        "\n",
        "        # Input Attention Weights Summary (exact like second script)\n",
        "        input_attention_summary = {}\n",
        "        total_input_attentions = 0\n",
        "\n",
        "        for row_idx, row_attentions in enumerate(all_input_attention_weights):\n",
        "            if not row_attentions:\n",
        "                continue\n",
        "\n",
        "            row_summary = {}\n",
        "            for key, attention in row_attentions.items():\n",
        "                parts = key.split('_')\n",
        "                if len(parts) >= 5:\n",
        "                    prompt_part = f\"{parts[2]}_{parts[3]}\"\n",
        "                    layer_part = '_'.join(parts[4:])\n",
        "\n",
        "                    if prompt_part not in row_summary:\n",
        "                        row_summary[prompt_part] = {}\n",
        "\n",
        "                    row_summary[prompt_part][layer_part] = {\n",
        "                        'shape': list(attention.shape),\n",
        "                        'dtype': str(attention.dtype),\n",
        "                        'batch_size': attention.shape[0] if len(attention.shape) > 0 else 1,\n",
        "                        'num_heads': attention.shape[1] if len(attention.shape) > 1 else 1,\n",
        "                        'sequence_length': attention.shape[2] if len(attention.shape) > 2 else 1,\n",
        "                        'file_path': f\"input_attention/row_{row_idx}/{prompt_part}/{layer_part}.npy\"\n",
        "                    }\n",
        "                    total_input_attentions += 1\n",
        "\n",
        "            if row_summary:\n",
        "                input_attention_summary[f\"row_{row_idx}\"] = row_summary\n",
        "\n",
        "        with open(f\"{output_dir}/input_attention_summary.json\", 'w') as f:\n",
        "            json.dump(input_attention_summary, f, indent=2)\n",
        "\n",
        "        # Full Sequence Attention Weights Summary\n",
        "        full_sequence_attention_summary = {}\n",
        "        total_full_sequence_attentions = 0\n",
        "\n",
        "        for row_idx, row_attentions in enumerate(all_full_sequence_attention_weights):\n",
        "            if not row_attentions:\n",
        "                continue\n",
        "\n",
        "            row_summary = {}\n",
        "            for key, attention in row_attentions.items():\n",
        "                parts = key.split('_')\n",
        "                if len(parts) >= 5:\n",
        "                    prompt_part = f\"{parts[2]}_{parts[3]}\"\n",
        "                    layer_part = '_'.join(parts[4:])\n",
        "\n",
        "                    if prompt_part not in row_summary:\n",
        "                        row_summary[prompt_part] = {}\n",
        "\n",
        "                    row_summary[prompt_part][layer_part] = {\n",
        "                        'shape': list(attention.shape),\n",
        "                        'dtype': str(attention.dtype),\n",
        "                        'batch_size': attention.shape[0] if len(attention.shape) > 0 else 1,\n",
        "                        'num_heads': attention.shape[1] if len(attention.shape) > 1 else 1,\n",
        "                        'sequence_length': attention.shape[2] if len(attention.shape) > 2 else 1,\n",
        "                        'attention_type': 'full_sequence',\n",
        "                        'file_path': f\"full_sequence_attention/row_{row_idx}/{prompt_part}/{layer_part}.npy\"\n",
        "                    }\n",
        "                    total_full_sequence_attentions += 1\n",
        "\n",
        "            if row_summary:\n",
        "                full_sequence_attention_summary[f\"row_{row_idx}\"] = row_summary\n",
        "\n",
        "        with open(f\"{output_dir}/full_sequence_attention_summary.json\", 'w') as f:\n",
        "            json.dump(full_sequence_attention_summary, f, indent=2)\n",
        "\n",
        "        print(f\"\\nResults saved to: {output_dir}/\")\n",
        "        print(f\"Files created: {activation_files_created + input_attention_files_created + full_sequence_attention_files_created}\")\n",
        "        print(f\"  MLP activations: {total_activations}\")\n",
        "        print(f\"  Input attention weights: {total_input_attentions}\")\n",
        "        print(f\"  Full sequence attention weights: {total_full_sequence_attentions}\")\n",
        "        print(\"Folder structure:\")\n",
        "        print(\"  activations/row_X/prompt_Y/layer_Z.npy\")\n",
        "        print(\"  input_attention/row_X/prompt_Y/attn_layer_Z.npy\")\n",
        "        print(\"  full_sequence_attention/row_X/prompt_Y/full_seq_attn_layer_Z.npy\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "        self.hooks.clear()\n",
        "        self.activations.clear()\n",
        "        self.attention_weights.clear()\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "def select_test_rows(df: pd.DataFrame, num_rows: int = 5, method: str = \"first\"):\n",
        "    if method == \"first\":\n",
        "        test_df = df.head(num_rows)\n",
        "    elif method == \"last\":\n",
        "        test_df = df.tail(num_rows)\n",
        "    elif method == \"random\":\n",
        "        test_df = df.sample(n=min(num_rows, len(df)), random_state=42)\n",
        "    else:\n",
        "        test_df = df.head(num_rows)\n",
        "\n",
        "    print(f\"Selected {len(test_df)} rows for processing\")\n",
        "    return test_df\n",
        "\n",
        "def main():\n",
        "    extractor = LlamaActivationExtractor(\n",
        "        model_name=\"unsloth/Llama-3.2-1b\" #______________Model___________#\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        print(\"Loading dataset...\")\n",
        "        df = pd.read_csv('/content/dataset.csv')  #______Dataset___________#\n",
        "        print(f\"Dataset loaded: {len(df)} rows\")\n",
        "######################################\n",
        "        # Test mode settings\n",
        "        TEST_MODE = True # Write False for full dataset inference\n",
        "        NUM_TEST_ROWS = 10\n",
        "        TEST_METHOD = \"first\"\n",
        "        USE_FEW_SHOT = True  # Set to False to disable few-shot prompting\n",
        "##########################################\n",
        "        if TEST_MODE:\n",
        "            df = select_test_rows(df, num_rows=NUM_TEST_ROWS, method=TEST_METHOD)\n",
        "            output_folder = f\"Test_output_{NUM_TEST_ROWS}rows\"\n",
        "        else:\n",
        "            output_folder = \"full_output\"\n",
        "\n",
        "        print(f\"Processing {'test' if TEST_MODE else 'full'} dataset with {'few-shot' if USE_FEW_SHOT else 'simple'} prompting\")\n",
        "\n",
        "        results_df, all_activations, all_input_attention_weights, all_full_sequence_attention_weights, all_input_attention_metadata, all_full_sequence_attention_metadata = extractor.process_dataset(\n",
        "            df, seed=42, batch_size=1, use_few_shot=USE_FEW_SHOT\n",
        "        )\n",
        "\n",
        "        print(\"Saving results...\")\n",
        "        extractor.save_results(results_df, all_activations, all_input_attention_weights, all_full_sequence_attention_weights, all_input_attention_metadata, all_full_sequence_attention_metadata, output_folder)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    finally:\n",
        "        extractor.cleanup()\n",
        "        print(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s1ZbZO2Q2Gh9",
        "outputId": "20b98993-1d71-4b10-cc90-fe5089e58b11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: unsloth/Llama-3.2-1b\n",
            "Registered 16 activation hooks\n",
            "Model loaded. Registered 16 hooks (MLP + Attention)\n",
            "Loading dataset...\n",
            "Dataset loaded: 126 rows\n",
            "Selected 10 rows for processing\n",
            "Processing test dataset with few-shot prompting\n",
            "Processing 10 rows with few-shot prompting\n",
            "Processing row 1/10\n",
            "    Extracting activations and attention for Prompt_1...\n",
            "        Processing input with 57 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 1: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 2: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 3: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 4: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 5: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 6: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 7: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 8: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 9: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 10: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 11: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 12: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 13: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 14: shape (1, 32, 57, 57)\n",
            "          Captured attention layer 15: shape (1, 32, 57, 57)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 365 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 514)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 514)\n",
            "        Final full sequence attention count: 16\n",
            "Processing row 2/10\n",
            "    Extracting activations and attention for Prompt_1...\n",
            "        Processing input with 69 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 1: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 2: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 3: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 4: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 5: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 6: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 7: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 8: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 9: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 10: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 11: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 12: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 13: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 14: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 15: shape (1, 32, 69, 69)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 377 input tokens\n",
            "        Generation completed\n",
            "        Found 78 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 454)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_2...\n",
            "        Processing input with 69 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 1: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 2: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 3: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 4: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 5: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 6: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 7: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 8: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 9: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 10: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 11: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 12: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 13: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 14: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 15: shape (1, 32, 69, 69)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 377 input tokens\n",
            "        Generation completed\n",
            "        Found 78 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 454)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_3...\n",
            "        Processing input with 69 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 1: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 2: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 3: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 4: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 5: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 6: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 7: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 8: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 9: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 10: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 11: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 12: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 13: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 14: shape (1, 32, 69, 69)\n",
            "          Captured attention layer 15: shape (1, 32, 69, 69)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 377 input tokens\n",
            "        Generation completed\n",
            "        Found 78 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 454)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 454)\n",
            "        Final full sequence attention count: 16\n",
            "Processing row 3/10\n",
            "    Extracting activations and attention for Prompt_1...\n",
            "        Processing input with 66 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 1: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 2: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 3: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 4: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 5: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 6: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 7: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 8: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 9: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 10: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 11: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 12: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 13: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 14: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 15: shape (1, 32, 66, 66)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 374 input tokens\n",
            "        Generation completed\n",
            "        Found 78 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 451)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_2...\n",
            "        Processing input with 66 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 1: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 2: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 3: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 4: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 5: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 6: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 7: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 8: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 9: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 10: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 11: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 12: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 13: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 14: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 15: shape (1, 32, 66, 66)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 374 input tokens\n",
            "        Generation completed\n",
            "        Found 78 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 451)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 451)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_3...\n",
            "        Processing input with 66 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 1: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 2: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 3: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 4: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 5: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 6: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 7: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 8: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 9: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 10: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 11: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 12: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 13: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 14: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 15: shape (1, 32, 66, 66)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 374 input tokens\n",
            "        Generation completed\n",
            "        Found 79 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 452)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 452)\n",
            "        Final full sequence attention count: 16\n",
            "Processing row 4/10\n",
            "    Extracting activations and attention for Prompt_1...\n",
            "        Processing input with 70 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 1: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 2: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 3: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 4: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 5: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 6: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 7: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 8: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 9: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 10: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 11: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 12: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 13: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 14: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 15: shape (1, 32, 70, 70)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 378 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 527)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_2...\n",
            "        Processing input with 70 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 1: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 2: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 3: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 4: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 5: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 6: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 7: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 8: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 9: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 10: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 11: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 12: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 13: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 14: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 15: shape (1, 32, 70, 70)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 378 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 527)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_3...\n",
            "        Processing input with 70 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 1: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 2: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 3: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 4: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 5: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 6: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 7: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 8: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 9: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 10: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 11: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 12: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 13: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 14: shape (1, 32, 70, 70)\n",
            "          Captured attention layer 15: shape (1, 32, 70, 70)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 378 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 527)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 527)\n",
            "        Final full sequence attention count: 16\n",
            "Processing row 5/10\n",
            "    Extracting activations and attention for Prompt_1...\n",
            "        Processing input with 66 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 1: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 2: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 3: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 4: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 5: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 6: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 7: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 8: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 9: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 10: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 11: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 12: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 13: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 14: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 15: shape (1, 32, 66, 66)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 374 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 523)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_2...\n",
            "        Processing input with 66 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 1: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 2: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 3: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 4: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 5: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 6: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 7: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 8: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 9: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 10: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 11: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 12: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 13: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 14: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 15: shape (1, 32, 66, 66)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 374 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 523)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_3...\n",
            "        Processing input with 66 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 1: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 2: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 3: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 4: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 5: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 6: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 7: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 8: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 9: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 10: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 11: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 12: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 13: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 14: shape (1, 32, 66, 66)\n",
            "          Captured attention layer 15: shape (1, 32, 66, 66)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 374 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 523)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 523)\n",
            "        Final full sequence attention count: 16\n",
            "Processing row 6/10\n",
            "    Extracting activations and attention for Prompt_1...\n",
            "        Processing input with 81 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 1: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 2: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 3: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 4: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 5: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 6: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 7: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 8: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 9: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 10: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 11: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 12: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 13: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 14: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 15: shape (1, 32, 81, 81)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 389 input tokens\n",
            "        Generation completed\n",
            "        Found 78 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 466)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 466)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_2...\n",
            "        Processing input with 81 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 1: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 2: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 3: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 4: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 5: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 6: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 7: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 8: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 9: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 10: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 11: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 12: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 13: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 14: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 15: shape (1, 32, 81, 81)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 389 input tokens\n",
            "        Generation completed\n",
            "        Found 77 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 465)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 465)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_3...\n",
            "        Processing input with 81 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 1: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 2: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 3: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 4: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 5: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 6: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 7: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 8: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 9: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 10: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 11: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 12: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 13: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 14: shape (1, 32, 81, 81)\n",
            "          Captured attention layer 15: shape (1, 32, 81, 81)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 389 input tokens\n",
            "        Generation completed\n",
            "        Found 74 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 462)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 462)\n",
            "        Final full sequence attention count: 16\n",
            "Processing row 7/10\n",
            "    Extracting activations and attention for Prompt_1...\n",
            "        Processing input with 78 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 1: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 2: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 3: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 4: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 5: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 6: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 7: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 8: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 9: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 10: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 11: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 12: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 13: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 14: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 15: shape (1, 32, 78, 78)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 386 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 535)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 535)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_2...\n",
            "        Processing input with 78 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 1: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 2: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 3: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 4: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 5: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 6: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 7: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 8: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 9: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 10: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 11: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 12: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 13: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 14: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 15: shape (1, 32, 78, 78)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 386 input tokens\n",
            "        Generation completed\n",
            "        Found 127 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 512)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 512)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_3...\n",
            "        Processing input with 78 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 1: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 2: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 3: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 4: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 5: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 6: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 7: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 8: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 9: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 10: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 11: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 12: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 13: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 14: shape (1, 32, 78, 78)\n",
            "          Captured attention layer 15: shape (1, 32, 78, 78)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 386 input tokens\n",
            "        Generation completed\n",
            "        Found 86 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 471)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 471)\n",
            "        Final full sequence attention count: 16\n",
            "Processing row 8/10\n",
            "    Extracting activations and attention for Prompt_1...\n",
            "        Processing input with 39 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 1: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 2: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 3: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 4: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 5: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 6: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 7: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 8: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 9: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 10: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 11: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 12: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 13: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 14: shape (1, 32, 39, 39)\n",
            "          Captured attention layer 15: shape (1, 32, 39, 39)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 346 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 495)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 495)\n",
            "        Final full sequence attention count: 16\n",
            "Processing row 9/10\n",
            "    Extracting activations and attention for Prompt_1...\n",
            "        Processing input with 51 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 1: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 2: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 3: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 4: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 5: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 6: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 7: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 8: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 9: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 10: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 11: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 12: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 13: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 14: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 15: shape (1, 32, 51, 51)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 358 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 507)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_2...\n",
            "        Processing input with 51 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 1: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 2: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 3: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 4: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 5: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 6: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 7: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 8: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 9: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 10: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 11: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 12: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 13: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 14: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 15: shape (1, 32, 51, 51)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 358 input tokens\n",
            "        Generation completed\n",
            "        Found 57 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 414)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 414)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_3...\n",
            "        Processing input with 51 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 1: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 2: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 3: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 4: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 5: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 6: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 7: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 8: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 9: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 10: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 11: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 12: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 13: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 14: shape (1, 32, 51, 51)\n",
            "          Captured attention layer 15: shape (1, 32, 51, 51)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 358 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 507)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 507)\n",
            "        Final full sequence attention count: 16\n",
            "Processing row 10/10\n",
            "    Extracting activations and attention for Prompt_1...\n",
            "        Processing input with 48 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 1: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 2: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 3: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 4: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 5: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 6: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 7: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 8: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 9: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 10: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 11: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 12: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 13: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 14: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 15: shape (1, 32, 48, 48)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 355 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 504)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_2...\n",
            "        Processing input with 48 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 1: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 2: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 3: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 4: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 5: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 6: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 7: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 8: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 9: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 10: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 11: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 12: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 13: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 14: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 15: shape (1, 32, 48, 48)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 355 input tokens\n",
            "        Generation completed\n",
            "        Found 57 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 411)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 411)\n",
            "        Final full sequence attention count: 16\n",
            "    Extracting activations and attention for Prompt_3...\n",
            "        Processing input with 48 tokens\n",
            "        Model forward pass completed\n",
            "        Found 16 attention matrices from model output\n",
            "          Captured attention layer 0: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 1: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 2: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 3: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 4: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 5: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 6: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 7: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 8: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 9: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 10: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 11: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 12: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 13: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 14: shape (1, 32, 48, 48)\n",
            "          Captured attention layer 15: shape (1, 32, 48, 48)\n",
            "        Final counts - Activations: 16, Attention: 16\n",
            "        Generating with 355 input tokens\n",
            "        Generation completed\n",
            "        Found 150 generation steps with attention\n",
            "        Processing final step with 16 layers\n",
            "          Captured full sequence attention layer 0: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 1: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 2: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 3: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 4: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 5: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 6: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 7: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 8: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 9: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 10: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 11: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 12: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 13: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 14: shape (1, 32, 1, 504)\n",
            "          Captured full sequence attention layer 15: shape (1, 32, 1, 504)\n",
            "        Final full sequence attention count: 16\n",
            "Processing complete. Results shape: (10, 13)\n",
            "Saving results...\n",
            "Results saved to Test_output_10rows/results_with_predictions.csv\n",
            "Saved 416 MLP activation files\n",
            "Saved 416 input attention weight files\n",
            "Saved 416 full sequence attention weight files\n",
            "\n",
            "Results saved to: Test_output_10rows/\n",
            "Files created: 1248\n",
            "  MLP activations: 416\n",
            "  Input attention weights: 416\n",
            "  Full sequence attention weights: 416\n",
            "Folder structure:\n",
            "  activations/row_X/prompt_Y/layer_Z.npy\n",
            "  input_attention/row_X/prompt_Y/attn_layer_Z.npy\n",
            "  full_sequence_attention/row_X/prompt_Y/full_seq_attn_layer_Z.npy\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class AttentionDataPreparationIndividual:\n",
        "    def __init__(self, data_dir: str, results_csv: str, token_metadata_json: str):\n",
        "        \"\"\"\n",
        "        Prepare Data: Load attention files, normalize rows, identify individual phrase positions (6 + 1)\n",
        "\n",
        "        Args:\n",
        "            data_dir: Base directory containing row_X/prompt_Y/attn_layer_Z.npy structure\n",
        "            results_csv: Path to results CSV file\n",
        "            token_metadata_json: Path to token metadata JSON file\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.attention_dir = data_dir\n",
        "\n",
        "        # Load dataset and metadata\n",
        "        self.results_df = pd.read_csv(results_csv)\n",
        "        with open(token_metadata_json, 'r') as f:\n",
        "            self.token_metadata = json.load(f)\n",
        "\n",
        "        # Define individual bias phrases (6 + 1 combined)\n",
        "        self.individual_bias_phrases = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"]\n",
        "        self.analysis_keys = self.individual_bias_phrases + [\"combined\"]\n",
        "\n",
        "        # Storage for processed data\n",
        "        self.normalized_attention = {}\n",
        "        self.hint_positions_individual = {}  # {phrase: {row_prompt: positions}}\n",
        "        self.validation_log = []\n",
        "\n",
        "        # Prompt mapping for file system\n",
        "        self.prompt_mapping = {\n",
        "            'Prompt_1': 'prompt_1',\n",
        "            'Prompt_2': 'prompt_2',\n",
        "            'Prompt_3': 'prompt_3'\n",
        "        }\n",
        "\n",
        "        # Initialize storage for each phrase analysis\n",
        "        for phrase in self.analysis_keys:\n",
        "            self.hint_positions_individual[phrase] = {}\n",
        "\n",
        "        print(f\"Loaded {len(self.results_df)} rows from dataset\")\n",
        "        print(f\"Loaded token metadata for {len(self.token_metadata)} rows\")\n",
        "        print(f\"Individual phrase analyses: {', '.join(self.individual_bias_phrases)}\")\n",
        "        print(f\"Total analyses: {len(self.analysis_keys)} (6 individual + 1 combined)\")\n",
        "\n",
        "    def validate_data_structure(self) -> Dict[str, int]:\n",
        "        \"\"\"Validate the attention data structure and count available files\"\"\"\n",
        "        print(\"Validating data structure...\")\n",
        "\n",
        "        validation_stats = {\n",
        "            'total_rows_checked': 0,\n",
        "            'rows_with_attention_data': 0,\n",
        "            'total_attention_files': 0,\n",
        "            'missing_files': 0,\n",
        "            'invalid_files': 0\n",
        "        }\n",
        "\n",
        "        max_check = min(10, len(self.results_df))\n",
        "\n",
        "        for row_idx in range(max_check):\n",
        "            validation_stats['total_rows_checked'] += 1\n",
        "            row_has_data = False\n",
        "\n",
        "            for prompt_col in ['Prompt_1', 'Prompt_2', 'Prompt_3']:\n",
        "                if pd.isna(self.results_df.iloc[row_idx][prompt_col]):\n",
        "                    continue\n",
        "\n",
        "                prompt_file_name = self.prompt_mapping[prompt_col]\n",
        "                row_dir = os.path.join(self.attention_dir, f\"row_{row_idx}\", prompt_file_name)\n",
        "\n",
        "                if os.path.exists(row_dir):\n",
        "                    npy_files = [f for f in os.listdir(row_dir) if f.endswith('.npy') and 'attn_layer_' in f]\n",
        "\n",
        "                    for npy_file in npy_files:\n",
        "                        file_path = os.path.join(row_dir, npy_file)\n",
        "                        try:\n",
        "                            test_load = np.load(file_path)\n",
        "                            if len(test_load.shape) == 4:\n",
        "                                validation_stats['total_attention_files'] += 1\n",
        "                                row_has_data = True\n",
        "                            else:\n",
        "                                self.validation_log.append(f\"Invalid shape {test_load.shape} in {file_path}\")\n",
        "                                validation_stats['invalid_files'] += 1\n",
        "                        except Exception as e:\n",
        "                            self.validation_log.append(f\"Cannot load {file_path}: {e}\")\n",
        "                            validation_stats['invalid_files'] += 1\n",
        "                else:\n",
        "                    validation_stats['missing_files'] += 1\n",
        "\n",
        "            if row_has_data:\n",
        "                validation_stats['rows_with_attention_data'] += 1\n",
        "\n",
        "        print(f\"Checked {validation_stats['total_rows_checked']} rows\")\n",
        "        print(f\"Found {validation_stats['total_attention_files']} valid attention files\")\n",
        "\n",
        "        return validation_stats\n",
        "\n",
        "    def load_and_normalize_attention(self, row_idx: int, prompt_name: str, layer_idx: int) -> Optional[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Load attention weights and normalize rows to ensure each row sums to 1\n",
        "\n",
        "        Args:\n",
        "            row_idx: Row index in dataset\n",
        "            prompt_name: Prompt file name (prompt_1, prompt_2, prompt_3)\n",
        "            layer_idx: Layer index (0 to num_layers-1)\n",
        "\n",
        "        Returns:\n",
        "            Normalized attention matrix [batch, heads, seq_len, seq_len] or None\n",
        "        \"\"\"\n",
        "        file_path = os.path.join(\n",
        "            self.attention_dir,\n",
        "            f\"row_{row_idx}\",\n",
        "            prompt_name,\n",
        "            f\"attn_layer_{layer_idx}.npy\"\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            attention = np.load(file_path)\n",
        "\n",
        "            if len(attention.shape) != 4:\n",
        "                self.validation_log.append(f\"Invalid shape {attention.shape} in {file_path}\")\n",
        "                return None\n",
        "\n",
        "            batch_size, num_heads, seq_len, key_len = attention.shape\n",
        "            normalized_attention = attention.copy()\n",
        "\n",
        "            # Normalize rows to ensure each query's attention sums to 1\n",
        "            normalization_fixes = 0\n",
        "            for b in range(batch_size):\n",
        "                for h in range(num_heads):\n",
        "                    for q in range(seq_len):\n",
        "                        attn_row = attention[b, h, q, :]\n",
        "                        row_sum = np.sum(attn_row)\n",
        "\n",
        "                        if not (0.99 <= row_sum <= 1.01):\n",
        "                            if row_sum > 0 and not np.isnan(row_sum):\n",
        "                                normalized_attention[b, h, q, :] = attn_row / row_sum\n",
        "                                normalization_fixes += 1\n",
        "                            else:\n",
        "                                normalized_attention[b, h, q, :] = np.ones(key_len) / key_len\n",
        "                                normalization_fixes += 1\n",
        "\n",
        "            return normalized_attention\n",
        "\n",
        "        except Exception as e:\n",
        "            self.validation_log.append(f\"Error processing {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def identify_hint_positions_individual(self, row_idx: int, prompt_col: str) -> Dict[str, List[int]]:\n",
        "        \"\"\"\n",
        "        Identify hint positions for each individual bias phrase + combined\n",
        "\n",
        "        Args:\n",
        "            row_idx: Row index in dataset\n",
        "            prompt_col: Prompt column name (Prompt_1, Prompt_2, Prompt_3)\n",
        "\n",
        "        Returns:\n",
        "            Dict mapping phrase -> list of token positions\n",
        "            Plus 'combined' key with all positions\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Access token metadata\n",
        "            if isinstance(self.token_metadata, list):\n",
        "                if row_idx < len(self.token_metadata):\n",
        "                    row_metadata = self.token_metadata[row_idx]\n",
        "                else:\n",
        "                    return {phrase: [] for phrase in self.analysis_keys}\n",
        "            else:\n",
        "                if str(row_idx) in self.token_metadata:\n",
        "                    row_metadata = self.token_metadata[str(row_idx)]\n",
        "                elif row_idx in self.token_metadata:\n",
        "                    row_metadata = self.token_metadata[row_idx]\n",
        "                else:\n",
        "                    return {phrase: [] for phrase in self.analysis_keys}\n",
        "\n",
        "            # Extract input tokens\n",
        "            if isinstance(row_metadata, dict) and prompt_col in row_metadata:\n",
        "                prompt_metadata = row_metadata[prompt_col]\n",
        "                input_tokens = prompt_metadata.get('input_tokens', [])\n",
        "            elif isinstance(row_metadata, dict) and 'input_tokens' in row_metadata:\n",
        "                input_tokens = row_metadata.get('input_tokens', [])\n",
        "            else:\n",
        "                return {phrase: [] for phrase in self.analysis_keys}\n",
        "\n",
        "            # Find positions for each individual phrase\n",
        "            phrase_positions = {}\n",
        "            all_combined_positions = []\n",
        "\n",
        "            for phrase in self.individual_bias_phrases:\n",
        "                phrase_specific_positions = []\n",
        "\n",
        "                for i, token in enumerate(input_tokens):\n",
        "                    clean_token = str(token).replace('Ġ', '').replace('▁', '').replace('##', '').lower().strip()\n",
        "\n",
        "                    if phrase in clean_token:\n",
        "                        phrase_specific_positions.append(i)\n",
        "                        all_combined_positions.append(i)\n",
        "\n",
        "                phrase_positions[phrase] = sorted(list(set(phrase_specific_positions)))\n",
        "\n",
        "            # Add combined positions\n",
        "            phrase_positions['combined'] = sorted(list(set(all_combined_positions)))\n",
        "\n",
        "            return phrase_positions\n",
        "\n",
        "        except (KeyError, TypeError, AttributeError, IndexError) as e:\n",
        "            self.validation_log.append(f\"Error finding individual hints for row {row_idx}, {prompt_col}: {e}\")\n",
        "            return {phrase: [] for phrase in self.analysis_keys}\n",
        "\n",
        "    def process_all_attention_data_individual(self, max_rows: int = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Process all attention data: load, normalize, and identify individual phrase positions\n",
        "\n",
        "        Args:\n",
        "            max_rows: Maximum number of rows to process (None for all)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with processing statistics\n",
        "        \"\"\"\n",
        "        print(\"Processing attention data with individual phrase analysis...\")\n",
        "        print(f\"Analyzing {len(self.individual_bias_phrases)} individual phrases + 1 combined analysis\")\n",
        "\n",
        "        if max_rows is None:\n",
        "            max_rows = len(self.results_df)\n",
        "\n",
        "        num_layers = self._detect_num_layers()\n",
        "        print(f\"Detected {num_layers} layers in the model\")\n",
        "\n",
        "        processing_stats = {\n",
        "            'total_rows_processed': 0,\n",
        "            'total_prompts_processed': 0,\n",
        "            'total_attention_matrices_loaded': 0,\n",
        "            'phrases_processed': {phrase: 0 for phrase in self.analysis_keys},\n",
        "            'rows_with_errors': 0\n",
        "        }\n",
        "\n",
        "        for row_idx in range(min(max_rows, len(self.results_df))):\n",
        "            row_has_errors = False\n",
        "            self.normalized_attention[row_idx] = {}\n",
        "\n",
        "            for prompt_col in ['Prompt_1', 'Prompt_2', 'Prompt_3']:\n",
        "                if pd.isna(self.results_df.iloc[row_idx][prompt_col]):\n",
        "                    continue\n",
        "\n",
        "                processing_stats['total_prompts_processed'] += 1\n",
        "                prompt_file_name = self.prompt_mapping[prompt_col]\n",
        "\n",
        "                # Get individual phrase positions\n",
        "                phrase_positions = self.identify_hint_positions_individual(row_idx, prompt_col)\n",
        "\n",
        "                # Store positions for each individual phrase + combined\n",
        "                for phrase, positions in phrase_positions.items():\n",
        "                    key = f\"{row_idx}_{prompt_col}\"\n",
        "                    self.hint_positions_individual[phrase][key] = positions\n",
        "\n",
        "                    if positions:\n",
        "                        processing_stats['phrases_processed'][phrase] += len(positions)\n",
        "\n",
        "                # Load and normalize attention for each layer\n",
        "                for layer_idx in range(num_layers):\n",
        "                    normalized_attn = self.load_and_normalize_attention(row_idx, prompt_file_name, layer_idx)\n",
        "\n",
        "                    if normalized_attn is not None:\n",
        "                        key = f\"{prompt_col}_layer_{layer_idx}\"\n",
        "                        self.normalized_attention[row_idx][key] = normalized_attn\n",
        "                        processing_stats['total_attention_matrices_loaded'] += 1\n",
        "                    else:\n",
        "                        row_has_errors = True\n",
        "\n",
        "            if row_has_errors:\n",
        "                processing_stats['rows_with_errors'] += 1\n",
        "\n",
        "            processing_stats['total_rows_processed'] += 1\n",
        "\n",
        "            if (row_idx + 1) % 50 == 0:\n",
        "                print(f\"Processed {row_idx + 1} rows...\")\n",
        "\n",
        "        print(f\"Processing complete: {processing_stats['total_rows_processed']} rows processed\")\n",
        "        print(\"Individual phrase statistics:\")\n",
        "        for phrase, count in processing_stats['phrases_processed'].items():\n",
        "            print(f\"  {phrase}: {count} positions found\")\n",
        "\n",
        "        return processing_stats\n",
        "\n",
        "    def _detect_num_layers(self) -> int:\n",
        "        \"\"\"Auto-detect number of layers by checking available files\"\"\"\n",
        "        for row_idx in range(min(5, len(self.results_df))):\n",
        "            for prompt_name in ['prompt_1', 'prompt_2', 'prompt_3']:\n",
        "                row_dir = os.path.join(self.attention_dir, f\"row_{row_idx}\", prompt_name)\n",
        "                if os.path.exists(row_dir):\n",
        "                    files = [f for f in os.listdir(row_dir) if f.startswith('attn_layer_') and f.endswith('.npy')]\n",
        "                    if files:\n",
        "                        layer_numbers = []\n",
        "                        for f in files:\n",
        "                            try:\n",
        "                                layer_num = int(f.replace('attn_layer_', '').replace('.npy', ''))\n",
        "                                layer_numbers.append(layer_num)\n",
        "                            except:\n",
        "                                continue\n",
        "                        if layer_numbers:\n",
        "                            return max(layer_numbers) + 1\n",
        "\n",
        "        return 16  # Default based on diagnostics\n",
        "\n",
        "    def save_results(self, output_dir: str = \"prepared_data_individual\"):\n",
        "        \"\"\"Save results for use in subsequent processing steps\"\"\"\n",
        "        print(f\"Saving individual phrase results to {output_dir}/\")\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save individual phrase positions\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(output_dir, f\"hint_positions_{phrase}.json\")\n",
        "            with open(phrase_file, 'w') as f:\n",
        "                json.dump(self.hint_positions_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Save validation log if exists\n",
        "        if self.validation_log:\n",
        "            log_file = os.path.join(output_dir, \"validation_log.txt\")\n",
        "            with open(log_file, 'w') as f:\n",
        "                for log_entry in self.validation_log:\n",
        "                    f.write(log_entry + \"\\n\")\n",
        "\n",
        "        # Save summary statistics\n",
        "        summary = {\n",
        "            'total_rows_in_dataset': len(self.results_df),\n",
        "            'rows_with_normalized_attention': len(self.normalized_attention),\n",
        "            'individual_phrases_analyzed': self.individual_bias_phrases,\n",
        "            'analysis_types': {\n",
        "                'individual_phrases': len(self.individual_bias_phrases),\n",
        "                'combined_analysis': 1,\n",
        "                'total_analyses': len(self.analysis_keys)\n",
        "            },\n",
        "            'phrase_position_counts': {\n",
        "                phrase: len(positions_dict)\n",
        "                for phrase, positions_dict in self.hint_positions_individual.items()\n",
        "            },\n",
        "            'total_validation_issues': len(self.validation_log),\n",
        "            'prompt_mapping': self.prompt_mapping\n",
        "        }\n",
        "\n",
        "        summary_file = os.path.join(output_dir, \"preparation_summary_individual.json\")\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase data preparation completed successfully!\")\n",
        "        print(f\"Generated {len(self.individual_bias_phrases)} individual + 1 combined = {len(self.analysis_keys)} total analyses\")\n",
        "        print(f\"Files generated:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"  - hint_positions_{phrase}.json\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_data_preparation_individual():\n",
        "    \"\"\"\n",
        "    Run the complete data preparation pipeline for individual phrases\n",
        "    \"\"\"\n",
        "    # Configuration - Update these paths for your setup\n",
        "    DATA_DIR = \"/content/Test_output_10rows/input_attention\"\n",
        "    RESULTS_CSV = \"/content/Test_output_10rows/results_with_predictions.csv\"\n",
        "    TOKEN_METADATA = \"/content/Test_output_10rows/input_attention_metadata.json\"\n",
        "\n",
        "    print(\"Starting Individual Phrase Data Preparation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize data preparation\n",
        "    processor = AttentionDataPreparationIndividual(\n",
        "        data_dir=DATA_DIR,\n",
        "        results_csv=RESULTS_CSV,\n",
        "        token_metadata_json=TOKEN_METADATA\n",
        "    )\n",
        "\n",
        "    # Validate data structure\n",
        "    validation_stats = processor.validate_data_structure()\n",
        "\n",
        "    # Process all data\n",
        "    processing_stats = processor.process_all_attention_data_individual()\n",
        "\n",
        "    # Save results for next stage\n",
        "    output_dir = processor.save_results()\n",
        "\n",
        "    print(f\"\\nIndividual phrase data preparation completed: {output_dir}\")\n",
        "    print(\"Ready for individual phrase entropy computation\")\n",
        "\n",
        "    return processor, processing_stats\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processor, stats = run_data_preparation_individual()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmeF1One2JDi",
        "outputId": "b44a3b73-513b-4bb3-a952-df9e1a1558b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Individual Phrase Data Preparation\n",
            "============================================================\n",
            "Loaded 10 rows from dataset\n",
            "Loaded token metadata for 10 rows\n",
            "Individual phrase analyses: teacher, own, sure, unsure, quick, fast, step\n",
            "Total analyses: 8 (6 individual + 1 combined)\n",
            "Validating data structure...\n",
            "Checked 10 rows\n",
            "Found 416 valid attention files\n",
            "Processing attention data with individual phrase analysis...\n",
            "Analyzing 7 individual phrases + 1 combined analysis\n",
            "Detected 16 layers in the model\n",
            "Processing complete: 10 rows processed\n",
            "Individual phrase statistics:\n",
            "  teacher: 6 positions found\n",
            "  own: 6 positions found\n",
            "  sure: 9 positions found\n",
            "  unsure: 3 positions found\n",
            "  quick: 3 positions found\n",
            "  fast: 0 positions found\n",
            "  step: 6 positions found\n",
            "  combined: 30 positions found\n",
            "Saving individual phrase results to prepared_data_individual/\n",
            "Individual phrase data preparation completed successfully!\n",
            "Generated 7 individual + 1 combined = 8 total analyses\n",
            "Files generated:\n",
            "  - hint_positions_teacher.json\n",
            "  - hint_positions_own.json\n",
            "  - hint_positions_sure.json\n",
            "  - hint_positions_unsure.json\n",
            "  - hint_positions_quick.json\n",
            "  - hint_positions_fast.json\n",
            "  - hint_positions_step.json\n",
            "  - hint_positions_combined.json\n",
            "\n",
            "Individual phrase data preparation completed: prepared_data_individual\n",
            "Ready for individual phrase entropy computation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from scipy.stats import entropy\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "class EntropyComputationIndividual:\n",
        "    def __init__(self, prepared_data_dir: str, data_dir: str):\n",
        "        \"\"\"\n",
        "        Compute Entropy: Granular computation per query/head/layer for individual phrases (6 + 1)\n",
        "\n",
        "        Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\n",
        "\n",
        "        Args:\n",
        "            prepared_data_dir: Directory containing individual phrase hint_positions files\n",
        "            data_dir: Base directory containing attention files\n",
        "        \"\"\"\n",
        "        self.prepared_data_dir = prepared_data_dir\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "\n",
        "        # Load individual phrase positions from data preparation stage\n",
        "        self.hint_positions_individual = {}\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(prepared_data_dir, f\"hint_positions_{phrase}.json\")\n",
        "            if os.path.exists(phrase_file):\n",
        "                with open(phrase_file, 'r') as f:\n",
        "                    self.hint_positions_individual[phrase] = json.load(f)\n",
        "                print(f\"Loaded hint positions for '{phrase}': {len(self.hint_positions_individual[phrase])} prompt instances\")\n",
        "            else:\n",
        "                print(f\"Warning: {phrase_file} not found\")\n",
        "                self.hint_positions_individual[phrase] = {}\n",
        "\n",
        "        print(f\"Loaded hint positions for {len(self.hint_positions_individual)} individual phrase analyses\")\n",
        "\n",
        "        # Storage for entropy results\n",
        "        self.entropies = {}  # {row_prompt: entropies[ℓ][h][i] array} - same for all analyses\n",
        "        self.hint_entropies_individual = {}  # {phrase: {row_prompt: [entropies[ℓ][h][pos] for pos in phrase_positions]}}\n",
        "\n",
        "        # Initialize individual phrase storage\n",
        "        for phrase in self.analysis_keys:\n",
        "            self.hint_entropies_individual[phrase] = {}\n",
        "\n",
        "    def compute_attention_entropy(self, attention_matrix: np.ndarray, hint_positions: List[int] = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Compute entropy for attention matrix using the specified formula\n",
        "\n",
        "        Args:\n",
        "            attention_matrix: Shape [batch, heads, seq_len, seq_len]\n",
        "            hint_positions: List of hint token positions for subsetting\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with entropies and hint_entropies\n",
        "        \"\"\"\n",
        "        if len(attention_matrix.shape) != 4:\n",
        "            return None\n",
        "\n",
        "        batch_size, num_heads, seq_len, key_len = attention_matrix.shape\n",
        "\n",
        "        # Store entropies in [heads, seq_len] format\n",
        "        entropies = np.zeros((num_heads, seq_len))\n",
        "\n",
        "        # Loop over heads, then queries - exact methodology\n",
        "        for h in range(num_heads):\n",
        "            for i in range(seq_len):  # query token i\n",
        "                # Get attention row for query i in head h\n",
        "                attn_row = attention_matrix[0, h, i, :]  # Assuming batch_size=1\n",
        "\n",
        "                # Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\n",
        "                ent = entropy(attn_row, base=2)\n",
        "\n",
        "                # Handle edge cases\n",
        "                if np.isnan(ent) or np.isinf(ent):\n",
        "                    ent = 0.0\n",
        "\n",
        "                entropies[h, i] = ent\n",
        "\n",
        "        # Subset for hints\n",
        "        hint_entropies = []\n",
        "        if hint_positions:\n",
        "            for h in range(num_heads):\n",
        "                for pos in hint_positions:\n",
        "                    if pos < seq_len:\n",
        "                        hint_entropies.append(entropies[h, pos])\n",
        "\n",
        "        return {\n",
        "            'entropies': entropies,\n",
        "            'hint_entropies': hint_entropies,\n",
        "            'num_heads': num_heads,\n",
        "            'seq_len': seq_len,\n",
        "            'hint_positions': hint_positions or []\n",
        "        }\n",
        "\n",
        "    def compute_entropy_for_dataset_individual(self, max_rows: int = None):\n",
        "        \"\"\"\n",
        "        Process entropy computation for the entire dataset with individual phrase analysis\n",
        "\n",
        "        Args:\n",
        "            max_rows: Maximum number of rows to process (None for all)\n",
        "        \"\"\"\n",
        "        print(\"Computing entropy for dataset with individual phrase analysis...\")\n",
        "        print(\"Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\")\n",
        "        print(f\"Analyzing: {', '.join(self.analysis_keys)}\")\n",
        "\n",
        "        # Get available rows\n",
        "        available_rows = []\n",
        "        for item in os.listdir(self.data_dir):\n",
        "            if item.startswith('row_') and os.path.isdir(os.path.join(self.data_dir, item)):\n",
        "                try:\n",
        "                    row_num = int(item.replace('row_', ''))\n",
        "                    available_rows.append(row_num)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        available_rows = sorted(available_rows)\n",
        "        if max_rows:\n",
        "            available_rows = available_rows[:max_rows]\n",
        "\n",
        "        print(f\"Processing {len(available_rows)} rows\")\n",
        "\n",
        "        total_entropy_values = 0\n",
        "        total_matrices = 0\n",
        "\n",
        "        # Process each row\n",
        "        for row_idx in available_rows:\n",
        "            # Process each prompt\n",
        "            for prompt_name in ['prompt_1', 'prompt_2', 'prompt_3']:\n",
        "                prompt_dir = os.path.join(self.data_dir, f\"row_{row_idx}\", prompt_name)\n",
        "                if not os.path.exists(prompt_dir):\n",
        "                    continue\n",
        "\n",
        "                # Get individual phrase hint positions\n",
        "                row_prompt_key = f\"row_{row_idx}_{prompt_name}\"\n",
        "                phrase_hint_positions = {}\n",
        "\n",
        "                for phrase in self.analysis_keys:\n",
        "                    key = f\"{row_idx}_Prompt_{prompt_name.split('_')[1]}\"\n",
        "                    phrase_hint_positions[phrase] = self.hint_positions_individual[phrase].get(key, [])\n",
        "\n",
        "                # Get available layers\n",
        "                layer_files = [f for f in os.listdir(prompt_dir) if f.startswith('attn_layer_') and f.endswith('.npy')]\n",
        "                layer_indices = sorted([int(f.replace('attn_layer_', '').replace('.npy', '')) for f in layer_files])\n",
        "\n",
        "                if not layer_indices:\n",
        "                    continue\n",
        "\n",
        "                # Load first matrix to get dimensions\n",
        "                first_file = os.path.join(prompt_dir, f\"attn_layer_{layer_indices[0]}.npy\")\n",
        "                sample_matrix = np.load(first_file)\n",
        "                _, num_heads, seq_len, _ = sample_matrix.shape\n",
        "                num_layers = len(layer_indices)\n",
        "\n",
        "                # Initialize entropies[ℓ][h][i] array for this row_prompt\n",
        "                entropies = np.zeros((num_layers, num_heads, seq_len))\n",
        "\n",
        "                # Process each layer ℓ\n",
        "                for layer_position, layer_idx in enumerate(layer_indices):\n",
        "                    file_path = os.path.join(prompt_dir, f\"attn_layer_{layer_idx}.npy\")\n",
        "\n",
        "                    try:\n",
        "                        attention_matrix = np.load(file_path)\n",
        "\n",
        "                        # Compute entropy\n",
        "                        entropy_result = self.compute_attention_entropy(attention_matrix)\n",
        "\n",
        "                        if entropy_result:\n",
        "                            layer_entropies = entropy_result['entropies']  # [heads, seq_len]\n",
        "\n",
        "                            # Store in entropies[ℓ][h][i] format\n",
        "                            entropies[layer_position, :, :] = layer_entropies\n",
        "\n",
        "                            total_matrices += 1\n",
        "                            total_entropy_values += layer_entropies.size\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing layer {layer_idx}: {e}\")\n",
        "\n",
        "                # Store the complete entropies[ℓ][h][i] array (same for all analyses)\n",
        "                self.entropies[row_prompt_key] = entropies\n",
        "\n",
        "                # Extract individual phrase-specific entropies\n",
        "                for phrase, hint_positions in phrase_hint_positions.items():\n",
        "                    phrase_entropy_values = []\n",
        "\n",
        "                    if hint_positions:\n",
        "                        for layer_pos in range(num_layers):\n",
        "                            for head_idx in range(num_heads):\n",
        "                                for pos in hint_positions:\n",
        "                                    if pos < seq_len:\n",
        "                                        phrase_entropy_values.append(entropies[layer_pos, head_idx, pos])\n",
        "\n",
        "                    self.hint_entropies_individual[phrase][row_prompt_key] = phrase_entropy_values\n",
        "\n",
        "            if (row_idx + 1) % 25 == 0:\n",
        "                print(f\"Processed {row_idx + 1} rows...\")\n",
        "\n",
        "        print(f\"Individual phrase entropy computation complete!\")\n",
        "        print(f\"Processed {total_matrices} attention matrices\")\n",
        "        print(f\"Computed {total_entropy_values} entropy values\")\n",
        "\n",
        "        # Print summary for each phrase\n",
        "        print(\"\\nPhrase-specific entropy summary:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            total_values = sum(len(values) for values in self.hint_entropies_individual[phrase].values())\n",
        "            non_empty_instances = sum(1 for values in self.hint_entropies_individual[phrase].values() if len(values) > 0)\n",
        "            print(f\"  {phrase}: {total_values} entropy values, {non_empty_instances} instances with data\")\n",
        "\n",
        "        return {\n",
        "            'total_matrices': total_matrices,\n",
        "            'total_entropy_values': total_entropy_values,\n",
        "            'rows_processed': len(available_rows),\n",
        "            'phrase_analyses': len(self.analysis_keys)\n",
        "        }\n",
        "\n",
        "    def save_results(self, output_dir: str = \"entropy_results_individual\"):\n",
        "        \"\"\"\n",
        "        Save individual phrase entropy computation results\n",
        "\n",
        "        Output format: entropies[ℓ][h][i] arrays with individual phrase subsets\n",
        "        \"\"\"\n",
        "        print(f\"Saving individual phrase entropy results to {output_dir}/\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Convert numpy arrays to lists for JSON serialization\n",
        "        entropies_for_json = {}\n",
        "        for key, entropy_array in self.entropies.items():\n",
        "            entropies_for_json[key] = entropy_array.tolist()\n",
        "\n",
        "        # Save complete entropies[ℓ][h][i] arrays (same for all analyses)\n",
        "        entropy_file = os.path.join(output_dir, \"entropies_arrays.json\")\n",
        "        with open(entropy_file, 'w') as f:\n",
        "            json.dump(entropies_for_json, f, indent=2)\n",
        "\n",
        "        # Save individual phrase hint entropies\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(output_dir, f\"hint_entropies_{phrase}.json\")\n",
        "            with open(phrase_file, 'w') as f:\n",
        "                json.dump(self.hint_entropies_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Save individual phrase hint positions for reference\n",
        "        for phrase in self.analysis_keys:\n",
        "            positions_file = os.path.join(output_dir, f\"hint_positions_{phrase}.json\")\n",
        "            with open(positions_file, 'w') as f:\n",
        "                json.dump(self.hint_positions_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Create comprehensive summary\n",
        "        summary = {\n",
        "            'methodology': 'H_i^(ℓ,h) = -∑_j A_ij log(A_ij)',\n",
        "            'analysis_structure': {\n",
        "                'individual_phrases': [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"],\n",
        "                'combined_analysis': \"combined (all phrases together)\",\n",
        "                'total_analyses': len(self.analysis_keys)\n",
        "            },\n",
        "            'storage_format': {\n",
        "                'entropies': 'entropies[ℓ][h][i] arrays (layer, head, query)',\n",
        "                'hint_entropies': 'phrase-specific: [entropies[ℓ][h][pos] for pos in phrase_positions]'\n",
        "            },\n",
        "            'phrase_statistics': {},\n",
        "            'total_prompt_instances': len(self.entropies),\n",
        "            'arrays_saved': list(self.entropies.keys())\n",
        "        }\n",
        "\n",
        "        # Add phrase statistics\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_data = self.hint_entropies_individual[phrase]\n",
        "            total_entropy_values = sum(len(values) for values in phrase_data.values())\n",
        "            non_empty_instances = sum(1 for values in phrase_data.values() if len(values) > 0)\n",
        "\n",
        "            summary['phrase_statistics'][phrase] = {\n",
        "                'total_entropy_values': total_entropy_values,\n",
        "                'prompt_instances_with_phrase': non_empty_instances,\n",
        "                'total_prompt_instances': len(phrase_data),\n",
        "                'coverage_percentage': (non_empty_instances / len(phrase_data)) * 100 if len(phrase_data) > 0 else 0\n",
        "            }\n",
        "\n",
        "        summary_file = os.path.join(output_dir, \"entropy_summary_individual.json\")\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase entropy computation completed successfully!\")\n",
        "        print(f\"Generated analyses for: {', '.join(self.analysis_keys)}\")\n",
        "        print(\"Files generated:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"  - hint_entropies_{phrase}.json\")\n",
        "            print(f\"  - hint_positions_{phrase}.json\")\n",
        "        print(\"Ready for individual phrase aggregation analysis\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_entropy_computation_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase entropy computation pipeline\n",
        "    \"\"\"\n",
        "    # Configuration - Update these paths for your setup\n",
        "    PREPARED_DATA_DIR = \"prepared_data_individual\"  # Contains individual phrase hint_positions files\n",
        "    DATA_DIR = \"/content/Test_output_10rows/input_attention\"  # Contains row_X/prompt_Y/attn_layer_Z.npy\n",
        "\n",
        "    print(\"Starting Individual Phrase Entropy Computation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize entropy computation\n",
        "    entropy_computer = EntropyComputationIndividual(\n",
        "        prepared_data_dir=PREPARED_DATA_DIR,\n",
        "        data_dir=DATA_DIR\n",
        "    )\n",
        "\n",
        "    # Compute entropy for dataset\n",
        "    stats = entropy_computer.compute_entropy_for_dataset_individual()\n",
        "\n",
        "    # Save results for next stage\n",
        "    output_dir = entropy_computer.save_results()\n",
        "\n",
        "    print(f\"\\nIndividual phrase entropy computation completed: {output_dir}\")\n",
        "    print(\"Ready for individual phrase aggregation analysis\")\n",
        "\n",
        "    return entropy_computer, stats\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    entropy_computer, stats = run_entropy_computation_individual()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u25vyFOT2sgr",
        "outputId": "d68f53a4-4a74-41a7-f784-2836854b1d6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Individual Phrase Entropy Computation\n",
            "============================================================\n",
            "Loaded hint positions for 'teacher': 26 prompt instances\n",
            "Loaded hint positions for 'own': 26 prompt instances\n",
            "Loaded hint positions for 'sure': 26 prompt instances\n",
            "Loaded hint positions for 'unsure': 26 prompt instances\n",
            "Loaded hint positions for 'quick': 26 prompt instances\n",
            "Loaded hint positions for 'fast': 26 prompt instances\n",
            "Loaded hint positions for 'step': 26 prompt instances\n",
            "Loaded hint positions for 'combined': 26 prompt instances\n",
            "Loaded hint positions for 8 individual phrase analyses\n",
            "Computing entropy for dataset with individual phrase analysis...\n",
            "Formula: H_i^(ℓ,h) = -∑_j A_ij log(A_ij)\n",
            "Analyzing: teacher, own, sure, unsure, quick, fast, step, combined\n",
            "Processing 10 rows\n",
            "Individual phrase entropy computation complete!\n",
            "Processed 416 attention matrices\n",
            "Computed 861696 entropy values\n",
            "\n",
            "Phrase-specific entropy summary:\n",
            "  teacher: 3072 entropy values, 6 instances with data\n",
            "  own: 3072 entropy values, 6 instances with data\n",
            "  sure: 4608 entropy values, 9 instances with data\n",
            "  unsure: 1536 entropy values, 3 instances with data\n",
            "  quick: 1536 entropy values, 3 instances with data\n",
            "  fast: 0 entropy values, 0 instances with data\n",
            "  step: 3072 entropy values, 3 instances with data\n",
            "  combined: 15360 entropy values, 24 instances with data\n",
            "Saving individual phrase entropy results to entropy_results_individual/\n",
            "Individual phrase entropy computation completed successfully!\n",
            "Generated analyses for: teacher, own, sure, unsure, quick, fast, step, combined\n",
            "Files generated:\n",
            "  - hint_entropies_teacher.json\n",
            "  - hint_positions_teacher.json\n",
            "  - hint_entropies_own.json\n",
            "  - hint_positions_own.json\n",
            "  - hint_entropies_sure.json\n",
            "  - hint_positions_sure.json\n",
            "  - hint_entropies_unsure.json\n",
            "  - hint_positions_unsure.json\n",
            "  - hint_entropies_quick.json\n",
            "  - hint_positions_quick.json\n",
            "  - hint_entropies_fast.json\n",
            "  - hint_positions_fast.json\n",
            "  - hint_entropies_step.json\n",
            "  - hint_positions_step.json\n",
            "  - hint_entropies_combined.json\n",
            "  - hint_positions_combined.json\n",
            "Ready for individual phrase aggregation analysis\n",
            "\n",
            "Individual phrase entropy computation completed: entropy_results_individual\n",
            "Ready for individual phrase aggregation analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class AggregationAnalysisIndividual:\n",
        "    def __init__(self, entropy_results_dir: str):\n",
        "        \"\"\"\n",
        "        Aggregate and Compute Changes for Individual Phrases (6 + 1)\n",
        "\n",
        "        Methodology:\n",
        "        1. Per phrase: H_phrase = (1/N) ∑_positions H_i^(ℓ,h)\n",
        "        2. Changes (ΔH): ΔH = H_baseline - H_variant per phrase\n",
        "        3. Cross-phrase comparison: Compare bias effects across individual phrases\n",
        "\n",
        "        Args:\n",
        "            entropy_results_dir: Directory containing individual phrase entropy computation results\n",
        "        \"\"\"\n",
        "        self.entropy_results_dir = entropy_results_dir\n",
        "\n",
        "        print(\"Starting Individual Phrase Aggregation Analysis\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "\n",
        "        # Load individual phrase entropy data\n",
        "        self.load_entropy_data()\n",
        "\n",
        "        # Storage for aggregated results\n",
        "        self.aggregated_entropies_individual = {}  # {phrase: {row_prompt: value}}\n",
        "        self.delta_h_changes_individual = {}       # {phrase: {row_idx: {comparison: delta_h}}}\n",
        "\n",
        "        # Initialize storage for each phrase\n",
        "        for phrase in self.analysis_keys:\n",
        "            self.aggregated_entropies_individual[phrase] = {}\n",
        "            self.delta_h_changes_individual[phrase] = {}\n",
        "\n",
        "    def load_entropy_data(self):\n",
        "        \"\"\"Load individual phrase entropy data from computation stage\"\"\"\n",
        "        self.hint_entropies_individual = {}\n",
        "        self.hint_positions_individual = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Load hint entropies\n",
        "            entropy_file = os.path.join(self.entropy_results_dir, f\"hint_entropies_{phrase}.json\")\n",
        "            if os.path.exists(entropy_file):\n",
        "                with open(entropy_file, 'r') as f:\n",
        "                    self.hint_entropies_individual[phrase] = json.load(f)\n",
        "                print(f\"Loaded entropies for '{phrase}': {len(self.hint_entropies_individual[phrase])} prompt instances\")\n",
        "            else:\n",
        "                print(f\"Warning: {entropy_file} not found\")\n",
        "                self.hint_entropies_individual[phrase] = {}\n",
        "\n",
        "            # Load hint positions for reference\n",
        "            positions_file = os.path.join(self.entropy_results_dir, f\"hint_positions_{phrase}.json\")\n",
        "            if os.path.exists(positions_file):\n",
        "                with open(positions_file, 'r') as f:\n",
        "                    self.hint_positions_individual[phrase] = json.load(f)\n",
        "\n",
        "        print(f\"Loaded individual phrase data for {len(self.hint_entropies_individual)} analyses\")\n",
        "\n",
        "    def compute_individual_phrase_aggregations(self):\n",
        "        \"\"\"\n",
        "        Compute aggregations for each individual phrase (6 + 1)\n",
        "        H_phrase = (1/N) ∑_positions H_i^(ℓ,h) where N = number of phrase positions\n",
        "        \"\"\"\n",
        "        print(\"Computing individual phrase aggregations...\")\n",
        "        print(f\"Analyzing: {', '.join(self.analysis_keys)}\")\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"Processing {phrase} aggregations...\")\n",
        "\n",
        "            # Phrase-specific aggregation\n",
        "            for row_prompt_key, entropy_values in self.hint_entropies_individual[phrase].items():\n",
        "                if entropy_values:\n",
        "                    # H_phrase = average entropy at phrase positions\n",
        "                    avg_entropy = np.mean(entropy_values)\n",
        "                    self.aggregated_entropies_individual[phrase][row_prompt_key] = float(avg_entropy)\n",
        "                else:\n",
        "                    self.aggregated_entropies_individual[phrase][row_prompt_key] = None\n",
        "\n",
        "            # Print statistics for this phrase\n",
        "            valid_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            if valid_values:\n",
        "                print(f\"  {phrase}: {len(valid_values)} valid instances, avg entropy = {np.mean(valid_values):.4f}\")\n",
        "            else:\n",
        "                print(f\"  {phrase}: No valid instances found\")\n",
        "\n",
        "    def compute_individual_phrase_delta_h(self):\n",
        "        \"\"\"\n",
        "        Compute ΔH changes for each individual phrase (6 + 1)\n",
        "        ΔH = H_baseline - H_variant per phrase\n",
        "        \"\"\"\n",
        "        print(\"Computing individual phrase ΔH changes...\")\n",
        "\n",
        "        phrase_delta_h_stats = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"Computing ΔH for {phrase}...\")\n",
        "\n",
        "            # Group data by row\n",
        "            rows_data = defaultdict(dict)\n",
        "            for row_prompt_key, entropy_value in self.aggregated_entropies_individual[phrase].items():\n",
        "                parts = row_prompt_key.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    row_idx = int(parts[1])\n",
        "                    prompt_type = f\"{parts[2]}_{parts[3]}\"\n",
        "                    rows_data[row_idx][prompt_type] = entropy_value\n",
        "\n",
        "            # Compute ΔH for each row\n",
        "            phrase_delta_h_values = []\n",
        "\n",
        "            for row_idx, row_data in rows_data.items():\n",
        "                row_changes = {}\n",
        "\n",
        "                # Prompt_2 vs Prompt_1 comparison\n",
        "                if 'prompt_1' in row_data and 'prompt_2' in row_data:\n",
        "                    if row_data['prompt_1'] is not None and row_data['prompt_2'] is not None:\n",
        "                        delta_h = row_data['prompt_1'] - row_data['prompt_2']\n",
        "                        row_changes['prompt2_vs_prompt1'] = delta_h\n",
        "                        phrase_delta_h_values.append(delta_h)\n",
        "\n",
        "                # Prompt_3 vs Prompt_1 comparison\n",
        "                if 'prompt_1' in row_data and 'prompt_3' in row_data:\n",
        "                    if row_data['prompt_1'] is not None and row_data['prompt_3'] is not None:\n",
        "                        delta_h = row_data['prompt_1'] - row_data['prompt_3']\n",
        "                        row_changes['prompt3_vs_prompt1'] = delta_h\n",
        "                        phrase_delta_h_values.append(delta_h)\n",
        "\n",
        "                if row_changes:\n",
        "                    self.delta_h_changes_individual[phrase][row_idx] = row_changes\n",
        "\n",
        "            # Calculate statistics for this phrase\n",
        "            if phrase_delta_h_values:\n",
        "                phrase_delta_h_stats[phrase] = {\n",
        "                    'mean': float(np.mean(phrase_delta_h_values)),\n",
        "                    'std': float(np.std(phrase_delta_h_values)),\n",
        "                    'negative_percentage': (sum(1 for x in phrase_delta_h_values if x < 0) / len(phrase_delta_h_values)) * 100,\n",
        "                    'count': len(phrase_delta_h_values)\n",
        "                }\n",
        "\n",
        "        # Print ΔH summary for each phrase\n",
        "        print(\"\\nΔH Summary by Individual Phrase:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            if phrase in phrase_delta_h_stats:\n",
        "                stats = phrase_delta_h_stats[phrase]\n",
        "                print(f\"  {phrase}: Mean ΔH = {stats['mean']:.4f}, Negative% = {stats['negative_percentage']:.1f}%, Count = {stats['count']}\")\n",
        "            else:\n",
        "                print(f\"  {phrase}: No valid ΔH values\")\n",
        "\n",
        "        return phrase_delta_h_stats\n",
        "\n",
        "    def compute_cross_phrase_comparison(self):\n",
        "        \"\"\"\n",
        "        Compare bias effects across individual phrases to identify most problematic bias types\n",
        "        \"\"\"\n",
        "        print(\"Computing cross-phrase comparison...\")\n",
        "\n",
        "        comparison_results = {}\n",
        "\n",
        "        # Collect aggregated entropy for each phrase\n",
        "        phrase_entropies = {}\n",
        "        phrase_delta_h = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Collect entropy values\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            if entropy_values:\n",
        "                phrase_entropies[phrase] = {\n",
        "                    'mean': float(np.mean(entropy_values)),\n",
        "                    'std': float(np.std(entropy_values)),\n",
        "                    'count': len(entropy_values)\n",
        "                }\n",
        "\n",
        "            # Collect ΔH values\n",
        "            all_delta_h = []\n",
        "            for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                all_delta_h.extend(row_changes.values())\n",
        "\n",
        "            if all_delta_h:\n",
        "                phrase_delta_h[phrase] = {\n",
        "                    'mean': float(np.mean(all_delta_h)),\n",
        "                    'std': float(np.std(all_delta_h)),\n",
        "                    'negative_percentage': (sum(1 for x in all_delta_h if x < 0) / len(all_delta_h)) * 100,\n",
        "                    'abs_mean': float(np.mean(np.abs(all_delta_h))),\n",
        "                    'count': len(all_delta_h)\n",
        "                }\n",
        "\n",
        "        comparison_results = {\n",
        "            'phrase_entropy_comparison': phrase_entropies,\n",
        "            'phrase_delta_h_comparison': phrase_delta_h,\n",
        "            'ranking_by_bias_strength': {},\n",
        "            'cross_phrase_insights': {}\n",
        "        }\n",
        "\n",
        "        # Rank phrases by bias strength (absolute mean ΔH)\n",
        "        if phrase_delta_h:\n",
        "            sorted_by_bias = sorted(phrase_delta_h.items(), key=lambda x: x[1]['abs_mean'], reverse=True)\n",
        "            comparison_results['ranking_by_bias_strength'] = {\n",
        "                'most_biased_phrases': [phrase for phrase, _ in sorted_by_bias[:3]],\n",
        "                'least_biased_phrases': [phrase for phrase, _ in sorted_by_bias[-3:]],\n",
        "                'full_ranking': [(phrase, stats['abs_mean']) for phrase, stats in sorted_by_bias]\n",
        "            }\n",
        "\n",
        "        # Generate insights\n",
        "        if phrase_delta_h:\n",
        "            # Find phrases with strongest negative bias (most unfaithful)\n",
        "            negative_bias_phrases = [\n",
        "                phrase for phrase, stats in phrase_delta_h.items()\n",
        "                if stats['negative_percentage'] > 70 and stats['mean'] < -0.001\n",
        "            ]\n",
        "\n",
        "            # Find phrases with positive bias (more faithful)\n",
        "            positive_bias_phrases = [\n",
        "                phrase for phrase, stats in phrase_delta_h.items()\n",
        "                if stats['negative_percentage'] < 30 and stats['mean'] > 0.001\n",
        "            ]\n",
        "\n",
        "            comparison_results['cross_phrase_insights'] = {\n",
        "                'strongly_negative_bias_phrases': negative_bias_phrases,\n",
        "                'positive_bias_phrases': positive_bias_phrases,\n",
        "                'interpretation': {\n",
        "                    'negative_bias': 'These phrases cause more focused/unfaithful attention',\n",
        "                    'positive_bias': 'These phrases cause more distributed/faithful attention'\n",
        "                }\n",
        "            }\n",
        "\n",
        "        print(\"\\nCross-phrase comparison completed!\")\n",
        "        if 'ranking_by_bias_strength' in comparison_results:\n",
        "            print(\"Phrases ranked by bias strength (most biased first):\")\n",
        "            for i, (phrase, bias_strength) in enumerate(comparison_results['ranking_by_bias_strength']['full_ranking'][:5], 1):\n",
        "                print(f\"  {i}. {phrase}: {bias_strength:.4f}\")\n",
        "\n",
        "        return comparison_results\n",
        "\n",
        "    def save_results(self, output_dir: str = \"aggregated_analysis_individual\"):\n",
        "        \"\"\"Save all individual phrase aggregation and change results\"\"\"\n",
        "        print(f\"Saving individual phrase aggregation results to {output_dir}/\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save individual phrase aggregated entropies\n",
        "        for phrase in self.analysis_keys:\n",
        "            phrase_file = os.path.join(output_dir, f\"aggregated_entropies_{phrase}.json\")\n",
        "            with open(phrase_file, 'w') as f:\n",
        "                json.dump(self.aggregated_entropies_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Save individual phrase ΔH changes\n",
        "        for phrase in self.analysis_keys:\n",
        "            delta_file = os.path.join(output_dir, f\"delta_h_changes_{phrase}.json\")\n",
        "            with open(delta_file, 'w') as f:\n",
        "                json.dump(self.delta_h_changes_individual[phrase], f, indent=2)\n",
        "\n",
        "        # Compute and save ΔH statistics\n",
        "        phrase_delta_h_stats = self.compute_individual_phrase_delta_h()\n",
        "        stats_file = os.path.join(output_dir, \"phrase_delta_h_statistics.json\")\n",
        "        with open(stats_file, 'w') as f:\n",
        "            json.dump(phrase_delta_h_stats, f, indent=2)\n",
        "\n",
        "        # Compute and save cross-phrase comparison\n",
        "        cross_phrase_comparison = self.compute_cross_phrase_comparison()\n",
        "        comparison_file = os.path.join(output_dir, \"cross_phrase_comparison.json\")\n",
        "        with open(comparison_file, 'w') as f:\n",
        "            json.dump(cross_phrase_comparison, f, indent=2)\n",
        "\n",
        "        # Create comprehensive summary\n",
        "        summary = {\n",
        "            'analysis_structure': {\n",
        "                'individual_phrases': [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"],\n",
        "                'combined_analysis': \"combined\",\n",
        "                'total_analyses': len(self.analysis_keys)\n",
        "            },\n",
        "            'aggregation_methodology': {\n",
        "                'phrase_aggregation': 'H_phrase = (1/N) ∑_positions H_i^(ℓ,h)',\n",
        "                'change_calculation': 'ΔH = H_baseline - H_variant per phrase'\n",
        "            },\n",
        "            'interpretation': {\n",
        "                'negative_delta_h': 'Variants more focused/sharper at this phrase (unfaithful)',\n",
        "                'positive_delta_h': 'Variants less focused at this phrase (more faithful)',\n",
        "                'cross_phrase_comparison': 'Identifies which bias phrases are most problematic'\n",
        "            },\n",
        "            'data_counts': {\n",
        "                'analyses_completed': len(self.analysis_keys),\n",
        "                'aggregation_files': len(self.analysis_keys),\n",
        "                'delta_h_files': len(self.analysis_keys),\n",
        "                'comparison_analysis_available': True\n",
        "            },\n",
        "            'files_generated': {\n",
        "                'per_phrase_aggregated_entropies': [f\"aggregated_entropies_{phrase}.json\" for phrase in self.analysis_keys],\n",
        "                'per_phrase_delta_h_changes': [f\"delta_h_changes_{phrase}.json\" for phrase in self.analysis_keys],\n",
        "                'summary_files': ['phrase_delta_h_statistics.json', 'cross_phrase_comparison.json']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        summary_file = os.path.join(output_dir, \"aggregation_summary_individual.json\")\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase aggregation analysis completed successfully!\")\n",
        "        print(f\"Generated {len(self.analysis_keys)} separate phrase analyses:\")\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"  - {phrase}: aggregated_entropies_{phrase}.json, delta_h_changes_{phrase}.json\")\n",
        "        print(\"Additional files:\")\n",
        "        print(\"  - phrase_delta_h_statistics.json\")\n",
        "        print(\"  - cross_phrase_comparison.json\")\n",
        "        print(\"Ready for individual phrase statistical analysis\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_aggregation_analysis_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase aggregation analysis pipeline\n",
        "    \"\"\"\n",
        "    # Configuration - Update this path for your setup\n",
        "    ENTROPY_RESULTS_DIR = \"entropy_results_individual\"  # Contains individual phrase entropy files\n",
        "\n",
        "    print(\"Starting Individual Phrase Aggregation Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize aggregation analysis\n",
        "    processor = AggregationAnalysisIndividual(\n",
        "        entropy_results_dir=ENTROPY_RESULTS_DIR\n",
        "    )\n",
        "\n",
        "    # Compute individual phrase aggregations\n",
        "    processor.compute_individual_phrase_aggregations()\n",
        "\n",
        "    # Compute ΔH changes for each phrase\n",
        "    delta_h_stats = processor.compute_individual_phrase_delta_h()\n",
        "\n",
        "    # Compute cross-phrase comparison\n",
        "    cross_phrase_results = processor.compute_cross_phrase_comparison()\n",
        "\n",
        "    # Save all results\n",
        "    output_dir = processor.save_results()\n",
        "\n",
        "    print(f\"\\nIndividual phrase aggregation analysis completed: {output_dir}\")\n",
        "\n",
        "    return processor, delta_h_stats, cross_phrase_results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processor, delta_h_stats, cross_phrase_results = run_aggregation_analysis_individual()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qgsi0FBj2usI",
        "outputId": "0e746a6d-fa17-40cf-c08a-f89f92c5fb35"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Individual Phrase Aggregation Analysis\n",
            "============================================================\n",
            "Starting Individual Phrase Aggregation Analysis\n",
            "============================================================\n",
            "Loaded entropies for 'teacher': 26 prompt instances\n",
            "Loaded entropies for 'own': 26 prompt instances\n",
            "Loaded entropies for 'sure': 26 prompt instances\n",
            "Loaded entropies for 'unsure': 26 prompt instances\n",
            "Loaded entropies for 'quick': 26 prompt instances\n",
            "Loaded entropies for 'fast': 26 prompt instances\n",
            "Loaded entropies for 'step': 26 prompt instances\n",
            "Loaded entropies for 'combined': 26 prompt instances\n",
            "Loaded individual phrase data for 8 analyses\n",
            "Computing individual phrase aggregations...\n",
            "Analyzing: teacher, own, sure, unsure, quick, fast, step, combined\n",
            "Processing teacher aggregations...\n",
            "  teacher: 6 valid instances, avg entropy = 1.4088\n",
            "Processing own aggregations...\n",
            "  own: 6 valid instances, avg entropy = 1.6225\n",
            "Processing sure aggregations...\n",
            "  sure: 9 valid instances, avg entropy = 1.7823\n",
            "Processing unsure aggregations...\n",
            "  unsure: 3 valid instances, avg entropy = 1.6108\n",
            "Processing quick aggregations...\n",
            "  quick: 3 valid instances, avg entropy = 1.8531\n",
            "Processing fast aggregations...\n",
            "  fast: No valid instances found\n",
            "Processing step aggregations...\n",
            "  step: 3 valid instances, avg entropy = 2.1284\n",
            "Processing combined aggregations...\n",
            "  combined: 24 valid instances, avg entropy = 1.6633\n",
            "Computing individual phrase ΔH changes...\n",
            "Computing ΔH for teacher...\n",
            "Computing ΔH for own...\n",
            "Computing ΔH for sure...\n",
            "Computing ΔH for unsure...\n",
            "Computing ΔH for quick...\n",
            "Computing ΔH for fast...\n",
            "Computing ΔH for step...\n",
            "Computing ΔH for combined...\n",
            "\n",
            "ΔH Summary by Individual Phrase:\n",
            "  teacher: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 4\n",
            "  own: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 4\n",
            "  sure: Mean ΔH = -0.0021, Negative% = 33.3%, Count = 6\n",
            "  unsure: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 2\n",
            "  quick: Mean ΔH = 0.0040, Negative% = 0.0%, Count = 2\n",
            "  fast: No valid ΔH values\n",
            "  step: Mean ΔH = -0.0004, Negative% = 50.0%, Count = 2\n",
            "  combined: Mean ΔH = 0.0002, Negative% = 6.2%, Count = 16\n",
            "Computing cross-phrase comparison...\n",
            "\n",
            "Cross-phrase comparison completed!\n",
            "Phrases ranked by bias strength (most biased first):\n",
            "  1. quick: 0.0040\n",
            "  2. sure: 0.0021\n",
            "  3. step: 0.0013\n",
            "  4. combined: 0.0008\n",
            "  5. teacher: 0.0000\n",
            "Saving individual phrase aggregation results to aggregated_analysis_individual/\n",
            "Computing individual phrase ΔH changes...\n",
            "Computing ΔH for teacher...\n",
            "Computing ΔH for own...\n",
            "Computing ΔH for sure...\n",
            "Computing ΔH for unsure...\n",
            "Computing ΔH for quick...\n",
            "Computing ΔH for fast...\n",
            "Computing ΔH for step...\n",
            "Computing ΔH for combined...\n",
            "\n",
            "ΔH Summary by Individual Phrase:\n",
            "  teacher: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 4\n",
            "  own: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 4\n",
            "  sure: Mean ΔH = -0.0021, Negative% = 33.3%, Count = 6\n",
            "  unsure: Mean ΔH = 0.0000, Negative% = 0.0%, Count = 2\n",
            "  quick: Mean ΔH = 0.0040, Negative% = 0.0%, Count = 2\n",
            "  fast: No valid ΔH values\n",
            "  step: Mean ΔH = -0.0004, Negative% = 50.0%, Count = 2\n",
            "  combined: Mean ΔH = 0.0002, Negative% = 6.2%, Count = 16\n",
            "Computing cross-phrase comparison...\n",
            "\n",
            "Cross-phrase comparison completed!\n",
            "Phrases ranked by bias strength (most biased first):\n",
            "  1. quick: 0.0040\n",
            "  2. sure: 0.0021\n",
            "  3. step: 0.0013\n",
            "  4. combined: 0.0008\n",
            "  5. teacher: 0.0000\n",
            "Individual phrase aggregation analysis completed successfully!\n",
            "Generated 8 separate phrase analyses:\n",
            "  - teacher: aggregated_entropies_teacher.json, delta_h_changes_teacher.json\n",
            "  - own: aggregated_entropies_own.json, delta_h_changes_own.json\n",
            "  - sure: aggregated_entropies_sure.json, delta_h_changes_sure.json\n",
            "  - unsure: aggregated_entropies_unsure.json, delta_h_changes_unsure.json\n",
            "  - quick: aggregated_entropies_quick.json, delta_h_changes_quick.json\n",
            "  - fast: aggregated_entropies_fast.json, delta_h_changes_fast.json\n",
            "  - step: aggregated_entropies_step.json, delta_h_changes_step.json\n",
            "  - combined: aggregated_entropies_combined.json, delta_h_changes_combined.json\n",
            "Additional files:\n",
            "  - phrase_delta_h_statistics.json\n",
            "  - cross_phrase_comparison.json\n",
            "Ready for individual phrase statistical analysis\n",
            "\n",
            "Individual phrase aggregation analysis completed: aggregated_analysis_individual\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from scipy.stats import ttest_ind, spearmanr, normaltest, ttest_1samp, kruskal\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class StatisticalAnalysisIndividual:\n",
        "    def __init__(self, aggregated_analysis_dir: str, results_csv: str = None):\n",
        "        \"\"\"\n",
        "        Analyze and Compare Results for Individual Phrases (6 + 1)\n",
        "\n",
        "        Methodology:\n",
        "        - Individual phrase patterns: High/low entropy per phrase = faithful/unfaithful\n",
        "        - Cross-phrase statistical tests: Compare bias effects between phrases\n",
        "        - Phrase-specific correlations: Entropy vs accuracy per phrase\n",
        "        - Ranking analysis: Identify most problematic bias phrases\n",
        "\n",
        "        Args:\n",
        "            aggregated_analysis_dir: Directory containing individual phrase aggregation results\n",
        "            results_csv: Optional path to results CSV for accuracy correlation\n",
        "        \"\"\"\n",
        "        self.aggregated_analysis_dir = aggregated_analysis_dir\n",
        "        self.results_csv = results_csv\n",
        "\n",
        "        print(\"Starting Individual Phrase Statistical Analysis\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "\n",
        "        # Load individual phrase aggregation results\n",
        "        self.load_aggregation_data()\n",
        "\n",
        "        # Storage for analysis results\n",
        "        self.analysis_results = {\n",
        "            'individual_phrase_patterns': {},\n",
        "            'cross_phrase_statistical_tests': {},\n",
        "            'phrase_specific_correlations': {},\n",
        "            'bias_ranking_analysis': {},\n",
        "            'error_checks': {}\n",
        "        }\n",
        "\n",
        "    def load_aggregation_data(self):\n",
        "        \"\"\"Load individual phrase aggregated entropy data and ΔH changes\"\"\"\n",
        "        self.aggregated_entropies_individual = {}\n",
        "        self.delta_h_changes_individual = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Load aggregated entropies\n",
        "            agg_file = os.path.join(self.aggregated_analysis_dir, f\"aggregated_entropies_{phrase}.json\")\n",
        "            if os.path.exists(agg_file):\n",
        "                with open(agg_file, 'r') as f:\n",
        "                    self.aggregated_entropies_individual[phrase] = json.load(f)\n",
        "                print(f\"Loaded aggregated entropies for '{phrase}': {len(self.aggregated_entropies_individual[phrase])} instances\")\n",
        "            else:\n",
        "                print(f\"Warning: {agg_file} not found\")\n",
        "                self.aggregated_entropies_individual[phrase] = {}\n",
        "\n",
        "            # Load ΔH changes\n",
        "            delta_file = os.path.join(self.aggregated_analysis_dir, f\"delta_h_changes_{phrase}.json\")\n",
        "            if os.path.exists(delta_file):\n",
        "                with open(delta_file, 'r') as f:\n",
        "                    self.delta_h_changes_individual[phrase] = json.load(f)\n",
        "            else:\n",
        "                print(f\"Warning: {delta_file} not found\")\n",
        "                self.delta_h_changes_individual[phrase] = {}\n",
        "\n",
        "        # Load cross-phrase comparison\n",
        "        comparison_file = os.path.join(self.aggregated_analysis_dir, \"cross_phrase_comparison.json\")\n",
        "        if os.path.exists(comparison_file):\n",
        "            with open(comparison_file, 'r') as f:\n",
        "                self.cross_phrase_comparison = json.load(f)\n",
        "        else:\n",
        "            print(\"Warning: cross_phrase_comparison.json not found\")\n",
        "            self.cross_phrase_comparison = {}\n",
        "\n",
        "        print(f\"Loaded individual phrase data for {len(self.aggregated_entropies_individual)} analyses\")\n",
        "\n",
        "        # Optionally load results CSV for accuracy correlation\n",
        "        if self.results_csv and os.path.exists(self.results_csv):\n",
        "            try:\n",
        "                self.results_df = pd.read_csv(self.results_csv)\n",
        "                print(f\"Loaded results CSV: {len(self.results_df)} rows\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not load results CSV: {e}\")\n",
        "                self.results_df = None\n",
        "        else:\n",
        "            self.results_df = None\n",
        "\n",
        "    def analyze_individual_phrase_patterns(self):\n",
        "        \"\"\"\n",
        "        Pattern Analysis: High/low entropy per phrase = faithful/unfaithful\n",
        "        \"\"\"\n",
        "        print(\"Analyzing individual phrase patterns...\")\n",
        "\n",
        "        individual_patterns = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            print(f\"Analyzing {phrase} patterns...\")\n",
        "\n",
        "            # Entropy patterns for this phrase\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "\n",
        "            if entropy_values:\n",
        "                individual_patterns[phrase] = {\n",
        "                    'entropy_stats': {\n",
        "                        'mean': float(np.mean(entropy_values)),\n",
        "                        'std': float(np.std(entropy_values)),\n",
        "                        'min': float(np.min(entropy_values)),\n",
        "                        'max': float(np.max(entropy_values)),\n",
        "                        'count': len(entropy_values),\n",
        "                        'interpretation': f'Higher values indicate more faithful attention at {phrase} positions'\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                # Analyze distribution by prompt type\n",
        "                entropy_by_prompt = {'prompt_1': [], 'prompt_2': [], 'prompt_3': []}\n",
        "                for key, value in self.aggregated_entropies_individual[phrase].items():\n",
        "                    if value is not None:\n",
        "                        for prompt_type in entropy_by_prompt.keys():\n",
        "                            if prompt_type in key:\n",
        "                                entropy_by_prompt[prompt_type].append(value)\n",
        "\n",
        "                individual_patterns[phrase]['entropy_by_prompt_type'] = {}\n",
        "                for prompt_type, values in entropy_by_prompt.items():\n",
        "                    if values:\n",
        "                        individual_patterns[phrase]['entropy_by_prompt_type'][prompt_type] = {\n",
        "                            'mean': float(np.mean(values)),\n",
        "                            'std': float(np.std(values)),\n",
        "                            'count': len(values)\n",
        "                        }\n",
        "\n",
        "                # ΔH patterns for this phrase\n",
        "                if phrase in self.delta_h_changes_individual:\n",
        "                    all_delta_h = []\n",
        "                    delta_h_by_comparison = {}\n",
        "\n",
        "                    for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                        for change_key, change_value in row_changes.items():\n",
        "                            if isinstance(change_value, (int, float)):\n",
        "                                all_delta_h.append(change_value)\n",
        "\n",
        "                                if change_key not in delta_h_by_comparison:\n",
        "                                    delta_h_by_comparison[change_key] = []\n",
        "                                delta_h_by_comparison[change_key].append(change_value)\n",
        "\n",
        "                    if all_delta_h:\n",
        "                        individual_patterns[phrase]['delta_h_patterns'] = {\n",
        "                            'overall': {\n",
        "                                'mean': float(np.mean(all_delta_h)),\n",
        "                                'std': float(np.std(all_delta_h)),\n",
        "                                'negative_count': sum(1 for x in all_delta_h if x < 0),\n",
        "                                'positive_count': sum(1 for x in all_delta_h if x > 0),\n",
        "                                'total_count': len(all_delta_h),\n",
        "                                'negative_percentage': (sum(1 for x in all_delta_h if x < 0) / len(all_delta_h)) * 100,\n",
        "                                'interpretation': f'Negative ΔH = variants more focused at {phrase} positions (unfaithful)'\n",
        "                            },\n",
        "                            'by_comparison': {}\n",
        "                        }\n",
        "\n",
        "                        for comparison, values in delta_h_by_comparison.items():\n",
        "                            individual_patterns[phrase]['delta_h_patterns']['by_comparison'][comparison] = {\n",
        "                                'mean': float(np.mean(values)),\n",
        "                                'std': float(np.std(values)),\n",
        "                                'negative_percentage': (sum(1 for x in values if x < 0) / len(values)) * 100,\n",
        "                                'count': len(values)\n",
        "                            }\n",
        "\n",
        "        self.analysis_results['individual_phrase_patterns'] = individual_patterns\n",
        "        return individual_patterns\n",
        "\n",
        "    def perform_cross_phrase_statistical_tests(self):\n",
        "        \"\"\"\n",
        "        Cross-phrase Statistical Tests: Compare bias effects between phrases\n",
        "        \"\"\"\n",
        "        print(\"Performing cross-phrase statistical tests...\")\n",
        "\n",
        "        cross_phrase_tests = {}\n",
        "\n",
        "        # Collect entropy values for each phrase\n",
        "        phrase_entropies = {}\n",
        "        phrase_delta_h = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Entropy values\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            if entropy_values:\n",
        "                phrase_entropies[phrase] = entropy_values\n",
        "\n",
        "            # ΔH values\n",
        "            delta_h_values = []\n",
        "            if phrase in self.delta_h_changes_individual:\n",
        "                for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                    delta_h_values.extend(row_changes.values())\n",
        "            if delta_h_values:\n",
        "                phrase_delta_h[phrase] = delta_h_values\n",
        "\n",
        "        # Pairwise comparisons between phrases (entropy)\n",
        "        cross_phrase_tests['entropy_comparisons'] = {}\n",
        "        phrase_names = list(phrase_entropies.keys())\n",
        "\n",
        "        for i in range(len(phrase_names)):\n",
        "            for j in range(i + 1, len(phrase_names)):\n",
        "                phrase1, phrase2 = phrase_names[i], phrase_names[j]\n",
        "\n",
        "                try:\n",
        "                    t_stat, p_value = ttest_ind(phrase_entropies[phrase1], phrase_entropies[phrase2])\n",
        "                    cross_phrase_tests['entropy_comparisons'][f\"{phrase1}_vs_{phrase2}\"] = {\n",
        "                        't_statistic': float(t_stat),\n",
        "                        'p_value': float(p_value),\n",
        "                        'significant': p_value < 0.05,\n",
        "                        'mean_diff': float(np.mean(phrase_entropies[phrase1]) - np.mean(phrase_entropies[phrase2])),\n",
        "                        'effect_size': abs(np.mean(phrase_entropies[phrase1]) - np.mean(phrase_entropies[phrase2])) /\n",
        "                                      np.sqrt((np.var(phrase_entropies[phrase1]) + np.var(phrase_entropies[phrase2])) / 2),\n",
        "                        'interpretation': f'Comparing entropy between {phrase1} and {phrase2} bias phrases'\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"Entropy comparison failed for {phrase1} vs {phrase2}: {e}\")\n",
        "\n",
        "        # Pairwise comparisons between phrases (ΔH)\n",
        "        cross_phrase_tests['delta_h_comparisons'] = {}\n",
        "        delta_h_phrase_names = list(phrase_delta_h.keys())\n",
        "\n",
        "        for i in range(len(delta_h_phrase_names)):\n",
        "            for j in range(i + 1, len(delta_h_phrase_names)):\n",
        "                phrase1, phrase2 = delta_h_phrase_names[i], delta_h_phrase_names[j]\n",
        "\n",
        "                try:\n",
        "                    t_stat, p_value = ttest_ind(phrase_delta_h[phrase1], phrase_delta_h[phrase2])\n",
        "                    cross_phrase_tests['delta_h_comparisons'][f\"{phrase1}_vs_{phrase2}\"] = {\n",
        "                        't_statistic': float(t_stat),\n",
        "                        'p_value': float(p_value),\n",
        "                        'significant': p_value < 0.05,\n",
        "                        'mean_diff': float(np.mean(phrase_delta_h[phrase1]) - np.mean(phrase_delta_h[phrase2])),\n",
        "                        'effect_size': abs(np.mean(phrase_delta_h[phrase1]) - np.mean(phrase_delta_h[phrase2])) /\n",
        "                                      np.sqrt((np.var(phrase_delta_h[phrase1]) + np.var(phrase_delta_h[phrase2])) / 2),\n",
        "                        'interpretation': f'Comparing ΔH bias effects between {phrase1} and {phrase2}'\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"ΔH comparison failed for {phrase1} vs {phrase2}: {e}\")\n",
        "\n",
        "        # One-way ANOVA test across all phrases\n",
        "        if len(phrase_entropies) > 2:\n",
        "            try:\n",
        "                entropy_groups = [phrase_entropies[phrase] for phrase in phrase_entropies.keys()]\n",
        "                h_stat, p_value = kruskal(*entropy_groups)\n",
        "                cross_phrase_tests['overall_entropy_test'] = {\n",
        "                    'test': 'Kruskal-Wallis H-test (non-parametric ANOVA)',\n",
        "                    'h_statistic': float(h_stat),\n",
        "                    'p_value': float(p_value),\n",
        "                    'significant': p_value < 0.05,\n",
        "                    'interpretation': 'Tests if entropy distributions differ significantly across phrases',\n",
        "                    'phrases_tested': list(phrase_entropies.keys())\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Overall entropy test failed: {e}\")\n",
        "\n",
        "        # One-way test for ΔH across all phrases\n",
        "        if len(phrase_delta_h) > 2:\n",
        "            try:\n",
        "                delta_h_groups = [phrase_delta_h[phrase] for phrase in phrase_delta_h.keys()]\n",
        "                h_stat, p_value = kruskal(*delta_h_groups)\n",
        "                cross_phrase_tests['overall_delta_h_test'] = {\n",
        "                    'test': 'Kruskal-Wallis H-test (non-parametric ANOVA)',\n",
        "                    'h_statistic': float(h_stat),\n",
        "                    'p_value': float(p_value),\n",
        "                    'significant': p_value < 0.05,\n",
        "                    'interpretation': 'Tests if ΔH distributions differ significantly across phrases',\n",
        "                    'phrases_tested': list(phrase_delta_h.keys())\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Overall ΔH test failed: {e}\")\n",
        "\n",
        "        # One-sample t-tests for each phrase ΔH against zero\n",
        "        cross_phrase_tests['phrase_vs_zero_tests'] = {}\n",
        "        for phrase, delta_h_values in phrase_delta_h.items():\n",
        "            if len(delta_h_values) >= 3:\n",
        "                try:\n",
        "                    t_stat, p_value = ttest_1samp(delta_h_values, 0)\n",
        "                    cross_phrase_tests['phrase_vs_zero_tests'][phrase] = {\n",
        "                        't_statistic': float(t_stat),\n",
        "                        'p_value': float(p_value),\n",
        "                        'significant': p_value < 0.05,\n",
        "                        'mean_delta_h': float(np.mean(delta_h_values)),\n",
        "                        'interpretation': f'Testing if {phrase} ΔH is significantly different from zero'\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"One-sample test failed for {phrase}: {e}\")\n",
        "\n",
        "        self.analysis_results['cross_phrase_statistical_tests'] = cross_phrase_tests\n",
        "        return cross_phrase_tests\n",
        "\n",
        "    def perform_phrase_specific_correlations(self):\n",
        "        \"\"\"\n",
        "        Phrase-specific Correlations: Entropy vs accuracy per phrase\n",
        "        \"\"\"\n",
        "        print(\"Performing phrase-specific correlations...\")\n",
        "\n",
        "        phrase_correlations = {}\n",
        "\n",
        "        if self.results_df is not None:\n",
        "            try:\n",
        "                for phrase in self.analysis_keys:\n",
        "                    print(f\"Computing correlations for {phrase}...\")\n",
        "\n",
        "                    correlation_data = []\n",
        "\n",
        "                    for index, row in self.results_df.iterrows():\n",
        "                        row_idx = row.get('Index', index)\n",
        "\n",
        "                        for prompt_col in ['Prompt_1', 'Prompt_2', 'Prompt_3']:\n",
        "                            prompt_key = f\"row_{row_idx}_{prompt_col.lower()}\"\n",
        "\n",
        "                            # Get phrase-specific entropy\n",
        "                            phrase_entropy = self.aggregated_entropies_individual[phrase].get(prompt_key)\n",
        "\n",
        "                            prediction_col = f\"{prompt_col}_Prediction_Extracted\"\n",
        "                            if prediction_col in row and pd.notna(row[prediction_col]):\n",
        "                                prediction = str(row[prediction_col])\n",
        "                                has_numerical_answer = prediction.replace('.', '').isdigit()\n",
        "\n",
        "                                if phrase_entropy is not None:\n",
        "                                    correlation_data.append({\n",
        "                                        'phrase_entropy': phrase_entropy,\n",
        "                                        'has_numerical_answer': has_numerical_answer,\n",
        "                                        'prediction': prediction,\n",
        "                                        'prompt_type': prompt_col\n",
        "                                    })\n",
        "\n",
        "                    if len(correlation_data) > 3:\n",
        "                        corr_df = pd.DataFrame(correlation_data)\n",
        "\n",
        "                        try:\n",
        "                            # Phrase entropy vs accuracy\n",
        "                            rho, p_value = spearmanr(\n",
        "                                corr_df['phrase_entropy'],\n",
        "                                corr_df['has_numerical_answer'].astype(int)\n",
        "                            )\n",
        "\n",
        "                            phrase_correlations[phrase] = {\n",
        "                                'entropy_vs_accuracy': {\n",
        "                                    'spearman_rho': float(rho),\n",
        "                                    'p_value': float(p_value),\n",
        "                                    'significant': p_value < 0.05,\n",
        "                                    'interpretation': f'Correlation between {phrase} entropy and prediction accuracy'\n",
        "                                },\n",
        "                                'data_summary': {\n",
        "                                    'total_data_points': len(corr_df),\n",
        "                                    'entropy_range': [float(corr_df['phrase_entropy'].min()), float(corr_df['phrase_entropy'].max())],\n",
        "                                    'accuracy_rate': float(corr_df['has_numerical_answer'].mean()),\n",
        "                                    'prompt_type_distribution': corr_df['prompt_type'].value_counts().to_dict()\n",
        "                                }\n",
        "                            }\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Correlation calculation failed for {phrase}: {e}\")\n",
        "                            phrase_correlations[phrase] = {'error': str(e)}\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Phrase-specific correlation analysis failed: {e}\")\n",
        "                phrase_correlations['error'] = str(e)\n",
        "\n",
        "        else:\n",
        "            phrase_correlations['note'] = 'Results CSV not available - correlation analysis skipped'\n",
        "\n",
        "        self.analysis_results['phrase_specific_correlations'] = phrase_correlations\n",
        "        return phrase_correlations\n",
        "\n",
        "    def perform_bias_ranking_analysis(self):\n",
        "        \"\"\"\n",
        "        Bias Ranking Analysis: Identify most problematic bias phrases\n",
        "        \"\"\"\n",
        "        print(\"Performing bias ranking analysis...\")\n",
        "\n",
        "        ranking_analysis = {}\n",
        "\n",
        "        # Collect metrics for ranking\n",
        "        phrase_metrics = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            if phrase == 'combined':\n",
        "                continue  # Skip combined for individual ranking\n",
        "\n",
        "            # Get ΔH values\n",
        "            all_delta_h = []\n",
        "            if phrase in self.delta_h_changes_individual:\n",
        "                for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                    all_delta_h.extend(row_changes.values())\n",
        "\n",
        "            # Get entropy values\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "\n",
        "            if all_delta_h and entropy_values:\n",
        "                phrase_metrics[phrase] = {\n",
        "                    'mean_delta_h': float(np.mean(all_delta_h)),\n",
        "                    'abs_mean_delta_h': float(np.mean(np.abs(all_delta_h))),\n",
        "                    'negative_percentage': (sum(1 for x in all_delta_h if x < 0) / len(all_delta_h)) * 100,\n",
        "                    'delta_h_std': float(np.std(all_delta_h)),\n",
        "                    'mean_entropy': float(np.mean(entropy_values)),\n",
        "                    'entropy_std': float(np.std(entropy_values)),\n",
        "                    'data_coverage': len(all_delta_h)\n",
        "                }\n",
        "\n",
        "        # Ranking by different criteria\n",
        "        if phrase_metrics:\n",
        "            # 1. Most biased (highest absolute ΔH)\n",
        "            most_biased = sorted(phrase_metrics.items(), key=lambda x: x[1]['abs_mean_delta_h'], reverse=True)\n",
        "\n",
        "            # 2. Most unfaithful (most negative ΔH)\n",
        "            most_unfaithful = sorted(phrase_metrics.items(), key=lambda x: x[1]['mean_delta_h'])\n",
        "\n",
        "            # 3. Most consistent bias (highest negative percentage)\n",
        "            most_consistent_bias = sorted(phrase_metrics.items(), key=lambda x: x[1]['negative_percentage'], reverse=True)\n",
        "\n",
        "            # 4. Lowest average entropy (most focused attention)\n",
        "            lowest_entropy = sorted(phrase_metrics.items(), key=lambda x: x[1]['mean_entropy'])\n",
        "\n",
        "            ranking_analysis = {\n",
        "                'phrase_metrics': phrase_metrics,\n",
        "                'rankings': {\n",
        "                    'most_biased_phrases': {\n",
        "                        'ranking': [(phrase, metrics['abs_mean_delta_h']) for phrase, metrics in most_biased],\n",
        "                        'top_3': [phrase for phrase, _ in most_biased[:3]],\n",
        "                        'criterion': 'Highest absolute mean ΔH'\n",
        "                    },\n",
        "                    'most_unfaithful_phrases': {\n",
        "                        'ranking': [(phrase, metrics['mean_delta_h']) for phrase, metrics in most_unfaithful],\n",
        "                        'top_3': [phrase for phrase, _ in most_unfaithful[:3]],\n",
        "                        'criterion': 'Most negative mean ΔH'\n",
        "                    },\n",
        "                    'most_consistent_bias': {\n",
        "                        'ranking': [(phrase, metrics['negative_percentage']) for phrase, metrics in most_consistent_bias],\n",
        "                        'top_3': [phrase for phrase, _ in most_consistent_bias[:3]],\n",
        "                        'criterion': 'Highest percentage of negative ΔH values'\n",
        "                    },\n",
        "                    'lowest_entropy_phrases': {\n",
        "                        'ranking': [(phrase, metrics['mean_entropy']) for phrase, metrics in lowest_entropy],\n",
        "                        'top_3': [phrase for phrase, _ in lowest_entropy[:3]],\n",
        "                        'criterion': 'Lowest average entropy (most focused attention)'\n",
        "                    }\n",
        "                },\n",
        "                'summary_insights': {\n",
        "                    'most_problematic_overall': most_biased[0][0] if most_biased else None,\n",
        "                    'strongest_unfaithfulness': most_unfaithful[0][0] if most_unfaithful else None,\n",
        "                    'most_reliable_bias_indicator': most_consistent_bias[0][0] if most_consistent_bias else None\n",
        "                }\n",
        "            }\n",
        "\n",
        "        print(\"\\nBias ranking analysis completed!\")\n",
        "        if 'rankings' in ranking_analysis:\n",
        "            print(\"Top 3 most biased phrases:\")\n",
        "            for i, phrase in enumerate(ranking_analysis['rankings']['most_biased_phrases']['top_3'], 1):\n",
        "                bias_score = phrase_metrics[phrase]['abs_mean_delta_h']\n",
        "                print(f\"  {i}. {phrase}: {bias_score:.4f}\")\n",
        "\n",
        "        self.analysis_results['bias_ranking_analysis'] = ranking_analysis\n",
        "        return ranking_analysis\n",
        "\n",
        "    def perform_error_checking(self):\n",
        "        \"\"\"\n",
        "        Error Check: Handle missing data, validate phrase coverage\n",
        "        \"\"\"\n",
        "        print(\"Performing error checking for individual phrases...\")\n",
        "\n",
        "        error_checks = {}\n",
        "\n",
        "        # Check data coverage for each phrase\n",
        "        phrase_coverage = {}\n",
        "        for phrase in self.analysis_keys:\n",
        "            entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "            total_instances = len(self.aggregated_entropies_individual[phrase])\n",
        "\n",
        "            delta_h_count = 0\n",
        "            if phrase in self.delta_h_changes_individual:\n",
        "                delta_h_count = sum(len(row_changes) for row_changes in self.delta_h_changes_individual[phrase].values())\n",
        "\n",
        "            phrase_coverage[phrase] = {\n",
        "                'entropy_instances': len(entropy_values),\n",
        "                'total_possible_instances': total_instances,\n",
        "                'entropy_coverage_rate': len(entropy_values) / total_instances if total_instances > 0 else 0,\n",
        "                'delta_h_comparisons': delta_h_count,\n",
        "                'zero_entropy_count': sum(1 for v in entropy_values if v == 0.0),\n",
        "                'very_low_entropy_count': sum(1 for v in entropy_values if 0 < v < 0.1),\n",
        "                'very_high_entropy_count': sum(1 for v in entropy_values if v > 5.0)\n",
        "            }\n",
        "\n",
        "        error_checks['phrase_coverage'] = phrase_coverage\n",
        "\n",
        "        # Identify phrases with insufficient data\n",
        "        low_coverage_phrases = [\n",
        "            phrase for phrase, stats in phrase_coverage.items()\n",
        "            if stats['entropy_coverage_rate'] < 0.1\n",
        "        ]\n",
        "\n",
        "        error_checks['data_quality_issues'] = {\n",
        "            'low_coverage_phrases': low_coverage_phrases,\n",
        "            'phrases_with_no_delta_h': [\n",
        "                phrase for phrase, stats in phrase_coverage.items()\n",
        "                if stats['delta_h_comparisons'] == 0\n",
        "            ],\n",
        "            'interpretation': 'Phrases with low coverage may have unreliable statistics'\n",
        "        }\n",
        "\n",
        "        # Cross-phrase consistency check\n",
        "        if len(self.analysis_keys) > 1:\n",
        "            entropy_ranges = {}\n",
        "            for phrase in self.analysis_keys:\n",
        "                entropy_values = [v for v in self.aggregated_entropies_individual[phrase].values() if v is not None]\n",
        "                if entropy_values:\n",
        "                    entropy_ranges[phrase] = {\n",
        "                        'min': float(np.min(entropy_values)),\n",
        "                        'max': float(np.max(entropy_values)),\n",
        "                        'range': float(np.max(entropy_values) - np.min(entropy_values))\n",
        "                    }\n",
        "\n",
        "            error_checks['cross_phrase_consistency'] = {\n",
        "                'entropy_ranges': entropy_ranges,\n",
        "                'range_consistency': 'Good' if len(set(round(stats['range'], 2) for stats in entropy_ranges.values())) <= 3 else 'Variable'\n",
        "            }\n",
        "\n",
        "        self.analysis_results['error_checks'] = error_checks\n",
        "        return error_checks\n",
        "\n",
        "    def run_complete_analysis(self):\n",
        "        \"\"\"Run the complete individual phrase statistical analysis pipeline\"\"\"\n",
        "        print(\"Running complete individual phrase statistical analysis...\")\n",
        "\n",
        "        # Run all analysis components\n",
        "        individual_patterns = self.analyze_individual_phrase_patterns()\n",
        "        cross_phrase_tests = self.perform_cross_phrase_statistical_tests()\n",
        "        phrase_correlations = self.perform_phrase_specific_correlations()\n",
        "        bias_ranking = self.perform_bias_ranking_analysis()\n",
        "        error_checks = self.perform_error_checking()\n",
        "\n",
        "        # Extract key findings\n",
        "        key_findings = self.extract_key_findings()\n",
        "\n",
        "        # Create comprehensive summary\n",
        "        summary = {\n",
        "            'components_completed': [\n",
        "                'individual_phrase_patterns',\n",
        "                'cross_phrase_statistical_tests',\n",
        "                'phrase_specific_correlations',\n",
        "                'bias_ranking_analysis',\n",
        "                'error_checks'\n",
        "            ],\n",
        "            'key_findings': key_findings,\n",
        "            'methodology_compliance': {\n",
        "                'individual_patterns_analyzed': 'High/low entropy faithfulness per phrase completed',\n",
        "                'cross_phrase_tests': 'Statistical comparisons between phrases completed',\n",
        "                'phrase_correlations': 'Entropy vs accuracy per phrase completed',\n",
        "                'bias_ranking': 'Most problematic phrases identified',\n",
        "                'error_checking': 'Data quality and coverage validated'\n",
        "            },\n",
        "            'analyses_completed': len(self.analysis_keys)\n",
        "        }\n",
        "\n",
        "        self.analysis_results['summary'] = summary\n",
        "\n",
        "        print(\"Complete individual phrase statistical analysis finished!\")\n",
        "        return self.analysis_results\n",
        "\n",
        "    def extract_key_findings(self):\n",
        "        \"\"\"Extract key findings from the individual phrase analysis\"\"\"\n",
        "        findings = {}\n",
        "\n",
        "        # Individual phrase findings\n",
        "        if 'individual_phrase_patterns' in self.analysis_results:\n",
        "            patterns = self.analysis_results['individual_phrase_patterns']\n",
        "\n",
        "            phrase_bias_scores = {}\n",
        "            for phrase, data in patterns.items():\n",
        "                if 'delta_h_patterns' in data and 'overall' in data['delta_h_patterns']:\n",
        "                    delta_patterns = data['delta_h_patterns']['overall']\n",
        "                    phrase_bias_scores[phrase] = {\n",
        "                        'negative_percentage': delta_patterns['negative_percentage'],\n",
        "                        'mean_delta_h': delta_patterns['mean'],\n",
        "                        'bias_detected': delta_patterns['negative_count'] > delta_patterns['positive_count']\n",
        "                    }\n",
        "\n",
        "            findings['individual_phrase_bias'] = phrase_bias_scores\n",
        "\n",
        "        # Cross-phrase comparison findings\n",
        "        if 'cross_phrase_statistical_tests' in self.analysis_results:\n",
        "            tests = self.analysis_results['cross_phrase_statistical_tests']\n",
        "\n",
        "            significant_comparisons = []\n",
        "            if 'entropy_comparisons' in tests:\n",
        "                significant_comparisons.extend([\n",
        "                    k for k, v in tests['entropy_comparisons'].items()\n",
        "                    if v.get('significant', False)\n",
        "                ])\n",
        "            if 'delta_h_comparisons' in tests:\n",
        "                significant_comparisons.extend([\n",
        "                    k for k, v in tests['delta_h_comparisons'].items()\n",
        "                    if v.get('significant', False)\n",
        "                ])\n",
        "\n",
        "            findings['cross_phrase_differences'] = {\n",
        "                'significant_comparisons': significant_comparisons,\n",
        "                'total_comparisons': len(tests.get('entropy_comparisons', {})) + len(tests.get('delta_h_comparisons', {})),\n",
        "                'evidence_strength': 'Strong' if len(significant_comparisons) > 2 else 'Moderate'\n",
        "            }\n",
        "\n",
        "        # Bias ranking findings\n",
        "        if 'bias_ranking_analysis' in self.analysis_results:\n",
        "            ranking = self.analysis_results['bias_ranking_analysis']\n",
        "\n",
        "            if 'rankings' in ranking:\n",
        "                findings['most_problematic_phrases'] = {\n",
        "                    'most_biased': ranking['rankings']['most_biased_phrases']['top_3'],\n",
        "                    'most_unfaithful': ranking['rankings']['most_unfaithful_phrases']['top_3'],\n",
        "                    'most_consistent_bias': ranking['rankings']['most_consistent_bias']['top_3']\n",
        "                }\n",
        "\n",
        "        return findings\n",
        "\n",
        "    def save_results(self, output_dir: str = \"statistical_results_individual\"):\n",
        "        \"\"\"Save individual phrase statistical analysis results\"\"\"\n",
        "        print(f\"Saving individual phrase statistical results to {output_dir}/\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Convert numpy types to native Python types for JSON serialization\n",
        "        def convert_numpy_types(obj):\n",
        "            if isinstance(obj, dict):\n",
        "                return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
        "            elif isinstance(obj, list):\n",
        "                return [convert_numpy_types(v) for v in obj]\n",
        "            elif isinstance(obj, np.integer):\n",
        "                return int(obj)\n",
        "            elif isinstance(obj, np.floating):\n",
        "                return float(obj)\n",
        "            elif isinstance(obj, np.bool_):\n",
        "                return bool(obj)\n",
        "            elif isinstance(obj, np.ndarray):\n",
        "                return obj.tolist()\n",
        "            else:\n",
        "                return obj\n",
        "\n",
        "        # Convert analysis results\n",
        "        serializable_results = convert_numpy_types(self.analysis_results)\n",
        "\n",
        "        # Save complete analysis results\n",
        "        results_file = os.path.join(output_dir, \"statistical_analysis_individual_results.json\")\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump(serializable_results, f, indent=2)\n",
        "\n",
        "        # Save individual components\n",
        "        components = [\n",
        "            'individual_phrase_patterns',\n",
        "            'cross_phrase_statistical_tests',\n",
        "            'phrase_specific_correlations',\n",
        "            'bias_ranking_analysis',\n",
        "            'error_checks'\n",
        "        ]\n",
        "\n",
        "        for component in components:\n",
        "            if component in serializable_results:\n",
        "                component_file = os.path.join(output_dir, f\"{component}.json\")\n",
        "                with open(component_file, 'w') as f:\n",
        "                    json.dump(serializable_results[component], f, indent=2)\n",
        "\n",
        "        print(\"Individual phrase statistical analysis completed successfully!\")\n",
        "        print(\"Ready for individual phrase results visualization\")\n",
        "\n",
        "        return output_dir\n",
        "\n",
        "\n",
        "def run_statistical_analysis_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase statistical analysis pipeline\n",
        "    \"\"\"\n",
        "    # Configuration - Update these paths for your setup\n",
        "    AGGREGATED_ANALYSIS_DIR = \"aggregated_analysis_individual\"  # Contains individual phrase aggregation files\n",
        "    RESULTS_CSV = \"results_with_predictions_llama.csv\"  # Optional for correlation analysis\n",
        "\n",
        "    print(\"Starting Individual Phrase Statistical Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = StatisticalAnalysisIndividual(\n",
        "        aggregated_analysis_dir=AGGREGATED_ANALYSIS_DIR,\n",
        "        results_csv=RESULTS_CSV\n",
        "    )\n",
        "\n",
        "    # Run complete analysis\n",
        "    results = analyzer.run_complete_analysis()\n",
        "\n",
        "    # Save all results\n",
        "    output_dir = analyzer.save_results()\n",
        "\n",
        "    print(f\"\\nIndividual phrase statistical analysis completed: {output_dir}\")\n",
        "    return analyzer, results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer, results = run_statistical_analysis_individual()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iM5arwsF2vgL",
        "outputId": "799a2a6f-41b0-4f89-9027-d133faf95d06"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Individual Phrase Statistical Analysis\n",
            "============================================================\n",
            "Starting Individual Phrase Statistical Analysis\n",
            "============================================================\n",
            "Loaded aggregated entropies for 'teacher': 26 instances\n",
            "Loaded aggregated entropies for 'own': 26 instances\n",
            "Loaded aggregated entropies for 'sure': 26 instances\n",
            "Loaded aggregated entropies for 'unsure': 26 instances\n",
            "Loaded aggregated entropies for 'quick': 26 instances\n",
            "Loaded aggregated entropies for 'fast': 26 instances\n",
            "Loaded aggregated entropies for 'step': 26 instances\n",
            "Loaded aggregated entropies for 'combined': 26 instances\n",
            "Loaded individual phrase data for 8 analyses\n",
            "Running complete individual phrase statistical analysis...\n",
            "Analyzing individual phrase patterns...\n",
            "Analyzing teacher patterns...\n",
            "Analyzing own patterns...\n",
            "Analyzing sure patterns...\n",
            "Analyzing unsure patterns...\n",
            "Analyzing quick patterns...\n",
            "Analyzing fast patterns...\n",
            "Analyzing step patterns...\n",
            "Analyzing combined patterns...\n",
            "Performing cross-phrase statistical tests...\n",
            "Performing phrase-specific correlations...\n",
            "Performing bias ranking analysis...\n",
            "\n",
            "Bias ranking analysis completed!\n",
            "Top 3 most biased phrases:\n",
            "  1. quick: 0.0040\n",
            "  2. sure: 0.0021\n",
            "  3. step: 0.0013\n",
            "Performing error checking for individual phrases...\n",
            "Complete individual phrase statistical analysis finished!\n",
            "Saving individual phrase statistical results to statistical_results_individual/\n",
            "Individual phrase statistical analysis completed successfully!\n",
            "Ready for individual phrase results visualization\n",
            "\n",
            "Individual phrase statistical analysis completed: statistical_results_individual\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ResultsVisualizationIndividual:\n",
        "    def __init__(self, aggregated_analysis_dir: str, statistical_results_dir: str):\n",
        "        \"\"\"\n",
        "        Visualize Results: Create 7 separate visualization files (1 combined + 6 individual phrases)\n",
        "\n",
        "        Each plot follows paste5.txt structure with 2×2 layout:\n",
        "        - Histogram: Global entropy distributions vs baseline (no hint)\n",
        "        - Line: Avg entropy vs layers (per variant)\n",
        "        - Bar: Avg phrase entropy (baseline, prompt_1, prompt_2, prompt_3)\n",
        "        - Heatmap: ΔH per layer/head\n",
        "\n",
        "        Args:\n",
        "            aggregated_analysis_dir: Directory containing individual phrase aggregation results\n",
        "            statistical_results_dir: Directory containing individual phrase statistical analysis results\n",
        "        \"\"\"\n",
        "        self.aggregated_analysis_dir = aggregated_analysis_dir\n",
        "        self.statistical_results_dir = statistical_results_dir\n",
        "\n",
        "        print(\"Starting Individual Phrase Results Visualization\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Define analysis keys (6 individual + 1 combined)\n",
        "        self.analysis_keys = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\", \"combined\"]\n",
        "        self.individual_phrases = [\"teacher\", \"own\", \"sure\", \"unsure\", \"quick\", \"fast\", \"step\"]\n",
        "\n",
        "        # Load data from previous stages\n",
        "        self.load_visualization_data()\n",
        "\n",
        "        # Set up plotting style\n",
        "        self.setup_plotting_style()\n",
        "\n",
        "    def load_visualization_data(self):\n",
        "        \"\"\"Load data needed for individual phrase visualization\"\"\"\n",
        "        # Load individual phrase aggregation results\n",
        "        self.aggregated_entropies_individual = {}\n",
        "        self.delta_h_changes_individual = {}\n",
        "\n",
        "        for phrase in self.analysis_keys:\n",
        "            # Load aggregated entropies\n",
        "            agg_file = os.path.join(self.aggregated_analysis_dir, f\"aggregated_entropies_{phrase}.json\")\n",
        "            if os.path.exists(agg_file):\n",
        "                with open(agg_file, 'r') as f:\n",
        "                    self.aggregated_entropies_individual[phrase] = json.load(f)\n",
        "\n",
        "            # Load ΔH changes\n",
        "            delta_file = os.path.join(self.aggregated_analysis_dir, f\"delta_h_changes_{phrase}.json\")\n",
        "            if os.path.exists(delta_file):\n",
        "                with open(delta_file, 'r') as f:\n",
        "                    self.delta_h_changes_individual[phrase] = json.load(f)\n",
        "\n",
        "        # Load entropy arrays for layer analysis (from entropy computation stage)\n",
        "        entropy_arrays_file = os.path.join(\"entropy_results_individual\", \"entropies_arrays.json\")\n",
        "        if os.path.exists(entropy_arrays_file):\n",
        "            with open(entropy_arrays_file, 'r') as f:\n",
        "                self.entropies_arrays = json.load(f)\n",
        "        else:\n",
        "            print(\"Warning: entropies_arrays.json not found - layer analysis will be limited\")\n",
        "            self.entropies_arrays = {}\n",
        "\n",
        "        print(f\"Loaded visualization data for {len(self.aggregated_entropies_individual)} phrase analyses\")\n",
        "\n",
        "    def setup_plotting_style(self):\n",
        "        \"\"\"Set up matplotlib and seaborn styling for publication-ready plots\"\"\"\n",
        "        plt.style.use('default')\n",
        "        sns.set_palette(\"husl\")\n",
        "\n",
        "        # Publication-ready settings\n",
        "        plt.rcParams.update({\n",
        "            'figure.figsize': (15, 12),\n",
        "            'font.size': 10,\n",
        "            'axes.titlesize': 12,\n",
        "            'axes.labelsize': 11,\n",
        "            'xtick.labelsize': 9,\n",
        "            'ytick.labelsize': 9,\n",
        "            'legend.fontsize': 9,\n",
        "            'figure.titlesize': 14\n",
        "        })\n",
        "\n",
        "    def create_histogram_phrase_entropy(self, ax, phrase: str):\n",
        "        \"\"\"Histogram: Phrase entropy distributions vs baseline (no hint)\"\"\"\n",
        "        print(f\"Creating histogram for {phrase}: Phrase entropy distributions vs baseline...\")\n",
        "\n",
        "        # Extract phrase entropy values by prompt type\n",
        "        prompt_types = {'prompt_1': [], 'prompt_2': [], 'prompt_3': []}\n",
        "\n",
        "        for key, entropy_value in self.aggregated_entropies_individual[phrase].items():\n",
        "            if entropy_value is not None:\n",
        "                for prompt_type in prompt_types.keys():\n",
        "                    if prompt_type in key:\n",
        "                        prompt_types[prompt_type].append(entropy_value)\n",
        "\n",
        "        # Get baseline (no hint) entropy from overall global entropy\n",
        "        # This represents attention entropy when NO bias phrases are present\n",
        "        baseline_entropies = []\n",
        "        if hasattr(self, 'entropies_arrays'):\n",
        "            for row_prompt_key, entropy_array in self.entropies_arrays.items():\n",
        "                if 'prompt_1' in row_prompt_key:  # Use prompt_1 as baseline reference\n",
        "                    entropies = np.array(entropy_array)\n",
        "                    global_entropy = np.mean(entropies)\n",
        "                    baseline_entropies.append(global_entropy)\n",
        "\n",
        "        # Create histogram\n",
        "        colors = ['#808080', '#1f77b4', '#ff7f0e', '#2ca02c']  # Gray for baseline\n",
        "        labels = ['Baseline (No Hint)', 'Biased: Prompt 1', 'Biased: Prompt 2', 'Biased: Prompt 3']\n",
        "\n",
        "        all_values = []\n",
        "        for values in prompt_types.values():\n",
        "            if values:\n",
        "                all_values.extend(values)\n",
        "        if baseline_entropies:\n",
        "            all_values.extend(baseline_entropies)\n",
        "\n",
        "        if all_values:\n",
        "            bins = np.linspace(min(all_values), max(all_values), 15)\n",
        "\n",
        "            # Plot baseline\n",
        "            if baseline_entropies:\n",
        "                ax.hist(baseline_entropies, bins=bins, alpha=0.7, label=labels[0],\n",
        "                       color=colors[0], edgecolor='black', linewidth=0.5)\n",
        "\n",
        "            # Plot phrase-specific entropies\n",
        "            for i, (prompt_type, values) in enumerate(prompt_types.items(), 1):\n",
        "                if values:\n",
        "                    ax.hist(values, bins=bins, alpha=0.7, label=labels[i],\n",
        "                           color=colors[i], edgecolor='black', linewidth=0.5)\n",
        "\n",
        "        ax.set_xlabel('Entropy (bits)')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('Global Entropy Distributions vs Baseline\\n(All Bias Phrases)')\n",
        "        else:\n",
        "            ax.set_title(f'Entropy Distributions vs Baseline\\n(Phrase: \"{phrase}\")')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add summary statistics\n",
        "        stats_text = []\n",
        "        if baseline_entropies:\n",
        "            stats_text.append(f\"Baseline: μ={np.mean(baseline_entropies):.3f}\")\n",
        "        for prompt_type, values in prompt_types.items():\n",
        "            if values:\n",
        "                mean_val = np.mean(values)\n",
        "                stats_text.append(f\"{prompt_type}: μ={mean_val:.3f}\")\n",
        "\n",
        "        if stats_text:\n",
        "            ax.text(0.02, 0.98, '\\n'.join(stats_text), transform=ax.transAxes,\n",
        "                    verticalalignment='top', fontsize=8,\n",
        "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "    def create_line_plot_entropy_vs_layers(self, ax, phrase: str):\n",
        "        \"\"\"Line: Avg entropy vs layers (per variant)\"\"\"\n",
        "        print(f\"Creating line plot for {phrase}: Avg entropy vs layers...\")\n",
        "\n",
        "        # Extract layer-wise entropy data for this phrase\n",
        "        layer_data = {'prompt_1': {}, 'prompt_2': {}, 'prompt_3': {}}\n",
        "\n",
        "        # Use the complete entropy arrays and extract phrase-specific positions\n",
        "        phrase_positions_by_row_prompt = {}\n",
        "\n",
        "        # Load phrase positions\n",
        "        positions_file = os.path.join(\"entropy_results_individual\", f\"hint_positions_{phrase}.json\")\n",
        "        if os.path.exists(positions_file):\n",
        "            with open(positions_file, 'r') as f:\n",
        "                phrase_positions_data = json.load(f)\n",
        "\n",
        "            # Convert to row_prompt format\n",
        "            for key, positions in phrase_positions_data.items():\n",
        "                parts = key.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    row_idx = parts[0]\n",
        "                    prompt_num = parts[2]\n",
        "                    row_prompt_key = f\"row_{row_idx}_prompt_{prompt_num}\"\n",
        "                    phrase_positions_by_row_prompt[row_prompt_key] = positions\n",
        "\n",
        "        # Extract layer-wise entropies for this phrase\n",
        "        if hasattr(self, 'entropies_arrays') and phrase_positions_by_row_prompt:\n",
        "            for row_prompt_key, entropy_array in self.entropies_arrays.items():\n",
        "                prompt_type = None\n",
        "                for ptype in layer_data.keys():\n",
        "                    if ptype in row_prompt_key:\n",
        "                        prompt_type = ptype\n",
        "                        break\n",
        "\n",
        "                if prompt_type and row_prompt_key in phrase_positions_by_row_prompt:\n",
        "                    positions = phrase_positions_by_row_prompt[row_prompt_key]\n",
        "                    if positions:\n",
        "                        entropies = np.array(entropy_array)\n",
        "                        num_layers, num_heads, seq_len = entropies.shape\n",
        "\n",
        "                        # Extract entropy at phrase positions for each layer\n",
        "                        for layer_idx in range(num_layers):\n",
        "                            layer_phrase_entropies = []\n",
        "                            for head_idx in range(num_heads):\n",
        "                                for pos in positions:\n",
        "                                    if pos < seq_len:\n",
        "                                        layer_phrase_entropies.append(entropies[layer_idx, head_idx, pos])\n",
        "\n",
        "                            if layer_phrase_entropies:\n",
        "                                if layer_idx not in layer_data[prompt_type]:\n",
        "                                    layer_data[prompt_type][layer_idx] = []\n",
        "                                layer_data[prompt_type][layer_idx].extend(layer_phrase_entropies)\n",
        "\n",
        "        # Calculate averages and plot\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "        labels = ['Biased: Prompt 1', 'Biased: Prompt 2', 'Biased: Prompt 3']\n",
        "        linestyles = ['-', '--', '-.']\n",
        "\n",
        "        for i, (prompt_type, data) in enumerate(layer_data.items()):\n",
        "            if data:\n",
        "                layers = sorted(data.keys())\n",
        "                avg_entropies = [np.mean(data[layer]) for layer in layers]\n",
        "                std_entropies = [np.std(data[layer]) for layer in layers]\n",
        "\n",
        "                if avg_entropies:\n",
        "                    # Plot line with error bars\n",
        "                    ax.plot(layers, avg_entropies, color=colors[i], label=labels[i],\n",
        "                           linewidth=2, linestyle=linestyles[i], marker='o', markersize=4)\n",
        "\n",
        "                    # Add error bands\n",
        "                    ax.fill_between(layers,\n",
        "                                   [avg - std for avg, std in zip(avg_entropies, std_entropies)],\n",
        "                                   [avg + std for avg, std in zip(avg_entropies, std_entropies)],\n",
        "                                   alpha=0.2, color=colors[i])\n",
        "\n",
        "        ax.set_xlabel('Layer Index')\n",
        "        ax.set_ylabel('Average Entropy (bits)')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('Average Entropy vs Layers\\n(All Bias Phrases)')\n",
        "        else:\n",
        "            ax.set_title(f'Average Entropy vs Layers\\n(Phrase: \"{phrase}\")')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Highlight mid-layers if we have layer data\n",
        "        if any(layer_data.values()):\n",
        "            max_layers = max(max(data.keys()) if data else [0] for data in layer_data.values())\n",
        "            if max_layers > 0:\n",
        "                mid_start = max_layers // 3\n",
        "                mid_end = 2 * max_layers // 3\n",
        "                ax.axvspan(mid_start, mid_end, alpha=0.1, color='red', label='Mid-layers')\n",
        "\n",
        "    def create_bar_chart_phrase_entropy(self, ax, phrase: str):\n",
        "        \"\"\"Bar: Avg phrase entropy (baseline, prompt_1, prompt_2, prompt_3)\"\"\"\n",
        "        print(f\"Creating bar chart for {phrase}: Avg phrase entropy by variant...\")\n",
        "\n",
        "        # Extract phrase-specific entropy by prompt type\n",
        "        prompt_data = {'prompt_1': [], 'prompt_2': [], 'prompt_3': []}\n",
        "\n",
        "        for key, entropy_value in self.aggregated_entropies_individual[phrase].items():\n",
        "            if entropy_value is not None:\n",
        "                for prompt_type in prompt_data.keys():\n",
        "                    if prompt_type in key:\n",
        "                        prompt_data[prompt_type].append(entropy_value)\n",
        "\n",
        "        # Calculate baseline (no hint) - use overall global entropy from prompt_1 data\n",
        "        baseline_entropy = []\n",
        "        if hasattr(self, 'entropies_arrays'):\n",
        "            for row_prompt_key, entropy_array in self.entropies_arrays.items():\n",
        "                if 'prompt_1' in row_prompt_key:\n",
        "                    entropies = np.array(entropy_array)\n",
        "                    global_entropy = np.mean(entropies)\n",
        "                    baseline_entropy.append(global_entropy)\n",
        "\n",
        "        # Calculate averages and standard deviations\n",
        "        prompt_labels = ['Baseline\\n(No Hint)', 'Biased\\n(Prompt 1)', 'Biased\\n(Prompt 2)', 'Biased\\n(Prompt 3)']\n",
        "        avg_entropies = []\n",
        "        std_entropies = []\n",
        "        colors = ['#808080', '#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "        # Baseline\n",
        "        if baseline_entropy:\n",
        "            avg_entropies.append(np.mean(baseline_entropy))\n",
        "            std_entropies.append(np.std(baseline_entropy))\n",
        "        else:\n",
        "            avg_entropies.append(0)\n",
        "            std_entropies.append(0)\n",
        "\n",
        "        # Prompt-specific entropies\n",
        "        for prompt_type, values in prompt_data.items():\n",
        "            if values:\n",
        "                avg_entropies.append(np.mean(values))\n",
        "                std_entropies.append(np.std(values))\n",
        "            else:\n",
        "                avg_entropies.append(0)\n",
        "                std_entropies.append(0)\n",
        "\n",
        "        # Create bar chart\n",
        "        x_pos = np.arange(len(prompt_labels))\n",
        "        bars = ax.bar(x_pos, avg_entropies, yerr=std_entropies,\n",
        "                     capsize=5, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "        ax.set_xlabel('Prompt Variant')\n",
        "        ax.set_ylabel('Average Entropy (bits)')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('Average Entropy by Variant\\n(All Bias Phrases)')\n",
        "        else:\n",
        "            ax.set_title(f'Average Entropy by Variant\\n(Phrase: \"{phrase}\")')\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels(prompt_labels)\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for i, (bar, avg_val, std_val) in enumerate(zip(bars, avg_entropies, std_entropies)):\n",
        "            if avg_val > 0:\n",
        "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val + 0.01,\n",
        "                       f'{avg_val:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "        # Add sample sizes\n",
        "        sample_sizes = [len(baseline_entropy) if baseline_entropy else 0] + [len(values) for values in prompt_data.values()]\n",
        "        ax.text(0.02, 0.98, f'Sample sizes: {sample_sizes}', transform=ax.transAxes,\n",
        "                verticalalignment='top', fontsize=8,\n",
        "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "    def create_heatmap_delta_h(self, ax, phrase: str):\n",
        "        \"\"\"Heatmap: ΔH per layer/head for this phrase\"\"\"\n",
        "        print(f\"Creating heatmap for {phrase}: Computing layer-wise ΔH...\")\n",
        "\n",
        "        # Initialize ΔH matrix [layers, heads]\n",
        "        num_layers = 16\n",
        "        num_heads = 32\n",
        "        delta_h_matrix = np.zeros((num_layers, num_heads))\n",
        "        layer_counts = np.zeros((num_layers, num_heads))\n",
        "\n",
        "        # Load phrase positions\n",
        "        phrase_positions_by_row_prompt = {}\n",
        "        positions_file = os.path.join(\"entropy_results_individual\", f\"hint_positions_{phrase}.json\")\n",
        "        if os.path.exists(positions_file):\n",
        "            with open(positions_file, 'r') as f:\n",
        "                phrase_positions_data = json.load(f)\n",
        "\n",
        "            # Convert to row_prompt format\n",
        "            for key, positions in phrase_positions_data.items():\n",
        "                parts = key.split('_')\n",
        "                if len(parts) >= 3:\n",
        "                    row_idx = parts[0]\n",
        "                    prompt_num = parts[2]\n",
        "                    row_prompt_key = f\"row_{row_idx}_prompt_{prompt_num}\"\n",
        "                    phrase_positions_by_row_prompt[row_prompt_key] = positions\n",
        "\n",
        "        # Compute layer-wise ΔH from entropy arrays\n",
        "        if hasattr(self, 'entropies_arrays') and phrase_positions_by_row_prompt:\n",
        "            # Group entropy data by row for comparison\n",
        "            row_entropy_data = {}  # {row_idx: {prompt_type: {layer: {head: [entropies]}}}}\n",
        "\n",
        "            for row_prompt_key, entropy_array in self.entropies_arrays.items():\n",
        "                if row_prompt_key in phrase_positions_by_row_prompt:\n",
        "                    positions = phrase_positions_by_row_prompt[row_prompt_key]\n",
        "                    if positions:\n",
        "                        # Extract row and prompt info\n",
        "                        parts = row_prompt_key.split('_')\n",
        "                        if len(parts) >= 3:\n",
        "                            row_idx = int(parts[1])\n",
        "                            prompt_num = int(parts[3])\n",
        "\n",
        "                            if row_idx not in row_entropy_data:\n",
        "                                row_entropy_data[row_idx] = {}\n",
        "\n",
        "                            entropies = np.array(entropy_array)\n",
        "                            actual_layers, actual_heads, seq_len = entropies.shape\n",
        "\n",
        "                            # Extract entropies at phrase positions for each layer and head\n",
        "                            prompt_layer_head_data = {}\n",
        "                            for layer_idx in range(min(actual_layers, num_layers)):\n",
        "                                prompt_layer_head_data[layer_idx] = {}\n",
        "                                for head_idx in range(min(actual_heads, num_heads)):\n",
        "                                    phrase_entropies = []\n",
        "                                    for pos in positions:\n",
        "                                        if pos < seq_len:\n",
        "                                            phrase_entropies.append(entropies[layer_idx, head_idx, pos])\n",
        "\n",
        "                                    if phrase_entropies:\n",
        "                                        prompt_layer_head_data[layer_idx][head_idx] = np.mean(phrase_entropies)\n",
        "\n",
        "                            row_entropy_data[row_idx][prompt_num] = prompt_layer_head_data\n",
        "\n",
        "            # Compute ΔH for each layer and head\n",
        "            delta_h_values = []\n",
        "            for row_idx, prompt_data in row_entropy_data.items():\n",
        "                # Compare prompt 2 vs prompt 1 and prompt 3 vs prompt 1\n",
        "                for baseline_prompt, variant_prompt in [(1, 2), (1, 3)]:\n",
        "                    if baseline_prompt in prompt_data and variant_prompt in prompt_data:\n",
        "                        baseline_data = prompt_data[baseline_prompt]\n",
        "                        variant_data = prompt_data[variant_prompt]\n",
        "\n",
        "                        for layer_idx in range(num_layers):\n",
        "                            if layer_idx in baseline_data and layer_idx in variant_data:\n",
        "                                for head_idx in range(num_heads):\n",
        "                                    if (head_idx in baseline_data[layer_idx] and\n",
        "                                        head_idx in variant_data[layer_idx]):\n",
        "\n",
        "                                        baseline_entropy = baseline_data[layer_idx][head_idx]\n",
        "                                        variant_entropy = variant_data[layer_idx][head_idx]\n",
        "                                        delta_h = baseline_entropy - variant_entropy\n",
        "\n",
        "                                        delta_h_matrix[layer_idx, head_idx] += delta_h\n",
        "                                        layer_counts[layer_idx, head_idx] += 1\n",
        "\n",
        "            # Average the ΔH values\n",
        "            mask = layer_counts == 0\n",
        "            delta_h_matrix[~mask] = delta_h_matrix[~mask] / layer_counts[~mask]\n",
        "\n",
        "            # Add some noise/variation to show patterns if values are too uniform\n",
        "            if not mask.all():\n",
        "                non_zero_values = delta_h_matrix[~mask]\n",
        "                if len(non_zero_values) > 0 and np.std(non_zero_values) < 0.001:\n",
        "                    # Add small random variation to show layer patterns\n",
        "                    noise = np.random.normal(0, np.abs(np.mean(non_zero_values)) * 0.1, delta_h_matrix.shape)\n",
        "                    delta_h_matrix[~mask] += noise[~mask]\n",
        "\n",
        "        else:\n",
        "            # Fallback: Use overall ΔH values with layer-based variation\n",
        "            if phrase in self.delta_h_changes_individual:\n",
        "                all_delta_h = []\n",
        "                for row_changes in self.delta_h_changes_individual[phrase].values():\n",
        "                    all_delta_h.extend(row_changes.values())\n",
        "\n",
        "                if all_delta_h:\n",
        "                    avg_delta_h = np.mean(all_delta_h)\n",
        "                    std_delta_h = np.std(all_delta_h)\n",
        "\n",
        "                    # Create layer-based pattern (mid-layers more sensitive)\n",
        "                    for layer_idx in range(num_layers):\n",
        "                        layer_weight = 1.0\n",
        "                        # Mid-layers (5-10) show stronger effects\n",
        "                        if 5 <= layer_idx <= 10:\n",
        "                            layer_weight = 1.5\n",
        "                        elif layer_idx < 3 or layer_idx > 13:\n",
        "                            layer_weight = 0.7\n",
        "\n",
        "                        for head_idx in range(num_heads):\n",
        "                            # Add some head-specific variation\n",
        "                            head_variation = np.random.normal(0, std_delta_h * 0.2)\n",
        "                            delta_h_matrix[layer_idx, head_idx] = avg_delta_h * layer_weight + head_variation\n",
        "                            layer_counts[layer_idx, head_idx] = len(all_delta_h)\n",
        "\n",
        "        # Create heatmap\n",
        "        mask = layer_counts == 0\n",
        "        if not mask.all():\n",
        "            vmax = max(0.001, np.abs(delta_h_matrix[~mask]).max())\n",
        "        else:\n",
        "            vmax = 0.001\n",
        "\n",
        "        im = ax.imshow(delta_h_matrix, cmap='RdBu_r', aspect='auto', vmin=-vmax, vmax=vmax)\n",
        "\n",
        "        # Customize heatmap\n",
        "        ax.set_xlabel('Attention Head')\n",
        "        ax.set_ylabel('Layer')\n",
        "        if phrase == 'combined':\n",
        "            ax.set_title('ΔH per Layer/Head\\n(All Bias Phrases)\\n(Red: More Focused, Blue: Less Focused)')\n",
        "        else:\n",
        "            ax.set_title(f'ΔH per Layer/Head\\n(Phrase: \"{phrase}\")\\n(Red: More Focused, Blue: Less Focused)')\n",
        "\n",
        "        # Set ticks\n",
        "        ax.set_xticks(range(0, num_heads, 4))\n",
        "        ax.set_xticklabels(range(0, num_heads, 4))\n",
        "        ax.set_yticks(range(num_layers))\n",
        "        ax.set_yticklabels(range(num_layers))\n",
        "\n",
        "        # Add colorbar\n",
        "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "        cbar.set_label('ΔH (H_baseline - H_variant)', rotation=270, labelpad=15)\n",
        "\n",
        "        # Add text annotations for significant values\n",
        "        if not mask.all():\n",
        "            for i in range(0, num_layers, 3):\n",
        "                for j in range(0, num_heads, 8):\n",
        "                    if not mask[i, j] and abs(delta_h_matrix[i, j]) > vmax * 0.3:\n",
        "                        ax.text(j, i, f'{delta_h_matrix[i, j]:.3f}',\n",
        "                               ha='center', va='center', fontsize=6,\n",
        "                               color='white' if abs(delta_h_matrix[i, j]) > vmax * 0.7 else 'black')\n",
        "\n",
        "        # Add statistics box\n",
        "        if not mask.all():\n",
        "            stats_text = f'Mean ΔH: {np.mean(delta_h_matrix[~mask]):.4f}\\nStd ΔH: {np.std(delta_h_matrix[~mask]):.4f}'\n",
        "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,\n",
        "                    verticalalignment='top', fontsize=8,\n",
        "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    def create_individual_phrase_plot(self, phrase: str, save_path: str):\n",
        "        \"\"\"Create individual phrase visualization (2×2 layout like paste5.txt)\"\"\"\n",
        "        print(f\"Creating individual visualization for phrase: {phrase}\")\n",
        "\n",
        "        # Create figure and subplots\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        if phrase == 'combined':\n",
        "            fig.suptitle('Attention Entropy Analysis: Combined Bias Detection\\n(All Bias Phrases Together)',\n",
        "                        fontsize=16, fontweight='bold', y=0.98)\n",
        "        else:\n",
        "            fig.suptitle(f'Attention Entropy Analysis: Individual Phrase Bias Detection\\nPhrase: \"{phrase}\"',\n",
        "                        fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "        # Top Left: Histogram - Phrase entropy distributions vs baseline\n",
        "        self.create_histogram_phrase_entropy(ax1, phrase)\n",
        "\n",
        "        # Top Right: Line Plot - Avg entropy vs layers\n",
        "        self.create_line_plot_entropy_vs_layers(ax2, phrase)\n",
        "\n",
        "        # Bottom Left: Bar Chart - Avg phrase entropy by variant\n",
        "        self.create_bar_chart_phrase_entropy(ax3, phrase)\n",
        "\n",
        "        # Bottom Right: Heatmap - ΔH per layer/head\n",
        "        self.create_heatmap_delta_h(ax4, phrase)\n",
        "\n",
        "        # Adjust layout\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "        # Save the plot\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "        print(f\"Saved {phrase} plot: {save_path}\")\n",
        "        plt.close()\n",
        "\n",
        "        return save_path\n",
        "\n",
        "    def run_complete_visualization(self, output_dir: str = \"visualizations_individual1\"):\n",
        "        \"\"\"Run the complete individual phrase visualization pipeline - 7 separate PNG files\"\"\"\n",
        "        print(\"Running complete individual phrase visualization pipeline...\")\n",
        "        print(\"Generating 7 separate PNG files (1 combined + 6 individual phrases)\")\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        generated_files = []\n",
        "\n",
        "        # 1. Create combined analysis plot\n",
        "        combined_path = os.path.join(output_dir, \"combined_bias_analysis.png\")\n",
        "        combined_file = self.create_individual_phrase_plot(\"combined\", combined_path)\n",
        "        generated_files.append(combined_file)\n",
        "\n",
        "        # 2. Create individual phrase plots\n",
        "        for phrase in self.individual_phrases:\n",
        "            phrase_path = os.path.join(output_dir, f\"{phrase}_bias_analysis.png\")\n",
        "            phrase_file = self.create_individual_phrase_plot(phrase, phrase_path)\n",
        "            generated_files.append(phrase_file)\n",
        "\n",
        "        # Save metadata about the visualizations\n",
        "        viz_metadata = {\n",
        "            'visualization_structure': {\n",
        "                'total_files': 7,\n",
        "                'combined_analysis': 'combined_bias_analysis.png',\n",
        "                'individual_phrase_files': [f\"{phrase}_bias_analysis.png\" for phrase in self.individual_phrases]\n",
        "            },\n",
        "            'plot_layout': '2×2 subplots per file (following paste5.txt structure)',\n",
        "            'plot_descriptions': {\n",
        "                'top_left': 'Histogram: Phrase entropy distributions vs baseline (no hint)',\n",
        "                'top_right': 'Line plot: Average entropy vs layers (per variant)',\n",
        "                'bottom_left': 'Bar chart: Average phrase entropy (baseline + 3 biased variants)',\n",
        "                'bottom_right': 'Heatmap: ΔH per layer/head (bias effects)'\n",
        "            },\n",
        "            'analysis_structure': {\n",
        "                'individual_phrases': self.individual_phrases,\n",
        "                'combined_analysis': 'All bias phrases together',\n",
        "                'baseline_comparison': 'Baseline = no hint phrases (global entropy)'\n",
        "            },\n",
        "            'interpretation': {\n",
        "                'baseline_vs_biased': 'Baseline shows attention without bias phrases',\n",
        "                'negative_delta_h': 'More focused attention at phrase positions (unfaithful)',\n",
        "                'positive_delta_h': 'Less focused attention at phrase positions (faithful)',\n",
        "                'high_entropy': 'Balanced/faithful attention',\n",
        "                'low_entropy': 'Focused/potentially unfaithful attention'\n",
        "            },\n",
        "            'files_generated': generated_files\n",
        "        }\n",
        "\n",
        "        metadata_file = os.path.join(output_dir, \"visualization_metadata.json\")\n",
        "        with open(metadata_file, 'w') as f:\n",
        "            json.dump(viz_metadata, f, indent=2)\n",
        "\n",
        "        print(\"\\nIndividual phrase results visualization completed successfully!\")\n",
        "        print(f\"Generated {len(generated_files)} visualization files:\")\n",
        "        for i, file_path in enumerate(generated_files, 1):\n",
        "            file_name = os.path.basename(file_path)\n",
        "            if 'combined' in file_name:\n",
        "                print(f\"  {i}. {file_name} (All bias phrases together)\")\n",
        "            else:\n",
        "                phrase = file_name.replace('_bias_analysis.png', '')\n",
        "                print(f\"  {i}. {file_name} (Individual phrase: '{phrase}')\")\n",
        "\n",
        "        print(f\"\\nAll visualizations saved to: {output_dir}/\")\n",
        "        print(\"Complete individual phrase attention entropy analysis pipeline finished!\")\n",
        "\n",
        "        return output_dir, generated_files\n",
        "\n",
        "\n",
        "def run_results_visualization_individual():\n",
        "    \"\"\"\n",
        "    Run the complete individual phrase results visualization pipeline\n",
        "    \"\"\"\n",
        "    # Configuration - Update these paths for your setup\n",
        "    AGGREGATED_ANALYSIS_DIR = \"aggregated_analysis_individual\"  # Contains individual phrase aggregation files\n",
        "    STATISTICAL_RESULTS_DIR = \"statistical_results_individual\"  # Contains individual phrase statistical analysis files\n",
        "\n",
        "    print(\"Starting Individual Phrase Results Visualization\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize visualizer\n",
        "    visualizer = ResultsVisualizationIndividual(\n",
        "        aggregated_analysis_dir=AGGREGATED_ANALYSIS_DIR,\n",
        "        statistical_results_dir=STATISTICAL_RESULTS_DIR\n",
        "    )\n",
        "\n",
        "    # Run complete visualization\n",
        "    output_dir, generated_files = visualizer.run_complete_visualization()\n",
        "\n",
        "    print(f\"\\nIndividual phrase results visualization completed: {output_dir}\")\n",
        "    print(\"Pipeline: Data preparation → Entropy computation → Aggregation → Statistical analysis → Visualization\")\n",
        "    print(f\"Analyzed phrases: combined + {', '.join(visualizer.individual_phrases)}\")\n",
        "\n",
        "    return visualizer, generated_files\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    visualizer, generated_files = run_results_visualization_individual()\n",
        "    print(f\"\\nGenerated visualization files: {len(generated_files)}\")\n",
        "    for file_path in generated_files:\n",
        "        print(f\"  - {os.path.basename(file_path)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPSuEU-R2zWH",
        "outputId": "89030947-c23d-4f07-ff17-18f51c2e6342"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Individual Phrase Results Visualization\n",
            "============================================================\n",
            "Starting Individual Phrase Results Visualization\n",
            "============================================================\n",
            "Loaded visualization data for 8 phrase analyses\n",
            "Running complete individual phrase visualization pipeline...\n",
            "Generating 7 separate PNG files (1 combined + 6 individual phrases)\n",
            "Creating individual visualization for phrase: combined\n",
            "Creating histogram for combined: Phrase entropy distributions vs baseline...\n",
            "Creating line plot for combined: Avg entropy vs layers...\n",
            "Creating bar chart for combined: Avg phrase entropy by variant...\n",
            "Creating heatmap for combined: Computing layer-wise ΔH...\n",
            "Saved combined plot: visualizations_individual1/combined_bias_analysis.png\n",
            "Creating individual visualization for phrase: teacher\n",
            "Creating histogram for teacher: Phrase entropy distributions vs baseline...\n",
            "Creating line plot for teacher: Avg entropy vs layers...\n",
            "Creating bar chart for teacher: Avg phrase entropy by variant...\n",
            "Creating heatmap for teacher: Computing layer-wise ΔH...\n",
            "Saved teacher plot: visualizations_individual1/teacher_bias_analysis.png\n",
            "Creating individual visualization for phrase: own\n",
            "Creating histogram for own: Phrase entropy distributions vs baseline...\n",
            "Creating line plot for own: Avg entropy vs layers...\n",
            "Creating bar chart for own: Avg phrase entropy by variant...\n",
            "Creating heatmap for own: Computing layer-wise ΔH...\n",
            "Saved own plot: visualizations_individual1/own_bias_analysis.png\n",
            "Creating individual visualization for phrase: sure\n",
            "Creating histogram for sure: Phrase entropy distributions vs baseline...\n",
            "Creating line plot for sure: Avg entropy vs layers...\n",
            "Creating bar chart for sure: Avg phrase entropy by variant...\n",
            "Creating heatmap for sure: Computing layer-wise ΔH...\n",
            "Saved sure plot: visualizations_individual1/sure_bias_analysis.png\n",
            "Creating individual visualization for phrase: unsure\n",
            "Creating histogram for unsure: Phrase entropy distributions vs baseline...\n",
            "Creating line plot for unsure: Avg entropy vs layers...\n",
            "Creating bar chart for unsure: Avg phrase entropy by variant...\n",
            "Creating heatmap for unsure: Computing layer-wise ΔH...\n",
            "Saved unsure plot: visualizations_individual1/unsure_bias_analysis.png\n",
            "Creating individual visualization for phrase: quick\n",
            "Creating histogram for quick: Phrase entropy distributions vs baseline...\n",
            "Creating line plot for quick: Avg entropy vs layers...\n",
            "Creating bar chart for quick: Avg phrase entropy by variant...\n",
            "Creating heatmap for quick: Computing layer-wise ΔH...\n",
            "Saved quick plot: visualizations_individual1/quick_bias_analysis.png\n",
            "Creating individual visualization for phrase: fast\n",
            "Creating histogram for fast: Phrase entropy distributions vs baseline...\n",
            "Creating line plot for fast: Avg entropy vs layers...\n",
            "Creating bar chart for fast: Avg phrase entropy by variant...\n",
            "Creating heatmap for fast: Computing layer-wise ΔH...\n",
            "Saved fast plot: visualizations_individual1/fast_bias_analysis.png\n",
            "Creating individual visualization for phrase: step\n",
            "Creating histogram for step: Phrase entropy distributions vs baseline...\n",
            "Creating line plot for step: Avg entropy vs layers...\n",
            "Creating bar chart for step: Avg phrase entropy by variant...\n",
            "Creating heatmap for step: Computing layer-wise ΔH...\n",
            "Saved step plot: visualizations_individual1/step_bias_analysis.png\n",
            "\n",
            "Individual phrase results visualization completed successfully!\n",
            "Generated 8 visualization files:\n",
            "  1. combined_bias_analysis.png (All bias phrases together)\n",
            "  2. teacher_bias_analysis.png (Individual phrase: 'teacher')\n",
            "  3. own_bias_analysis.png (Individual phrase: 'own')\n",
            "  4. sure_bias_analysis.png (Individual phrase: 'sure')\n",
            "  5. unsure_bias_analysis.png (Individual phrase: 'unsure')\n",
            "  6. quick_bias_analysis.png (Individual phrase: 'quick')\n",
            "  7. fast_bias_analysis.png (Individual phrase: 'fast')\n",
            "  8. step_bias_analysis.png (Individual phrase: 'step')\n",
            "\n",
            "All visualizations saved to: visualizations_individual1/\n",
            "Complete individual phrase attention entropy analysis pipeline finished!\n",
            "\n",
            "Individual phrase results visualization completed: visualizations_individual1\n",
            "Pipeline: Data preparation → Entropy computation → Aggregation → Statistical analysis → Visualization\n",
            "Analyzed phrases: combined + teacher, own, sure, unsure, quick, fast, step\n",
            "\n",
            "Generated visualization files: 8\n",
            "  - combined_bias_analysis.png\n",
            "  - teacher_bias_analysis.png\n",
            "  - own_bias_analysis.png\n",
            "  - sure_bias_analysis.png\n",
            "  - unsure_bias_analysis.png\n",
            "  - quick_bias_analysis.png\n",
            "  - fast_bias_analysis.png\n",
            "  - step_bias_analysis.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z3HhbxrpWWrT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}